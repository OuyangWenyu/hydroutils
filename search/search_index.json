{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to hydroutils","text":"<p>A collection of commonly used utility functions for hydrological modeling and analysis</p> <p><code>hydroutils</code> is a comprehensive Python package that provides essential tools and utilities for hydrological data processing, statistical analysis, and modeling. It is designed to streamline common tasks in hydrology research and engineering applications.</p>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#statistical-analysis","title":"\ud83d\udcca Statistical Analysis","text":"<ul> <li>Comprehensive hydrological statistics (NSE, KGE, RMSE, Bias, etc.)</li> <li>Flow duration curve analysis</li> <li>Peak flow analysis and timing metrics</li> <li>Flood event extraction and characterization</li> </ul>"},{"location":"#time-series-processing","title":"\ud83d\udd50 Time Series Processing","text":"<ul> <li>Time unit conversions and standardization</li> <li>Time interval detection and validation</li> <li>Temporal data manipulation utilities</li> </ul>"},{"location":"#data-visualization","title":"\ud83d\udcc8 Data Visualization","text":"<ul> <li>Specialized plotting functions for hydrological data</li> <li>Flow duration curves</li> <li>Time series plots with hydrological context</li> </ul>"},{"location":"#file-operations","title":"\ud83d\udcc1 File Operations","text":"<ul> <li>NetCDF file handling</li> <li>CSV and text file processing</li> <li>Data import/export utilities</li> </ul>"},{"location":"#cloud-integration","title":"\u2601\ufe0f Cloud Integration","text":"<ul> <li>AWS S3 integration for large dataset handling</li> <li>Cloud-based data storage and retrieval</li> </ul>"},{"location":"#mathematical-operations","title":"\ud83e\uddee Mathematical Operations","text":"<ul> <li>Hydrological unit conversions</li> <li>Mathematical utilities for water resources calculations</li> <li>Array operations optimized for hydrological data</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>import hydroutils as hu\n\n# Calculate hydrological statistics\nnse = hu.stat_error(observed, simulated)['NSE']\n\n# Extract flood events\nevents = hu.extract_flood_events(dataframe)\n\n# Convert streamflow units\nconverted = hu.streamflow_unit_conv(data, from_unit='cms', to_unit='cfs')\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install hydroutils\n</code></pre>"},{"location":"#documentation-structure","title":"Documentation Structure","text":"<ul> <li>Installation Guide - Detailed installation instructions</li> <li>Usage Examples - Practical examples and tutorials  </li> <li>API Reference - Complete API documentation</li> <li>Contributing - How to contribute to the project</li> <li>FAQ - Frequently asked questions</li> </ul>"},{"location":"#license-credits","title":"License &amp; Credits","text":"<ul> <li>Free software: MIT license</li> <li>Documentation: https://zhuanglaihong.github.io/hydroutils</li> <li>Created with Cookiecutter and the giswqs/pypackage project template</li> </ul>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to the hydroutils project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#v0014-2025-08-19","title":"v0.0.14 - 2025-08-19","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>\u5b8c\u6574\u7684\u9879\u76ee\u6587\u6863\u7ed3\u6784\uff0c\u5305\u62ecAPI\u53c2\u8003\u3001\u4f7f\u7528\u6307\u5357\u548c\u793a\u4f8b</li> <li>\u65b0\u589e\u6c34\u6587\u7edf\u8ba1\u5206\u6790\u6a21\u5757 (<code>hydro_stat</code>)</li> <li>\u652f\u6301NSE\u3001KGE\u3001RMSE\u7b49\u591a\u79cd\u8bc4\u4ef7\u6307\u6807</li> <li>\u6d2a\u6c34\u4e8b\u4ef6\u63d0\u53d6\u548c\u5206\u6790\u529f\u80fd</li> <li>\u6d41\u91cf\u6301\u7eed\u66f2\u7ebf\u5206\u6790</li> <li>\u65f6\u95f4\u5e8f\u5217\u5904\u7406\u6a21\u5757 (<code>hydro_time</code>)</li> <li>\u65f6\u95f4\u95f4\u9694\u68c0\u6d4b\u548c\u9a8c\u8bc1</li> <li>\u5355\u4f4d\u8f6c\u6362\u529f\u80fd</li> <li>\u53ef\u89c6\u5316\u5de5\u5177\u6a21\u5757 (<code>hydro_plot</code>)</li> <li>\u6c34\u6587\u6570\u636e\u4e13\u7528\u7ed8\u56fe\u51fd\u6570</li> <li>\u6a21\u578b\u8bc4\u4ef7\u53ef\u89c6\u5316\u5de5\u5177</li> <li>\u53d1\u5e03\u7ea7\u522b\u56fe\u8868\u8f93\u51fa</li> <li>AWS S3\u96c6\u6210\u6a21\u5757 (<code>hydro_s3</code>)</li> <li>\u652f\u6301\u5927\u89c4\u6a21\u6c34\u6587\u6570\u636e\u4e91\u5b58\u50a8</li> <li>\u6279\u91cf\u6570\u636e\u4e0a\u4f20\u4e0b\u8f7d</li> <li>\u65e5\u5fd7\u5de5\u5177\u6a21\u5757 (<code>hydro_log</code>)</li> <li>\u4e13\u4e1a\u7684\u6c34\u6587\u5206\u6790\u65e5\u5fd7\u8bb0\u5f55</li> <li>\u6027\u80fd\u76d1\u63a7\u548c\u9519\u8bef\u8ffd\u8e2a</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>\u91cd\u6784\u4e86\u9879\u76ee\u7ed3\u6784\uff0c\u4f18\u5316\u6a21\u5757\u7ec4\u7ec7</li> <li>\u6539\u8fdb\u4e86\u6570\u636e\u5904\u7406\u6d41\u7a0b\uff0c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387</li> <li>\u66f4\u65b0\u4e86\u6240\u6709\u4f9d\u8d56\u5305\u7684\u7248\u672c\u8981\u6c42</li> <li>\u7edf\u4e00\u4e86\u4ee3\u7801\u98ce\u683c\u548c\u6587\u6863\u683c\u5f0f</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>\u4fee\u590d\u4e86\u7edf\u8ba1\u8ba1\u7b97\u4e2d\u7684NaN\u503c\u5904\u7406\u95ee\u9898</li> <li>\u89e3\u51b3\u4e86\u65f6\u95f4\u5e8f\u5217\u5bf9\u9f50\u7684bug</li> <li>\u4fee\u6b63\u4e86\u5355\u4f4d\u8f6c\u6362\u7684\u7cbe\u5ea6\u95ee\u9898</li> <li>\u4f18\u5316\u4e86\u5185\u5b58\u4f7f\u7528\uff0c\u89e3\u51b3\u4e86\u5927\u6570\u636e\u5904\u7406\u65f6\u7684\u5185\u5b58\u6ea2\u51fa</li> </ul>"},{"location":"changelog/#deprecated","title":"Deprecated","text":"<ul> <li>\u79fb\u9664\u4e86\u8fc7\u65f6\u7684\u6570\u636e\u683c\u5f0f\u652f\u6301</li> <li>\u5e9f\u5f03\u4e86\u90e8\u5206\u4e0d\u63a8\u8350\u4f7f\u7528\u7684\u51fd\u6570\u63a5\u53e3</li> </ul>"},{"location":"changelog/#v0013-2025-07-15","title":"v0.0.13 - 2025-07-15","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>\u521d\u59cb\u7248\u672c\u53d1\u5e03</li> <li>\u57fa\u7840\u7684\u6c34\u6587\u7edf\u8ba1\u529f\u80fd</li> <li>\u7b80\u5355\u7684\u6570\u636e\u5904\u7406\u5de5\u5177</li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>\u57fa\u7840\u529f\u80fd\u5b9e\u73b0\u548c\u6d4b\u8bd5</li> </ul>"},{"location":"changelog/#unreleased","title":"[Unreleased]","text":""},{"location":"changelog/#planned","title":"Planned","text":"<ul> <li>\u589e\u52a0\u673a\u5668\u5b66\u4e60\u6a21\u5757\u652f\u6301</li> <li>\u6dfb\u52a0\u66f4\u591a\u6c34\u6587\u6a21\u578b\u8bc4\u4ef7\u6307\u6807</li> <li>\u6539\u8fdb\u6570\u636e\u53ef\u89c6\u5316\u529f\u80fd</li> <li>\u4f18\u5316\u5927\u89c4\u6a21\u6570\u636e\u5904\u7406\u6027\u80fd</li> <li>\u6dfb\u52a0\u66f4\u591a\u5355\u5143\u6d4b\u8bd5\u548c\u96c6\u6210\u6d4b\u8bd5</li> </ul>"},{"location":"changelog/#version-number-guide","title":"Version Number Guide","text":"<ul> <li>MAJOR version (x.0.0) - \u4e0d\u517c\u5bb9\u7684API\u4fee\u6539</li> <li>MINOR version (0.x.0) - \u5411\u540e\u517c\u5bb9\u7684\u529f\u80fd\u6027\u65b0\u589e</li> <li>PATCH version (0.0.x) - \u5411\u540e\u517c\u5bb9\u7684\u95ee\u9898\u4fee\u590d</li> </ul>"},{"location":"changelog/#links","title":"Links","text":""},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/zhuanglaihong/hydroutils/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with <code>bug</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with <code>enhancement</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>hydroutils could always use more documentation, whether as part of the official hydroutils docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/zhuanglaihong/hydroutils/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up hydroutils for local development.</p> <ol> <li> <p>Fork the hydroutils repo on GitHub.</p> </li> <li> <p>Clone your fork locally:</p> <pre><code>$ git clone git@github.com:your_name_here/hydroutils.git\n</code></pre> </li> <li> <p>Install your local copy into a virtualenv. Assuming you have     virtualenvwrapper installed, this is how you set up your fork for     local development:</p> <pre><code>$ mkvirtualenv hydroutils\n$ cd hydroutils/\n$ python setup.py develop\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass flake8     and the tests, including testing other Python versions with tox:</p> <pre><code>$ flake8 hydroutils tests\n$ python setup.py test or pytest\n$ tox\n</code></pre> <p>To get flake8 and tox, just pip install them into your virtualenv.</p> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated.     Put your new functionality into a function with a docstring, and add     the feature to the list in README.rst.</li> <li>The pull request should work for Python 3.5, 3.6, 3.7 and 3.8, and     for PyPy. Check https://github.com/zhuanglaihong/hydroutils/pull_requests and make sure that the tests pass for all     supported Python versions.</li> </ol>"},{"location":"faq/","title":"Frequently Asked Questions (FAQ)","text":""},{"location":"faq/#installation-and-setup","title":"Installation and Setup","text":""},{"location":"faq/#q-how-do-i-install-hydroutils","title":"Q: How do I install hydroutils?","text":"<p>A: The easiest way is using pip: <pre><code>pip install hydroutils\n</code></pre></p> <p>For the latest development version: <pre><code>pip install git+https://github.com/zhuanglaihong/hydroutils.git\n</code></pre></p>"},{"location":"faq/#q-what-python-versions-are-supported","title":"Q: What Python versions are supported?","text":"<p>A: hydroutils supports Python 3.8 and higher. We recommend using Python 3.10 or later for the best performance and compatibility.</p>"},{"location":"faq/#q-im-getting-import-errors-what-should-i-do","title":"Q: I'm getting import errors. What should I do?","text":"<p>A: First, ensure all dependencies are installed: <pre><code>pip install --upgrade hydroutils\n</code></pre></p> <p>If you're still having issues, try installing in a fresh virtual environment: <pre><code>python -m venv hydroutils-env\nsource hydroutils-env/bin/activate  # On Windows: hydroutils-env\\Scripts\\activate\npip install hydroutils\n</code></pre></p>"},{"location":"faq/#usage-questions","title":"Usage Questions","text":""},{"location":"faq/#q-how-do-i-calculate-basic-hydrological-statistics","title":"Q: How do I calculate basic hydrological statistics?","text":"<p>A: Use the <code>stat_error</code> function: <pre><code>import hydroutils as hu\nimport numpy as np\n\nobserved = np.array([10.5, 12.3, 8.7, 15.2, 11.8])\nsimulated = np.array([10.1, 12.8, 8.9, 14.7, 11.2])\n\nstats = hu.stat_error(observed, simulated)\nprint(f\"NSE: {stats['NSE'][0]:.3f}\")\nprint(f\"RMSE: {stats['RMSE'][0]:.3f}\")\n</code></pre></p>"},{"location":"faq/#q-can-i-handle-missing-data-nan-values","title":"Q: Can I handle missing data (NaN values)?","text":"<p>A: Yes, most functions automatically handle NaN values by excluding them from calculations: <pre><code>obs_with_nan = np.array([1.0, 2.0, np.nan, 4.0, 5.0])\nsim_with_nan = np.array([1.1, 2.2, 3.1, 4.2, np.nan])\n\n# This works fine - NaN values are automatically excluded\nstats = hu.stat_error(obs_with_nan, sim_with_nan)\n</code></pre></p>"},{"location":"faq/#q-how-do-i-convert-between-different-flow-units","title":"Q: How do I convert between different flow units?","text":"<p>A: Use the <code>streamflow_unit_conv</code> function: <pre><code># Convert from cubic meters per second to cubic feet per second\nflow_cms = np.array([10.5, 12.3, 8.7])\nflow_cfs = hu.streamflow_unit_conv(flow_cms, from_unit='cms', to_unit='cfs')\n</code></pre></p>"},{"location":"faq/#q-what-performance-metrics-are-available","title":"Q: What performance metrics are available?","text":"<p>A: hydroutils provides many standard hydrological metrics: - NSE: Nash-Sutcliffe Efficiency - KGE: Kling-Gupta Efficiency - RMSE: Root Mean Square Error - Bias: Mean Error - Corr: Pearson Correlation Coefficient - R2: Coefficient of Determination - FHV/FLV: High/Low Flow Volume metrics</p>"},{"location":"faq/#data-processing","title":"Data Processing","text":""},{"location":"faq/#q-how-do-i-process-multiple-time-series-at-once","title":"Q: How do I process multiple time series at once?","text":"<p>A: Use the <code>stat_errors</code> function for batch processing: <pre><code># Multiple stations data (5 stations, 100 time steps each)\nobserved_series = np.random.rand(5, 100)\nsimulated_series = observed_series + np.random.normal(0, 0.1, (5, 100))\n\n# Calculate statistics for all series\nall_stats = hu.stat_errors(observed_series, simulated_series)\n\n# Extract NSE values for all stations\nnse_values = [stats['NSE'][0] for stats in all_stats]\n</code></pre></p>"},{"location":"faq/#q-can-i-work-with-pandas-dataframes","title":"Q: Can I work with pandas DataFrames?","text":"<p>A: Yes, you can easily work with pandas DataFrames: <pre><code>import pandas as pd\n\n# Convert DataFrame columns to numpy arrays\ndf = pd.read_csv('streamflow_data.csv')\nobs = df['observed'].values\nsim = df['simulated'].values\n\nstats = hu.stat_error(obs, sim)\n</code></pre></p>"},{"location":"faq/#q-how-do-i-handle-different-time-intervals","title":"Q: How do I handle different time intervals?","text":"<p>A: Use the time processing functions: <pre><code># Detect time interval automatically\ntime_series = pd.date_range('2020-01-01', periods=100, freq='D')\ninterval = hu.detect_time_interval(time_series)\n\n# Validate unit compatibility\nis_compatible = hu.validate_unit_compatibility('cms', 'streamflow')\n</code></pre></p>"},{"location":"faq/#visualization","title":"Visualization","text":""},{"location":"faq/#q-how-do-i-create-basic-plots","title":"Q: How do I create basic plots?","text":"<p>A: Use the hydro_plot module: <pre><code>import matplotlib.pyplot as plt\n\n# Time series plot\nfig, ax = hu.plot_timeseries(\n    dates, observed, simulated,\n    labels=['Observed', 'Simulated'],\n    title='Streamflow Comparison'\n)\nplt.show()\n\n# Performance scatter plot\nfig, ax = hu.plot_scatter_performance(\n    observed, simulated,\n    add_stats=True,\n    add_1to1_line=True\n)\nplt.show()\n</code></pre></p>"},{"location":"faq/#q-can-i-customize-plot-appearance","title":"Q: Can I customize plot appearance?","text":"<p>A: Yes, hydroutils provides several styling options: <pre><code># Set hydrological plot style\nhu.set_hydro_plot_style()\n\n# Use hydrological color schemes\ncolors = hu.get_hydro_colors(data_type='streamflow')\n</code></pre></p>"},{"location":"faq/#advanced-features","title":"Advanced Features","text":""},{"location":"faq/#q-how-do-i-use-aws-s3-integration","title":"Q: How do I use AWS S3 integration?","text":"<p>A: First configure your AWS credentials, then use S3 functions: <pre><code># Upload data to S3\nhu.upload_to_s3(\n    local_file='data.csv',\n    bucket='my-hydro-data',\n    s3_key='station_001/data.csv'\n)\n\n# Download from S3\nhu.download_from_s3(\n    bucket='my-hydro-data',\n    s3_key='station_001/data.csv',\n    local_file='downloaded_data.csv'\n)\n</code></pre></p>"},{"location":"faq/#q-how-do-i-enable-logging-for-my-analysis","title":"Q: How do I enable logging for my analysis?","text":"<p>A: Use the logging utilities: <pre><code># Setup logger\nlogger = hu.setup_hydro_logger(\n    name='my_analysis',\n    log_file='analysis.log',\n    level='INFO'\n)\n\n# Log your analysis steps\nlogger.info(\"Starting streamflow analysis\")\nstats = hu.stat_error(observed, simulated)\nlogger.info(f\"NSE calculated: {stats['NSE'][0]:.3f}\")\n</code></pre></p>"},{"location":"faq/#troubleshooting","title":"Troubleshooting","text":""},{"location":"faq/#q-im-getting-unexpected-nse-values-what-could-be-wrong","title":"Q: I'm getting unexpected NSE values. What could be wrong?","text":"<p>A: Check these common issues: 1. Data alignment: Ensure observed and simulated data have the same time periods 2. Missing values: Make sure missing data is properly handled 3. Data quality: Check for outliers or unrealistic values 4. Array dimensions: Verify that arrays have the same shape</p> <pre><code># Debug your data\nprint(f\"Observed shape: {observed.shape}\")\nprint(f\"Simulated shape: {simulated.shape}\")\nprint(f\"NaN count in observed: {np.isnan(observed).sum()}\")\nprint(f\"NaN count in simulated: {np.isnan(simulated).sum()}\")\n</code></pre>"},{"location":"faq/#q-why-am-i-getting-poor-performance-metrics","title":"Q: Why am I getting poor performance metrics?","text":"<p>A: Consider these factors: 1. Model quality: The underlying model may need improvement 2. Data period: Performance can vary by season or flow conditions 3. Metric selection: Different metrics emphasize different aspects of performance 4. Data preprocessing: Check if data normalization or transformation is needed</p>"},{"location":"faq/#q-functions-are-running-slowly-how-can-i-improve-performance","title":"Q: Functions are running slowly. How can I improve performance?","text":"<p>A: Try these optimization strategies: 1. Use appropriate data types: Convert to float32 if high precision isn't needed 2. Process in chunks: For very large datasets, process data in smaller chunks 3. Vectorize operations: Use NumPy operations instead of loops 4. Consider memory usage: Monitor memory consumption for large arrays</p> <pre><code># Example of chunked processing\ndef process_large_dataset(large_array, chunk_size=10000):\n    results = []\n    for i in range(0, len(large_array), chunk_size):\n        chunk = large_array[i:i+chunk_size]\n        result = hu.stat_error(chunk['obs'], chunk['sim'])\n        results.append(result)\n    return results\n</code></pre>"},{"location":"faq/#getting-help","title":"Getting Help","text":""},{"location":"faq/#q-where-can-i-find-more-examples","title":"Q: Where can I find more examples?","text":"<p>A: Check these resources: 1. Usage Guide: Detailed examples in the Usage section 2. API Documentation: Complete function reference in API Reference 3. GitHub Examples: Example notebooks in the repository 4. Community: Ask questions in GitHub Issues</p>"},{"location":"faq/#q-how-do-i-report-bugs-or-request-features","title":"Q: How do I report bugs or request features?","text":"<p>A: Please use the GitHub Issues: 1. Bug Reports: Create a bug report 2. Feature Requests: Request a new feature 3. Questions: Use the Discussions section</p>"},{"location":"faq/#q-can-i-contribute-to-the-project","title":"Q: Can I contribute to the project?","text":"<p>A: Yes! We welcome contributions. See the Contributing Guide for details on: - Setting up a development environment - Code style guidelines - Testing requirements - Submitting pull requests</p>"},{"location":"faq/#q-is-there-a-citation-for-academic-use","title":"Q: Is there a citation for academic use?","text":"<p>A: Yes, if you use hydroutils in academic research, please cite: <pre><code>@software{hydroutils,\n  author = {Your Name},\n  title = {hydroutils: A Python package for hydrological analysis},\n  url = {https://github.com/zhuanglaihong/hydroutils},\n  version = {X.X.X},\n  year = {2024}\n}\n</code></pre></p>"},{"location":"faq/#still-need-help","title":"Still Need Help?","text":"<p>If your question isn't answered here:</p> <ol> <li>Search existing issues: GitHub Issues</li> <li>Ask a question: GitHub Discussions</li> <li>Email support: [Contact Information]</li> </ol> <p>We're here to help you succeed with your hydrological analysis!</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#requirements","title":"Requirements","text":"<p><code>hydroutils</code> requires Python 3.8 or higher. The package has been tested on:</p> <ul> <li>Python 3.8, 3.9, 3.10, 3.11</li> <li>Windows, macOS, and Linux</li> </ul>"},{"location":"installation/#core-dependencies","title":"Core Dependencies","text":"<p>The following packages are automatically installed:</p> <ul> <li><code>numpy</code> - Array operations and mathematical functions</li> <li><code>pandas</code> - Data manipulation and analysis</li> <li><code>scipy</code> - Scientific computing</li> <li><code>matplotlib</code> - Plotting and visualization</li> <li><code>xarray</code> - Labeled multi-dimensional arrays</li> <li><code>netCDF4</code> - NetCDF file handling</li> <li><code>HydroErr</code> - Hydrological error metrics</li> </ul>"},{"location":"installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>For extended functionality:</p> <ul> <li><code>boto3</code> - AWS S3 integration (for cloud features)</li> <li><code>jupyter</code> - For notebook examples</li> </ul>"},{"location":"installation/#installation-methods","title":"Installation Methods","text":""},{"location":"installation/#1-stable-release-recommended","title":"1. Stable Release (Recommended)","text":"<p>Install the latest stable release from PyPI:</p> <pre><code>pip install hydroutils\n</code></pre> <p>This is the preferred method as it installs the most recent stable release with all dependencies.</p>"},{"location":"installation/#2-development-version","title":"2. Development Version","text":"<p>For the latest features and bug fixes, install from GitHub:</p> <pre><code>pip install git+https://github.com/zhuanglaihong/hydroutils.git\n</code></pre>"},{"location":"installation/#3-from-source","title":"3. From Source","text":"<p>If you want to contribute or modify the code:</p> <pre><code># Clone the repository\ngit clone https://github.com/zhuanglaihong/hydroutils.git\ncd hydroutils\n\n# Install in development mode\npip install -e .\n</code></pre>"},{"location":"installation/#4-with-optional-dependencies","title":"4. With Optional Dependencies","text":"<p>To install with all optional dependencies:</p> <pre><code>pip install hydroutils[all]\n</code></pre> <p>Or install specific optional dependencies:</p> <pre><code>pip install hydroutils[aws]     # For S3 functionality\npip install hydroutils[viz]     # For advanced visualization\npip install hydroutils[dev]     # For development tools\n</code></pre>"},{"location":"installation/#virtual-environment-setup","title":"Virtual Environment Setup","text":"<p>We recommend using a virtual environment to avoid dependency conflicts:</p>"},{"location":"installation/#using-conda","title":"Using conda","text":"<pre><code># Create a new environment\nconda create -n hydroutils python=3.10\nconda activate hydroutils\n\n# Install hydroutils\npip install hydroutils\n</code></pre>"},{"location":"installation/#using-venv","title":"Using venv","text":"<pre><code># Create a new environment\npython -m venv hydroutils-env\n\n# Activate the environment\n# On Windows:\nhydroutils-env\\Scripts\\activate\n# On macOS/Linux:\nsource hydroutils-env/bin/activate\n\n# Install hydroutils\npip install hydroutils\n</code></pre>"},{"location":"installation/#verification","title":"Verification","text":"<p>Test your installation:</p> <pre><code>import hydroutils as hu\nprint(hu.__version__)\n\n# Quick functionality test\nimport numpy as np\nobs = np.random.rand(100)\nsim = obs + np.random.normal(0, 0.1, 100)\nstats = hu.stat_error(obs, sim)\nprint(f\"NSE: {stats['NSE'][0]:.3f}\")\n</code></pre>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Import Error: Make sure all dependencies are installed:    <pre><code>pip install --upgrade hydroutils\n</code></pre></p> </li> <li> <p>Permission Denied: Use <code>--user</code> flag:    <pre><code>pip install --user hydroutils\n</code></pre></p> </li> <li> <p>SSL Certificate Error: Try with trusted hosts:    <pre><code>pip install --trusted-host pypi.org --trusted-host pypi.python.org hydroutils\n</code></pre></p> </li> </ol>"},{"location":"installation/#platform-specific-notes","title":"Platform-Specific Notes","text":"<p>Windows Users: - Consider using Anaconda for easier scientific package management - Some dependencies may require Visual C++ Build Tools</p> <p>macOS Users: - Xcode command line tools may be required for some dependencies - Use Homebrew to install system-level dependencies if needed</p> <p>Linux Users: - Install system dependencies for scientific packages:   <pre><code># Ubuntu/Debian\nsudo apt-get install python3-dev gfortran libopenblas-dev\n\n# CentOS/RHEL\nsudo yum install python3-devel gcc-gfortran openblas-devel\n</code></pre></p>"},{"location":"installation/#getting-help","title":"Getting Help","text":"<p>If you encounter any installation issues:</p> <ol> <li>Check the FAQ for common solutions</li> <li>Search existing issues</li> <li>Create a new issue with:</li> <li>Your operating system and Python version</li> <li>Complete error message</li> <li>Steps to reproduce the problem</li> </ol>"},{"location":"usage/","title":"Usage Guide","text":"<p>This guide provides practical examples of how to use <code>hydroutils</code> for common hydrological analysis tasks.</p>"},{"location":"usage/#getting-started","title":"Getting Started","text":"<pre><code>import hydroutils as hu\nimport numpy as np\nimport pandas as pd\n</code></pre>"},{"location":"usage/#1-statistical-analysis","title":"1. Statistical Analysis","text":""},{"location":"usage/#basic-hydrological-statistics","title":"Basic Hydrological Statistics","text":"<p>Calculate common hydrological performance metrics:</p> <pre><code># Sample data\nobserved = np.array([10.5, 12.3, 8.7, 15.2, 11.8, 9.4, 13.6])\nsimulated = np.array([10.1, 12.8, 8.9, 14.7, 11.2, 9.8, 13.1])\n\n# Calculate comprehensive statistics\nstats = hu.stat_error(observed, simulated)\n\nprint(f\"Nash-Sutcliffe Efficiency (NSE): {stats['NSE'][0]:.3f}\")\nprint(f\"Root Mean Square Error (RMSE): {stats['RMSE'][0]:.3f}\")\nprint(f\"Bias: {stats['Bias'][0]:.3f}\")\nprint(f\"Correlation: {stats['Corr'][0]:.3f}\")\nprint(f\"Kling-Gupta Efficiency (KGE): {stats['KGE'][0]:.3f}\")\n</code></pre>"},{"location":"usage/#kling-gupta-efficiency","title":"Kling-Gupta Efficiency","text":"<p>Calculate KGE individually:</p> <pre><code>kge_value = hu.KGE(simulated, observed)\nprint(f\"KGE: {kge_value:.3f}\")\n</code></pre>"},{"location":"usage/#flow-duration-curve-analysis","title":"Flow Duration Curve Analysis","text":"<pre><code># Calculate flow duration curve slope\nfms_value = hu.fms(observed, simulated, lower=0.2, upper=0.7)\nprint(f\"Flow Duration Curve Middle Slope: {fms_value:.3f}\")\n</code></pre>"},{"location":"usage/#2-time-series-processing","title":"2. Time Series Processing","text":""},{"location":"usage/#unit-conversions","title":"Unit Conversions","text":"<p>Convert between different streamflow units:</p> <pre><code># Convert cubic meters per second to cubic feet per second\nflow_cms = np.array([10.5, 12.3, 8.7, 15.2])\nflow_cfs = hu.streamflow_unit_conv(flow_cms, from_unit='cms', to_unit='cfs')\nprint(f\"Flow in CFS: {flow_cfs}\")\n\n# Detect time interval\ntime_series = pd.date_range('2020-01-01', periods=100, freq='D')\ninterval = hu.detect_time_interval(time_series)\nprint(f\"Detected interval: {interval}\")\n</code></pre>"},{"location":"usage/#time-interval-validation","title":"Time Interval Validation","text":"<pre><code># Validate unit compatibility\nis_compatible = hu.validate_unit_compatibility('cms', 'streamflow')\nprint(f\"CMS compatible with streamflow: {is_compatible}\")\n\n# Get time interval information\ninterval_info = hu.get_time_interval_info('1D')\nprint(f\"Daily interval info: {interval_info}\")\n</code></pre>"},{"location":"usage/#3-data-processing-with-files","title":"3. Data Processing with Files","text":""},{"location":"usage/#reading-and-processing-data","title":"Reading and Processing Data","text":"<pre><code># Example of processing a CSV file with hydrological data\ndata = pd.read_csv('streamflow_data.csv', parse_dates=['date'])\n\n# Calculate statistics for multiple stations\nstations = ['station_001', 'station_002', 'station_003']\nresults = {}\n\nfor station in stations:\n    if f'{station}_obs' in data.columns and f'{station}_sim' in data.columns:\n        obs = data[f'{station}_obs'].dropna()\n        sim = data[f'{station}_sim'].dropna()\n\n        # Align data\n        min_length = min(len(obs), len(sim))\n        obs = obs[:min_length]\n        sim = sim[:min_length]\n\n        results[station] = hu.stat_error(obs.values, sim.values)\n\n# Display results\nfor station, stats in results.items():\n    print(f\"\\n{station}:\")\n    print(f\"  NSE: {stats['NSE'][0]:.3f}\")\n    print(f\"  RMSE: {stats['RMSE'][0]:.3f}\")\n</code></pre>"},{"location":"usage/#4-advanced-statistical-analysis","title":"4. Advanced Statistical Analysis","text":""},{"location":"usage/#statistical-transformations","title":"Statistical Transformations","text":"<pre><code># Calculate statistical properties\nflow_data = np.random.lognormal(2, 1, 1000)  # Log-normal distributed flow\n\n# Basic statistics\nbasic_stats = hu.cal_stat(flow_data)\nprint(f\"Basic statistics: {basic_stats}\")\n\n# Gamma transformation statistics\ngamma_stats = hu.cal_stat_gamma(flow_data)\nprint(f\"Gamma-transformed statistics: {gamma_stats}\")\n\n# Four key statistical indices\nfour_stats = hu.cal_4_stat_inds(flow_data)\nprint(f\"P10, P90, Mean, Std: {four_stats}\")\n</code></pre>"},{"location":"usage/#empirical-cumulative-distribution-function","title":"Empirical Cumulative Distribution Function","text":"<pre><code># Calculate ECDF\nsorted_data, probabilities = hu.ecdf(flow_data)\n\n# Plot ECDF (requires matplotlib)\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(8, 6))\nplt.plot(sorted_data, probabilities)\nplt.xlabel('Flow')\nplt.ylabel('Probability')\nplt.title('Empirical Cumulative Distribution Function')\nplt.grid(True)\nplt.show()\n</code></pre>"},{"location":"usage/#5-working-with-multiple-time-series","title":"5. Working with Multiple Time Series","text":""},{"location":"usage/#batch-processing","title":"Batch Processing","text":"<pre><code># Process multiple time series\nobserved_series = np.random.rand(5, 100)  # 5 stations, 100 time steps\nsimulated_series = observed_series + np.random.normal(0, 0.1, (5, 100))\n\n# Calculate statistics for all series\nall_stats = hu.stat_errors(observed_series, simulated_series)\n\n# Extract NSE values for all stations\nnse_values = [stats['NSE'][0] for stats in all_stats]\nprint(f\"NSE values for all stations: {nse_values}\")\n</code></pre>"},{"location":"usage/#6-practical-example-complete-workflow","title":"6. Practical Example: Complete Workflow","text":"<p>Here's a complete example of a typical hydrological analysis workflow:</p> <pre><code>import hydroutils as hu\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 1. Load data\ndef load_sample_data():\n    \"\"\"Generate sample hydrological data\"\"\"\n    dates = pd.date_range('2020-01-01', '2022-12-31', freq='D')\n    # Simulate observed streamflow with seasonal pattern\n    base_flow = 10 + 5 * np.sin(2 * np.pi * np.arange(len(dates)) / 365.25)\n    observed = base_flow + np.random.normal(0, 2, len(dates))\n\n    # Simulate model predictions with some bias and error\n    simulated = observed * 0.95 + np.random.normal(0, 1.5, len(dates))\n\n    return pd.DataFrame({\n        'date': dates,\n        'observed': observed,\n        'simulated': simulated\n    })\n\n# 2. Load and prepare data\ndf = load_sample_data()\nprint(f\"Data shape: {df.shape}\")\nprint(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n\n# 3. Calculate comprehensive statistics\nstats = hu.stat_error(df['observed'].values, df['simulated'].values)\n\nprint(\"\\nPerformance Metrics:\")\nprint(f\"NSE: {stats['NSE'][0]:.3f}\")\nprint(f\"KGE: {stats['KGE'][0]:.3f}\")\nprint(f\"RMSE: {stats['RMSE'][0]:.3f}\")\nprint(f\"Bias: {stats['Bias'][0]:.3f}\")\nprint(f\"Correlation: {stats['Corr'][0]:.3f}\")\n\n# 4. Additional analysis\nkge_individual = hu.KGE(df['simulated'].values, df['observed'].values)\nprint(f\"KGE (individual calculation): {kge_individual:.3f}\")\n\n# 5. Unit conversion example\nflow_cfs = hu.streamflow_unit_conv(df['observed'].values, 'cms', 'cfs')\nprint(f\"Mean flow: {df['observed'].mean():.1f} cms = {flow_cfs.mean():.1f} cfs\")\n\n# 6. Visualization (optional)\nplt.figure(figsize=(12, 8))\n\nplt.subplot(2, 1, 1)\nplt.plot(df['date'], df['observed'], label='Observed', alpha=0.7)\nplt.plot(df['date'], df['simulated'], label='Simulated', alpha=0.7)\nplt.ylabel('Streamflow (cms)')\nplt.title('Time Series Comparison')\nplt.legend()\nplt.grid(True)\n\nplt.subplot(2, 1, 2)\nplt.scatter(df['observed'], df['simulated'], alpha=0.5)\nplt.plot([df['observed'].min(), df['observed'].max()], \n         [df['observed'].min(), df['observed'].max()], 'r--')\nplt.xlabel('Observed (cms)')\nplt.ylabel('Simulated (cms)')\nplt.title(f'Scatter Plot (NSE: {stats[\"NSE\"][0]:.3f})')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nAnalysis complete!\")\n</code></pre>"},{"location":"usage/#7-error-handling-and-best-practices","title":"7. Error Handling and Best Practices","text":""},{"location":"usage/#handling-missing-data","title":"Handling Missing Data","text":"<pre><code># Sample data with NaN values\nobs_with_nan = np.array([1.0, 2.0, np.nan, 4.0, 5.0])\nsim_with_nan = np.array([1.1, 2.2, 3.1, 4.2, np.nan])\n\n# The stat_error function automatically handles NaN values\ntry:\n    stats = hu.stat_error(obs_with_nan, sim_with_nan)\n    print(\"Statistics calculated successfully with NaN handling\")\nexcept Exception as e:\n    print(f\"Error: {e}\")\n</code></pre>"},{"location":"usage/#data-validation","title":"Data Validation","text":"<pre><code># Validate input data before analysis\ndef validate_data(observed, simulated):\n    \"\"\"Validate input data for hydrological analysis\"\"\"\n\n    if len(observed) != len(simulated):\n        raise ValueError(\"Observed and simulated data must have same length\")\n\n    if len(observed) == 0:\n        raise ValueError(\"Data arrays cannot be empty\")\n\n    valid_obs = ~np.isnan(observed)\n    valid_sim = ~np.isnan(simulated)\n    valid_both = valid_obs &amp; valid_sim\n\n    if np.sum(valid_both) &lt; 10:\n        print(\"Warning: Less than 10 valid data points\")\n\n    return valid_both\n\n# Example usage\nobs = np.random.rand(100)\nsim = obs + np.random.normal(0, 0.1, 100)\n\n# Add some NaN values\nobs[5:10] = np.nan\nsim[15:20] = np.nan\n\nvalid_mask = validate_data(obs, sim)\nprint(f\"Valid data points: {np.sum(valid_mask)}/{len(obs)}\")\n</code></pre>"},{"location":"usage/#next-steps","title":"Next Steps","text":"<ul> <li>Explore the complete API Reference for all available functions</li> <li>Check out specific module documentation for specialized features</li> <li>See Contributing Guidelines if you want to add new features</li> <li>Visit FAQ for common questions and troubleshooting</li> </ul>"},{"location":"api/hydro_event/","title":"hydro_event","text":"<p>The <code>hydro_event</code> module provides utilities for flood event extraction and analysis from hydrological time series data.</p>"},{"location":"api/hydro_event/#core-functions","title":"Core Functions","text":""},{"location":"api/hydro_event/#extract_flood_events","title":"extract_flood_events","text":"<pre><code>def extract_flood_events(\n    df: pd.DataFrame,\n    warmup_length: int = 0,\n    flood_event_col: str = \"flood_event\",\n    time_col: str = \"time\"\n) -&gt; List[Dict]\n</code></pre> <p>Extracts flood events from a DataFrame based on a binary flood event indicator.</p> <p>Args: - <code>df</code>: DataFrame with flood_event and time columns - <code>warmup_length</code>: Number of time steps to include as warmup period - <code>flood_event_col</code>: Name of flood event indicator column - <code>time_col</code>: Name of time column</p> <p>Returns: - List of flood event dictionaries containing event data and metadata</p> <p>Example: <pre><code>import hydroutils as hu\nimport pandas as pd\n\n# Sample data with flood events\ndf = pd.DataFrame({\n    'time': pd.date_range('2020-01-01', periods=10),\n    'flood_event': [0, 0, 1, 1, 1, 0, 0, 1, 1, 0],\n    'flow': [100, 120, 200, 300, 250, 150, 130, 180, 220, 140]\n})\n\n# Extract flood events\nevents = hu.extract_flood_events(df, warmup_length=1)\nprint(f\"Found {len(events)} flood events\")\n</code></pre></p>"},{"location":"api/hydro_event/#time_to_ten_digits","title":"time_to_ten_digits","text":"<pre><code>def time_to_ten_digits(time_obj) -&gt; str\n</code></pre> <p>Converts time objects to ten-digit format (YYYYMMDDHH).</p> <p>Example: <pre><code>from datetime import datetime\nimport hydroutils as hu\n\ndt = datetime(2020, 1, 1, 12, 0)\ntime_str = hu.time_to_ten_digits(dt)  # Returns '2020010112'\n</code></pre></p>"},{"location":"api/hydro_event/#extract_peaks","title":"extract_peaks","text":"<pre><code>def extract_peaks(\n    data: np.ndarray,\n    threshold: float = None,\n    min_distance: int = 1\n) -&gt; Tuple[np.ndarray, np.ndarray]\n</code></pre> <p>Extracts peak values and their indices from time series data.</p>"},{"location":"api/hydro_event/#calculate_event_statistics","title":"calculate_event_statistics","text":"<pre><code>def calculate_event_statistics(events: List[Dict]) -&gt; pd.DataFrame\n</code></pre> <p>Calculates statistical summary for extracted flood events.</p>"},{"location":"api/hydro_event/#api-reference","title":"API Reference","text":"<p>Author: Wenyu Ouyang Date: 2025-01-17 LastEditTime: 2025-08-17 09:29:42 LastEditors: Wenyu Ouyang Description: Flood event extraction utilities for hydrological data processing FilePath: \\hydromodeld:\\Code\\hydroutils\\hydroutils\\hydro_event.py Copyright (c) 2023-2026 Wenyu Ouyang. All rights reserved.</p>"},{"location":"api/hydro_event/#hydroutils.hydro_event.extract_event_data_by_columns","title":"<code>extract_event_data_by_columns(df, event_indices, data_columns)</code>","text":"<p>Extract event data for specified columns using event indices.</p> <p>This function extracts data from specified columns for a flood event using the index information from get_event_indices or extract_flood_events.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Original DataFrame containing all data.</p> required <code>event_indices</code> <code>Dict</code> <p>Event index information dictionary containing: - warmup_start_idx (int): Start index including warmup period - end_idx (int): End index of event</p> required <code>data_columns</code> <code>List[str]</code> <p>List of column names to extract.</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict</code> <p>Dictionary mapping column names to numpy arrays containing the extracted data. If a column is not found, it will contain an array of NaN values.</p> Example <p>df = pd.DataFrame({ ...     'time': pd.date_range('2020-01-01', periods=5), ...     'flow': [100, 200, 300, 250, 150] ... }) indices = {'warmup_start_idx': 1, 'end_idx': 4} data = extract_event_data_by_columns(df, indices, ['flow']) data['flow'] array([200., 300., 250.])</p> Source code in <code>hydroutils\\hydro_event.py</code> <pre><code>def extract_event_data_by_columns(\n    df: pd.DataFrame, event_indices: Dict, data_columns: List[str]\n) -&gt; Dict:\n    \"\"\"Extract event data for specified columns using event indices.\n\n    This function extracts data from specified columns for a flood event using\n    the index information from get_event_indices or extract_flood_events.\n\n    Args:\n        df (pd.DataFrame): Original DataFrame containing all data.\n        event_indices (Dict): Event index information dictionary containing:\n            - warmup_start_idx (int): Start index including warmup period\n            - end_idx (int): End index of event\n        data_columns (List[str]): List of column names to extract.\n\n    Returns:\n        Dict: Dictionary mapping column names to numpy arrays containing the\n            extracted data. If a column is not found, it will contain an array\n            of NaN values.\n\n    Example:\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     'time': pd.date_range('2020-01-01', periods=5),\n        ...     'flow': [100, 200, 300, 250, 150]\n        ... })\n        &gt;&gt;&gt; indices = {'warmup_start_idx': 1, 'end_idx': 4}\n        &gt;&gt;&gt; data = extract_event_data_by_columns(df, indices, ['flow'])\n        &gt;&gt;&gt; data['flow']\n        array([200., 300., 250.])\n    \"\"\"\n    start_idx = event_indices[\"warmup_start_idx\"]\n    end_idx = event_indices[\"end_idx\"]\n\n    event_data = {}\n    for col in data_columns:\n        if col in df.columns:\n            event_data[col] = df.iloc[start_idx:end_idx][col].values\n        else:\n            # \u5982\u679c\u5217\u4e0d\u5b58\u5728\uff0c\u7528NaN\u6570\u7ec4\u586b\u5145\n            event_data[col] = np.full(end_idx - start_idx, np.nan)\n\n    return event_data\n</code></pre>"},{"location":"api/hydro_event/#hydroutils.hydro_event.extract_flood_events","title":"<code>extract_flood_events(df, warmup_length=0, flood_event_col='flood_event', time_col='time')</code>","text":"<p>Extract flood events from a DataFrame based on a flood event indicator column.</p> <p>This function extracts flood events based on a binary indicator column (flood_event). The design philosophy is to be agnostic about other columns, letting the caller decide how to handle the data columns. The function only requires the flood_event column to mark events and a time column for event naming.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing site data. Must have flood_event and time columns.</p> required <code>warmup_length</code> <code>int</code> <p>Number of time steps to include as warmup period before each event. Defaults to 0.</p> <code>0</code> <code>flood_event_col</code> <code>str</code> <p>Name of the flood event indicator column. Defaults to \"flood_event\".</p> <code>'flood_event'</code> <code>time_col</code> <code>str</code> <p>Name of the time column. Defaults to \"time\".</p> <code>'time'</code> <p>Returns:</p> Type Description <code>List[Dict]</code> <p>List[Dict]: List of flood events. Each dictionary contains: - event_name (str): Event name based on start/end times - start_idx (int): Start index of actual event in original DataFrame - end_idx (int): End index of actual event in original DataFrame - warmup_start_idx (int): Start index including warmup period - data (pd.DataFrame): Event data including warmup period - is_warmup_mask (np.ndarray): Boolean array marking warmup rows - actual_start_time: Start time of actual event - actual_end_time: End time of actual event</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required columns are missing from DataFrame.</p> Example <p>df = pd.DataFrame({ ...     'time': pd.date_range('2020-01-01', periods=5), ...     'flood_event': [0, 1, 1, 1, 0], ...     'flow': [100, 200, 300, 250, 150] ... }) events = extract_flood_events(df, warmup_length=1) len(events) 1 events[0]['data']    time  flood_event  flow 0  2020-01-01    0  100  # warmup period 1  2020-01-02    1  200  # event start 2  2020-01-03    1  300 3  2020-01-04    1  250  # event end</p> Source code in <code>hydroutils\\hydro_event.py</code> <pre><code>def extract_flood_events(\n    df: pd.DataFrame,\n    warmup_length: int = 0,\n    flood_event_col: str = \"flood_event\",\n    time_col: str = \"time\",\n) -&gt; List[Dict]:\n    \"\"\"Extract flood events from a DataFrame based on a flood event indicator column.\n\n    This function extracts flood events based on a binary indicator column (flood_event).\n    The design philosophy is to be agnostic about other columns, letting the caller\n    decide how to handle the data columns. The function only requires the flood_event\n    column to mark events and a time column for event naming.\n\n    Args:\n        df (pd.DataFrame): DataFrame containing site data. Must have flood_event and\n            time columns.\n        warmup_length (int, optional): Number of time steps to include as warmup\n            period before each event. Defaults to 0.\n        flood_event_col (str, optional): Name of the flood event indicator column.\n            Defaults to \"flood_event\".\n        time_col (str, optional): Name of the time column. Defaults to \"time\".\n\n    Returns:\n        List[Dict]: List of flood events. Each dictionary contains:\n            - event_name (str): Event name based on start/end times\n            - start_idx (int): Start index of actual event in original DataFrame\n            - end_idx (int): End index of actual event in original DataFrame\n            - warmup_start_idx (int): Start index including warmup period\n            - data (pd.DataFrame): Event data including warmup period\n            - is_warmup_mask (np.ndarray): Boolean array marking warmup rows\n            - actual_start_time: Start time of actual event\n            - actual_end_time: End time of actual event\n\n    Raises:\n        ValueError: If required columns are missing from DataFrame.\n\n    Example:\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     'time': pd.date_range('2020-01-01', periods=5),\n        ...     'flood_event': [0, 1, 1, 1, 0],\n        ...     'flow': [100, 200, 300, 250, 150]\n        ... })\n        &gt;&gt;&gt; events = extract_flood_events(df, warmup_length=1)\n        &gt;&gt;&gt; len(events)\n        1\n        &gt;&gt;&gt; events[0]['data']\n           time  flood_event  flow\n        0  2020-01-01    0  100  # warmup period\n        1  2020-01-02    1  200  # event start\n        2  2020-01-03    1  300\n        3  2020-01-04    1  250  # event end\n    \"\"\"\n    events: List[Dict] = []\n\n    # \u68c0\u67e5\u5fc5\u8981\u7684\u5217\u662f\u5426\u5b58\u5728\n    required_cols = [flood_event_col, time_col]\n    missing_cols = [col for col in required_cols if col not in df.columns]\n    if missing_cols:\n        raise ValueError(f\"DataFrame\u7f3a\u5c11\u5fc5\u8981\u7684\u5217: {missing_cols}\")\n\n    # \u627e\u5230\u8fde\u7eed\u7684flood_event &gt; 0\u533a\u95f4\n    flood_mask = df[flood_event_col] &gt; 0\n    if not flood_mask.any():\n        return events\n\n    # \u627e\u8fde\u7eed\u533a\u95f4\n    in_event = False\n    start_idx = None\n\n    for idx, is_flood in enumerate(flood_mask):\n        if is_flood and not in_event:\n            start_idx = idx\n            in_event = True\n        elif not is_flood and in_event and start_idx is not None:\n            # \u4e8b\u4ef6\u7ed3\u675f\uff0c\u63d0\u53d6\u4e8b\u4ef6\u6570\u636e\n            event_dict = _extract_single_event(\n                df, start_idx, idx, warmup_length, flood_event_col, time_col\n            )\n            if event_dict is not None:\n                events.append(event_dict)\n            in_event = False\n\n    # \u5904\u7406\u6700\u540e\u4e00\u4e2a\u4e8b\u4ef6\uff08\u5982\u679c\u6570\u636e\u7ed3\u675f\u65f6\u4ecd\u5728\u4e8b\u4ef6\u4e2d\uff09\n    if in_event and start_idx is not None:\n        event_dict = _extract_single_event(\n            df, start_idx, len(df), warmup_length, flood_event_col, time_col\n        )\n        if event_dict is not None:\n            events.append(event_dict)\n\n    return events\n</code></pre>"},{"location":"api/hydro_event/#hydroutils.hydro_event.find_flood_event_segments_as_tuples","title":"<code>find_flood_event_segments_as_tuples(flood_event_array, warmup_length=0)</code>","text":"<p>Find continuous flood event segments and return them as tuples.</p> <p>This is a convenience function that returns event segments as tuples instead of dictionaries, for compatibility with existing code.</p> <p>Parameters:</p> Name Type Description Default <code>flood_event_array</code> <code>ndarray</code> <p>Binary array where values &gt; 0 indicate flood events.</p> required <code>warmup_length</code> <code>int</code> <p>Number of time steps to include as warmup period before each event. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>List[Tuple[int, int, int, int]]</code> <p>List[Tuple[int, int, int, int]]: List of tuples, each containing: (extended_start, extended_end, original_start, original_end) where: - extended_start: Start index including warmup period - extended_end: End index of event - original_start: Start index of actual event - original_end: End index of actual event</p> Example <p>arr = np.array([0, 0, 1, 1, 1, 0]) segments = find_flood_event_segments_as_tuples(arr, warmup_length=1) segments[0]  # (warmup_start, event_end, event_start, event_end) (1, 4, 2, 4)</p> Source code in <code>hydroutils\\hydro_event.py</code> <pre><code>def find_flood_event_segments_as_tuples(\n    flood_event_array: np.ndarray,\n    warmup_length: int = 0,\n) -&gt; List[Tuple[int, int, int, int]]:\n    \"\"\"Find continuous flood event segments and return them as tuples.\n\n    This is a convenience function that returns event segments as tuples instead\n    of dictionaries, for compatibility with existing code.\n\n    Args:\n        flood_event_array (np.ndarray): Binary array where values &gt; 0 indicate\n            flood events.\n        warmup_length (int, optional): Number of time steps to include as warmup\n            period before each event. Defaults to 0.\n\n    Returns:\n        List[Tuple[int, int, int, int]]: List of tuples, each containing:\n            (extended_start, extended_end, original_start, original_end)\n            where:\n            - extended_start: Start index including warmup period\n            - extended_end: End index of event\n            - original_start: Start index of actual event\n            - original_end: End index of actual event\n\n    Example:\n        &gt;&gt;&gt; arr = np.array([0, 0, 1, 1, 1, 0])\n        &gt;&gt;&gt; segments = find_flood_event_segments_as_tuples(arr, warmup_length=1)\n        &gt;&gt;&gt; segments[0]  # (warmup_start, event_end, event_start, event_end)\n        (1, 4, 2, 4)\n    \"\"\"\n    segments = find_flood_event_segments_from_array(flood_event_array, warmup_length)\n\n    return [\n        (\n            seg[\"extended_start\"],\n            seg[\"extended_end\"],\n            seg[\"original_start\"],\n            seg[\"original_end\"],\n        )\n        for seg in segments\n    ]\n</code></pre>"},{"location":"api/hydro_event/#hydroutils.hydro_event.find_flood_event_segments_from_array","title":"<code>find_flood_event_segments_from_array(flood_event_array, warmup_length=0)</code>","text":"<p>Find continuous flood event segments in a binary indicator array.</p> <p>This is a low-level function that handles the core logic of segmenting a flood event indicator array into continuous events. It can be reused by different higher-level functions.</p> <p>Parameters:</p> Name Type Description Default <code>flood_event_array</code> <code>ndarray</code> <p>Binary array where values &gt; 0 indicate flood events.</p> required <code>warmup_length</code> <code>int</code> <p>Number of time steps to include as warmup period before each event. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>List[Dict]</code> <p>List[Dict]: List of event segment information. Each dictionary contains: - extended_start (int): Start index including warmup period - extended_end (int): End index of event - original_start (int): Start index of actual event - original_end (int): End index of actual event - duration (int): Duration of actual event in time steps - total_length (int): Total length including warmup period</p> Example <p>arr = np.array([0, 0, 1, 1, 1, 0, 0, 1, 1, 0]) segments = find_flood_event_segments_from_array(arr, warmup_length=1) len(segments)  # Two events found 2 segments[0]  # First event with one timestep warmup {     'extended_start': 1,  # Warmup start     'extended_end': 4,    # Event end     'original_start': 2,  # Actual event start     'original_end': 4,    # Actual event end     'duration': 3,        # Event duration     'total_length': 4     # Total length with warmup }</p> Source code in <code>hydroutils\\hydro_event.py</code> <pre><code>def find_flood_event_segments_from_array(\n    flood_event_array: np.ndarray,\n    warmup_length: int = 0,\n) -&gt; List[Dict]:\n    \"\"\"Find continuous flood event segments in a binary indicator array.\n\n    This is a low-level function that handles the core logic of segmenting a\n    flood event indicator array into continuous events. It can be reused by\n    different higher-level functions.\n\n    Args:\n        flood_event_array (np.ndarray): Binary array where values &gt; 0 indicate\n            flood events.\n        warmup_length (int, optional): Number of time steps to include as warmup\n            period before each event. Defaults to 0.\n\n    Returns:\n        List[Dict]: List of event segment information. Each dictionary contains:\n            - extended_start (int): Start index including warmup period\n            - extended_end (int): End index of event\n            - original_start (int): Start index of actual event\n            - original_end (int): End index of actual event\n            - duration (int): Duration of actual event in time steps\n            - total_length (int): Total length including warmup period\n\n    Example:\n        &gt;&gt;&gt; arr = np.array([0, 0, 1, 1, 1, 0, 0, 1, 1, 0])\n        &gt;&gt;&gt; segments = find_flood_event_segments_from_array(arr, warmup_length=1)\n        &gt;&gt;&gt; len(segments)  # Two events found\n        2\n        &gt;&gt;&gt; segments[0]  # First event with one timestep warmup\n        {\n            'extended_start': 1,  # Warmup start\n            'extended_end': 4,    # Event end\n            'original_start': 2,  # Actual event start\n            'original_end': 4,    # Actual event end\n            'duration': 3,        # Event duration\n            'total_length': 4     # Total length with warmup\n        }\n    \"\"\"\n    segments = []\n\n    # \u627e\u5230\u6240\u6709 flood_event &gt; 0 \u7684\u7d22\u5f15\n    event_indices = np.where(flood_event_array &gt; 0)[0]\n\n    if len(event_indices) == 0:\n        return segments\n\n    # \u627e\u5230\u8fde\u7eed\u6bb5\u7684\u5206\u5272\u70b9\n    gaps = np.diff(event_indices) &gt; 1\n    split_points = np.where(gaps)[0] + 1\n    split_indices = np.split(event_indices, split_points)\n\n    # \u4e3a\u6bcf\u4e2a\u8fde\u7eed\u6bb5\u751f\u6210\u4fe1\u606f\n    for indices in split_indices:\n        if len(indices) &gt; 0:\n            original_start = indices[0]\n            original_end = indices[-1]\n\n            # \u6dfb\u52a0\u9884\u70ed\u671f\n            extended_start = max(0, original_start - warmup_length)\n\n            segments.append(\n                {\n                    \"extended_start\": extended_start,\n                    \"extended_end\": original_end,\n                    \"original_start\": original_start,\n                    \"original_end\": original_end,\n                    \"duration\": original_end - original_start + 1,\n                    \"total_length\": original_end - extended_start + 1,\n                }\n            )\n\n    return segments\n</code></pre>"},{"location":"api/hydro_event/#hydroutils.hydro_event.get_event_indices","title":"<code>get_event_indices(df, warmup_length=0, flood_event_col='flood_event')</code>","text":"<p>Get index information for flood events without extracting data.</p> <p>This function identifies flood events in the DataFrame and returns their index information, but does not extract the actual data. This is useful when you only need to know the locations and durations of events.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing site data.</p> required <code>warmup_length</code> <code>int</code> <p>Number of time steps to include as warmup period before each event. Defaults to 0.</p> <code>0</code> <code>flood_event_col</code> <code>str</code> <p>Name of flood event indicator column. Defaults to \"flood_event\".</p> <code>'flood_event'</code> <p>Returns:</p> Type Description <code>List[Dict]</code> <p>List[Dict]: List of event index information. Each dictionary contains: - start_idx (int): Start index of actual event - end_idx (int): End index of actual event - warmup_start_idx (int): Start index including warmup period - duration (int): Duration of actual event in time steps - total_length (int): Total length including warmup period</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If flood_event_col is not found in DataFrame.</p> Example <p>df = pd.DataFrame({'flood_event': [0, 1, 1, 1, 0]}) indices = get_event_indices(df, warmup_length=1) indices[0] {     'start_idx': 1,     'end_idx': 4,     'warmup_start_idx': 0,     'duration': 3,     'total_length': 4 }</p> Source code in <code>hydroutils\\hydro_event.py</code> <pre><code>def get_event_indices(\n    df: pd.DataFrame, warmup_length: int = 0, flood_event_col: str = \"flood_event\"\n) -&gt; List[Dict]:\n    \"\"\"Get index information for flood events without extracting data.\n\n    This function identifies flood events in the DataFrame and returns their index\n    information, but does not extract the actual data. This is useful when you only\n    need to know the locations and durations of events.\n\n    Args:\n        df (pd.DataFrame): DataFrame containing site data.\n        warmup_length (int, optional): Number of time steps to include as warmup\n            period before each event. Defaults to 0.\n        flood_event_col (str, optional): Name of flood event indicator column.\n            Defaults to \"flood_event\".\n\n    Returns:\n        List[Dict]: List of event index information. Each dictionary contains:\n            - start_idx (int): Start index of actual event\n            - end_idx (int): End index of actual event\n            - warmup_start_idx (int): Start index including warmup period\n            - duration (int): Duration of actual event in time steps\n            - total_length (int): Total length including warmup period\n\n    Raises:\n        ValueError: If flood_event_col is not found in DataFrame.\n\n    Example:\n        &gt;&gt;&gt; df = pd.DataFrame({'flood_event': [0, 1, 1, 1, 0]})\n        &gt;&gt;&gt; indices = get_event_indices(df, warmup_length=1)\n        &gt;&gt;&gt; indices[0]\n        {\n            'start_idx': 1,\n            'end_idx': 4,\n            'warmup_start_idx': 0,\n            'duration': 3,\n            'total_length': 4\n        }\n    \"\"\"\n    # \u68c0\u67e5\u5fc5\u8981\u7684\u5217\u662f\u5426\u5b58\u5728\n    if flood_event_col not in df.columns:\n        raise ValueError(f\"DataFrame\u7f3a\u5c11\u6d2a\u6c34\u4e8b\u4ef6\u6807\u8bb0\u5217: {flood_event_col}\")\n\n    # \u4f7f\u7528\u5e95\u5c42\u51fd\u6570\u5904\u7406\u5206\u5272\u903b\u8f91\n    flood_event_array = df[flood_event_col].values\n    segments = find_flood_event_segments_from_array(flood_event_array, warmup_length)\n\n    # \u8f6c\u6362\u4e3a\u4e0e\u539f\u63a5\u53e3\u517c\u5bb9\u7684\u683c\u5f0f\n    events = []\n    for seg in segments:\n        events.append(\n            {\n                \"start_idx\": seg[\"original_start\"],\n                \"end_idx\": seg[\"original_end\"] + 1,  # +1 \u56e0\u4e3a\u539f\u6765\u662f\u4e0d\u5305\u542b\u7ed3\u675f\u7d22\u5f15\u7684\n                \"warmup_start_idx\": seg[\"extended_start\"],\n                \"duration\": seg[\"duration\"],\n                \"total_length\": seg[\"total_length\"],\n            }\n        )\n\n    return events\n</code></pre>"},{"location":"api/hydro_event/#hydroutils.hydro_event.time_to_ten_digits","title":"<code>time_to_ten_digits(time_obj)</code>","text":"<p>Convert a time object to a ten-digit format YYYYMMDDHH.</p> <p>Parameters:</p> Name Type Description Default <code>time_obj</code> <code>Union[datetime, datetime64, str]</code> <p>Time object to convert. Can be datetime, numpy.datetime64, or string.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Ten-digit time string in YYYYMMDDHH format.</p> Example <p>time_to_ten_digits(datetime.datetime(2020, 1, 1, 12, 0)) '2020010112' time_to_ten_digits(np.datetime64('2020-01-01T12')) '2020010112' time_to_ten_digits('2020-01-01T12:00:00') '2020010112'</p> Source code in <code>hydroutils\\hydro_event.py</code> <pre><code>def time_to_ten_digits(time_obj) -&gt; str:\n    \"\"\"Convert a time object to a ten-digit format YYYYMMDDHH.\n\n    Args:\n        time_obj (Union[datetime.datetime, np.datetime64, str]): Time object to convert.\n            Can be datetime, numpy.datetime64, or string.\n\n    Returns:\n        str: Ten-digit time string in YYYYMMDDHH format.\n\n    Example:\n        &gt;&gt;&gt; time_to_ten_digits(datetime.datetime(2020, 1, 1, 12, 0))\n        '2020010112'\n        &gt;&gt;&gt; time_to_ten_digits(np.datetime64('2020-01-01T12'))\n        '2020010112'\n        &gt;&gt;&gt; time_to_ten_digits('2020-01-01T12:00:00')\n        '2020010112'\n    \"\"\"\n    if isinstance(time_obj, np.datetime64):\n        # \u5982\u679c\u662fnumpy datetime64\u5bf9\u8c61\n        return (\n            time_obj.astype(\"datetime64[h]\")\n            .astype(str)\n            .replace(\"-\", \"\")\n            .replace(\"T\", \"\")\n            .replace(\":\", \"\")\n        )\n    elif hasattr(time_obj, \"strftime\"):\n        # \u5982\u679c\u662fdatetime\u5bf9\u8c61\n        return time_obj.strftime(\"%Y%m%d%H\")\n    else:\n        # \u5982\u679c\u662f\u5b57\u7b26\u4e32\uff0c\u5c1d\u8bd5\u89e3\u6790\n        try:\n            if isinstance(time_obj, str):\n                dt = datetime.fromisoformat(time_obj.replace(\"Z\", \"+00:00\"))\n                return dt.strftime(\"%Y%m%d%H\")\n            else:\n                return \"0000000000\"  # \u9ed8\u8ba4\u503c\n        except Exception:\n            return \"0000000000\"  # \u9ed8\u8ba4\u503c\n</code></pre>"},{"location":"api/hydro_file/","title":"hydro_file","text":"<p>The <code>hydro_file</code> module provides utilities for file I/O operations, including reading and writing various data formats commonly used in hydrological applications.</p>"},{"location":"api/hydro_file/#core-functions","title":"Core Functions","text":""},{"location":"api/hydro_file/#read_ts_xrdataset","title":"read_ts_xrdataset","text":"<pre><code>def read_ts_xrdataset(\n    file_path: str,\n    var_name: str = None,\n    time_name: str = \"time\",\n    lat_name: str = \"lat\",\n    lon_name: str = \"lon\"\n) -&gt; xr.Dataset\n</code></pre> <p>Reads time series data from NetCDF files into xarray Dataset format.</p> <p>Example: <pre><code>import hydroutils as hu\n\n# Read NetCDF file\nds = hu.read_ts_xrdataset('data.nc', var_name='precipitation')\nprint(f\"Dataset shape: {ds.dims}\")\n</code></pre></p>"},{"location":"api/hydro_file/#write_ts_xrdataset","title":"write_ts_xrdataset","text":"<pre><code>def write_ts_xrdataset(\n    ds: xr.Dataset,\n    file_path: str,\n    var_name: str = None,\n    encoding: dict = None\n) -&gt; None\n</code></pre> <p>Writes xarray Dataset to NetCDF file.</p>"},{"location":"api/hydro_file/#read_csv","title":"read_csv","text":"<pre><code>def read_csv(file_path: str, **kwargs) -&gt; pd.DataFrame\n</code></pre> <p>Reads CSV files with enhanced error handling and encoding detection.</p>"},{"location":"api/hydro_file/#write_csv","title":"write_csv","text":"<pre><code>def write_csv(df: pd.DataFrame, file_path: str, **kwargs) -&gt; None\n</code></pre> <p>Writes DataFrame to CSV with proper encoding and error handling.</p>"},{"location":"api/hydro_file/#json-functions","title":"JSON Functions","text":""},{"location":"api/hydro_file/#serialize_json","title":"serialize_json","text":"<pre><code>def serialize_json(my_dict: dict, my_file: str) -&gt; None\n</code></pre> <p>Saves a dictionary to a JSON file.</p>"},{"location":"api/hydro_file/#unserialize_json","title":"unserialize_json","text":"<pre><code>def unserialize_json(my_file: str) -&gt; dict\n</code></pre> <p>Loads a JSON file into a dictionary.</p>"},{"location":"api/hydro_file/#serialize_json_np","title":"serialize_json_np","text":"<pre><code>def serialize_json_np(my_dict: dict, my_file: str) -&gt; None\n</code></pre> <p>Saves a dictionary containing NumPy arrays to a JSON file.</p>"},{"location":"api/hydro_file/#pickle-functions","title":"Pickle Functions","text":""},{"location":"api/hydro_file/#serialize_pickle","title":"serialize_pickle","text":"<pre><code>def serialize_pickle(my_object: object, my_file: str) -&gt; None\n</code></pre> <p>Saves an object to a pickle file.</p>"},{"location":"api/hydro_file/#unserialize_pickle","title":"unserialize_pickle","text":"<pre><code>def unserialize_pickle(my_file: str) -&gt; object\n</code></pre> <p>Loads an object from a pickle file.</p>"},{"location":"api/hydro_file/#numpy-array-functions","title":"NumPy Array Functions","text":""},{"location":"api/hydro_file/#serialize_numpy","title":"serialize_numpy","text":"<pre><code>def serialize_numpy(my_array: np.ndarray, my_file: str) -&gt; None\n</code></pre> <p>Saves a NumPy array to a .npy file.</p>"},{"location":"api/hydro_file/#unserialize_numpy","title":"unserialize_numpy","text":"<pre><code>def unserialize_numpy(my_file: str) -&gt; np.ndarray\n</code></pre> <p>Loads a NumPy array from a .npy file.</p>"},{"location":"api/hydro_file/#file-management-functions","title":"File Management Functions","text":""},{"location":"api/hydro_file/#get_lastest_file_in_a_dir","title":"get_lastest_file_in_a_dir","text":"<pre><code>def get_lastest_file_in_a_dir(dir_path: str) -&gt; str\n</code></pre> <p>Gets the most recently modified .pth file in a directory.</p>"},{"location":"api/hydro_file/#get_cache_dir","title":"get_cache_dir","text":"<pre><code>def get_cache_dir(app_name: str = \"hydro\") -&gt; str\n</code></pre> <p>Gets the appropriate cache directory for the current platform.</p>"},{"location":"api/hydro_file/#classes","title":"Classes","text":""},{"location":"api/hydro_file/#numpyarrayencoder","title":"NumpyArrayEncoder","text":"<pre><code>class NumpyArrayEncoder(json.JSONEncoder)\n</code></pre> <p>JSON encoder that handles NumPy arrays and scalars.</p>"},{"location":"api/hydro_file/#api-reference","title":"API Reference","text":"<p>Author: Wenyu Ouyang Date: 2024-08-15 10:08:59 LastEditTime: 2025-02-02 06:27:44 LastEditors: Wenyu Ouyang Description: some methods for file operations FilePath: \\hydroutils\\hydroutils\\hydro_file.py Copyright (c) 2023-2024 Wenyu Ouyang. All rights reserved.</p>"},{"location":"api/hydro_file/#hydroutils.hydro_file.NumpyArrayEncoder","title":"<code>NumpyArrayEncoder</code>","text":"<p>               Bases: <code>JSONEncoder</code></p> <p>JSON encoder that handles NumPy arrays and scalar types.</p> <p>This encoder converts NumPy arrays and scalar types to Python native types that can be serialized by the standard JSON encoder.</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>class NumpyArrayEncoder(json.JSONEncoder):\n    \"\"\"JSON encoder that handles NumPy arrays and scalar types.\n\n    This encoder converts NumPy arrays and scalar types to Python native types\n    that can be serialized by the standard JSON encoder.\n    \"\"\"\n\n    def default(self, obj):\n        \"\"\"Convert NumPy types to JSON serializable objects.\n\n        Args:\n            obj: Object to encode.\n\n        Returns:\n            JSON serializable object.\n        \"\"\"\n        if isinstance(obj, np.ndarray):\n            return self.convert_ndarray(obj)\n        elif isinstance(obj, (np.integer, np.floating)):\n            return obj.item()\n        return json.JSONEncoder.default(self, obj)\n\n    def convert_ndarray(self, array):\n        \"\"\"Convert a NumPy array to a nested list.\n\n        Args:\n            array (np.ndarray): NumPy array to convert.\n\n        Returns:\n            list or scalar: Python native type equivalent of the array.\n        \"\"\"\n        if array.ndim == 0:\n            return array.item()\n        return [\n            (\n                self.convert_ndarray(element)\n                if isinstance(element, np.ndarray)\n                else element\n            )\n            for element in array\n        ]\n</code></pre>"},{"location":"api/hydro_file/#hydroutils.hydro_file.NumpyArrayEncoder.convert_ndarray","title":"<code>convert_ndarray(array)</code>","text":"<p>Convert a NumPy array to a nested list.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>NumPy array to convert.</p> required <p>Returns:</p> Type Description <p>list or scalar: Python native type equivalent of the array.</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def convert_ndarray(self, array):\n    \"\"\"Convert a NumPy array to a nested list.\n\n    Args:\n        array (np.ndarray): NumPy array to convert.\n\n    Returns:\n        list or scalar: Python native type equivalent of the array.\n    \"\"\"\n    if array.ndim == 0:\n        return array.item()\n    return [\n        (\n            self.convert_ndarray(element)\n            if isinstance(element, np.ndarray)\n            else element\n        )\n        for element in array\n    ]\n</code></pre>"},{"location":"api/hydro_file/#hydroutils.hydro_file.NumpyArrayEncoder.default","title":"<code>default(obj)</code>","text":"<p>Convert NumPy types to JSON serializable objects.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <p>Object to encode.</p> required <p>Returns:</p> Type Description <p>JSON serializable object.</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def default(self, obj):\n    \"\"\"Convert NumPy types to JSON serializable objects.\n\n    Args:\n        obj: Object to encode.\n\n    Returns:\n        JSON serializable object.\n    \"\"\"\n    if isinstance(obj, np.ndarray):\n        return self.convert_ndarray(obj)\n    elif isinstance(obj, (np.integer, np.floating)):\n        return obj.item()\n    return json.JSONEncoder.default(self, obj)\n</code></pre>"},{"location":"api/hydro_file/#hydroutils.hydro_file.download_a_file_from_google_drive","title":"<code>download_a_file_from_google_drive(drive, dir_id, download_dir)</code>","text":"<p>Download files from Google Drive.</p> <p>Parameters:</p> Name Type Description Default <code>drive</code> <p>Google Drive API instance.</p> required <code>dir_id</code> <code>str</code> <p>ID of the Google Drive directory.</p> required <code>download_dir</code> <code>str</code> <p>Local directory to save downloaded files.</p> required <p>Returns:</p> Type Description <p>None</p> Note <p>Handles both files and folders recursively. Skips already downloaded files.</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def download_a_file_from_google_drive(drive, dir_id, download_dir):\n    \"\"\"Download files from Google Drive.\n\n    Args:\n        drive: Google Drive API instance.\n        dir_id (str): ID of the Google Drive directory.\n        download_dir (str): Local directory to save downloaded files.\n\n    Returns:\n        None\n\n    Note:\n        Handles both files and folders recursively.\n        Skips already downloaded files.\n    \"\"\"\n    file_list = drive.ListFile(\n        {\"q\": f\"'{dir_id}' in parents and trashed=false\"}\n    ).GetList()\n    for file in file_list:\n        print(f'title: {file[\"title\"]}, id: {file[\"id\"]}')\n        file_dl = drive.CreateFile({\"id\": file[\"id\"]})\n        print(f'mimetype is {file_dl[\"mimeType\"]}')\n        if file_dl[\"mimeType\"] == \"application/vnd.google-apps.folder\":\n            download_dir_sub = os.path.join(download_dir, file_dl[\"title\"])\n            if not os.path.isdir(download_dir_sub):\n                os.makedirs(download_dir_sub)\n            download_a_file_from_google_drive(drive, file_dl[\"id\"], download_dir_sub)\n        else:\n            # download\n            temp_file = os.path.join(download_dir, file_dl[\"title\"])\n            if os.path.isfile(temp_file):\n                print(\"file has been downloaded\")\n                continue\n            file_dl.GetContentFile(os.path.join(download_dir, file_dl[\"title\"]))\n            print(\"Downloading file finished\")\n</code></pre>"},{"location":"api/hydro_file/#hydroutils.hydro_file.download_excel","title":"<code>download_excel(data_url, temp_file)</code>","text":"<p>Download an Excel file from URL.</p> <p>Parameters:</p> Name Type Description Default <code>data_url</code> <code>str</code> <p>URL of the Excel file to download.</p> required <code>temp_file</code> <code>str</code> <p>Path where the Excel file will be saved.</p> required <p>Returns:</p> Type Description <p>None</p> Note <p>Only downloads if the file doesn't already exist locally.</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def download_excel(data_url, temp_file):\n    \"\"\"Download an Excel file from URL.\n\n    Args:\n        data_url (str): URL of the Excel file to download.\n        temp_file (str): Path where the Excel file will be saved.\n\n    Returns:\n        None\n\n    Note:\n        Only downloads if the file doesn't already exist locally.\n    \"\"\"\n    if not os.path.isfile(temp_file):\n        urllib.request.urlretrieve(data_url, temp_file)\n</code></pre>"},{"location":"api/hydro_file/#hydroutils.hydro_file.download_one_zip","title":"<code>download_one_zip(data_url, data_dir)</code>","text":"<p>Download one zip file from URL and extract it.</p> <p>Parameters:</p> Name Type Description Default <code>data_url</code> <code>str</code> <p>The URL of the file to download.</p> required <code>data_dir</code> <code>str</code> <p>Directory where the file will be downloaded and extracted.</p> required <p>Returns:</p> Type Description <p>None</p> Note <p>The function will create the target directory if it doesn't exist.</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def download_one_zip(data_url, data_dir):\n    \"\"\"Download one zip file from URL and extract it.\n\n    Args:\n        data_url (str): The URL of the file to download.\n        data_dir (str): Directory where the file will be downloaded and extracted.\n\n    Returns:\n        None\n\n    Note:\n        The function will create the target directory if it doesn't exist.\n    \"\"\"\n\n    zipfile_path, unzip_dir = zip_file_name_from_url(data_url, data_dir)\n    if not is_there_file(zipfile_path, unzip_dir):\n        if not os.path.isdir(unzip_dir):\n            os.makedirs(unzip_dir)\n        r = requests.get(data_url, stream=True)\n        with open(zipfile_path, \"wb\") as py_file:\n            for chunk in r.iter_content(chunk_size=1024):  # 1024 bytes\n                if chunk:\n                    py_file.write(chunk)\n        unzip_nested_zip(zipfile_path, unzip_dir), download_small_file\n</code></pre>"},{"location":"api/hydro_file/#hydroutils.hydro_file.download_small_file","title":"<code>download_small_file(data_url, temp_file)</code>","text":"<p>Download a small file from URL.</p> <p>Parameters:</p> Name Type Description Default <code>data_url</code> <code>str</code> <p>URL of the file to download.</p> required <code>temp_file</code> <code>str</code> <p>Path where the downloaded file will be saved.</p> required <p>Returns:</p> Type Description <p>None</p> Note <p>Uses requests library for downloading.</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def download_small_file(data_url, temp_file):\n    \"\"\"Download a small file from URL.\n\n    Args:\n        data_url (str): URL of the file to download.\n        temp_file (str): Path where the downloaded file will be saved.\n\n    Returns:\n        None\n\n    Note:\n        Uses requests library for downloading.\n    \"\"\"\n    r = requests.get(data_url)\n    with open(temp_file, \"w\") as f:\n        f.write(r.text)\n</code></pre>"},{"location":"api/hydro_file/#hydroutils.hydro_file.download_small_zip","title":"<code>download_small_zip(data_url, data_dir)</code>","text":"<p>Download a small zip file and extract it.</p> <p>Parameters:</p> Name Type Description Default <code>data_url</code> <code>str</code> <p>URL of the zip file to download.</p> required <code>data_dir</code> <code>str</code> <p>Directory where the file will be downloaded and extracted.</p> required <p>Returns:</p> Type Description <p>None</p> Note <p>Uses urllib.request for downloading small files.</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def download_small_zip(data_url, data_dir):\n    \"\"\"Download a small zip file and extract it.\n\n    Args:\n        data_url (str): URL of the zip file to download.\n        data_dir (str): Directory where the file will be downloaded and extracted.\n\n    Returns:\n        None\n\n    Note:\n        Uses urllib.request for downloading small files.\n    \"\"\"\n    zipfile_path, unzip_dir = zip_file_name_from_url(data_url, data_dir)\n    if not is_there_file(zipfile_path, unzip_dir):\n        if not os.path.isdir(unzip_dir):\n            os.mkdir(unzip_dir)\n        zipfile_path, _ = urllib.request.urlretrieve(data_url, zipfile_path)\n        unzip_nested_zip(zipfile_path, unzip_dir)\n</code></pre>"},{"location":"api/hydro_file/#hydroutils.hydro_file.download_zip_files","title":"<code>download_zip_files(urls, the_dir)</code>","text":"<p>Download multiple files from multiple URLs.</p> <p>Parameters:</p> Name Type Description Default <code>urls</code> <code>list</code> <p>List of URLs to download files from.</p> required <code>the_dir</code> <code>Path</code> <p>Directory where downloaded files will be stored.</p> required <p>Returns:</p> Type Description <p>None</p> Note <p>Uses a temporary directory for caching during download.</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def download_zip_files(urls, the_dir: Path):\n    \"\"\"Download multiple files from multiple URLs.\n\n    Args:\n        urls (list): List of URLs to download files from.\n        the_dir (Path): Directory where downloaded files will be stored.\n\n    Returns:\n        None\n\n    Note:\n        Uses a temporary directory for caching during download.\n    \"\"\"\n    with tempfile.TemporaryDirectory() as tmpdir:\n        cache_names = tmpdir.joinpath(f\"{the_dir.stem}.sqlite\")\n        r = ar.retrieve(urls, \"binary\", cache_name=cache_names, ssl=False)\n        files = [the_dir.joinpath(url.split(\"/\")[-1]) for url in urls]\n        [files[i].write_bytes(io.BytesIO(r[i]).getbuffer()) for i in range(len(files))]\n</code></pre>"},{"location":"api/hydro_file/#hydroutils.hydro_file.get_cache_dir","title":"<code>get_cache_dir(app_name='hydro')</code>","text":"<p>Get the appropriate cache directory for the current operating system.</p> <p>Parameters:</p> Name Type Description Default <code>app_name</code> <code>str</code> <p>Name of the application. Defaults to \"hydro\".</p> <code>'hydro'</code> <p>Returns:</p> Name Type Description <code>str</code> <p>Path to the cache directory.</p> Note <p>Creates the directory if it doesn't exist. Follows OS-specific conventions: - Windows: %LOCALAPPDATA%/app_name/Cache - macOS: ~/Library/Caches/app_name - Linux: ~/.cache/app_name</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def get_cache_dir(app_name=\"hydro\"):\n    \"\"\"Get the appropriate cache directory for the current operating system.\n\n    Args:\n        app_name (str, optional): Name of the application. Defaults to \"hydro\".\n\n    Returns:\n        str: Path to the cache directory.\n\n    Note:\n        Creates the directory if it doesn't exist.\n        Follows OS-specific conventions:\n        - Windows: %LOCALAPPDATA%/app_name/Cache\n        - macOS: ~/Library/Caches/app_name\n        - Linux: ~/.cache/app_name\n    \"\"\"\n    home = os.path.expanduser(\"~\")\n    system = platform.system()\n\n    if system == \"Windows\":\n        cache_dir = os.path.join(home, \"AppData\", \"Local\", app_name, \"Cache\")\n    elif system == \"Darwin\":\n        cache_dir = os.path.join(home, \"Library\", \"Caches\", app_name)\n    else:\n        cache_dir = os.path.join(home, \".cache\", app_name)\n\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    return cache_dir\n</code></pre>"},{"location":"api/hydro_file/#hydroutils.hydro_file.get_lastest_file_in_a_dir","title":"<code>get_lastest_file_in_a_dir(dir_path)</code>","text":"<p>Get the last file in a directory</p>"},{"location":"api/hydro_file/#hydroutils.hydro_file.get_lastest_file_in_a_dir--parameters","title":"Parameters","text":"<p>dir_path : str     the directory</p>"},{"location":"api/hydro_file/#hydroutils.hydro_file.get_lastest_file_in_a_dir--returns","title":"Returns","text":"<p>str     the path of the weight file</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def get_lastest_file_in_a_dir(dir_path):\n    \"\"\"Get the last file in a directory\n\n    Parameters\n    ----------\n    dir_path : str\n        the directory\n\n    Returns\n    -------\n    str\n        the path of the weight file\n    \"\"\"\n    pth_files_lst = [\n        os.path.join(dir_path, file)\n        for file in os.listdir(dir_path)\n        if fnmatch.fnmatch(file, \"*.pth\")\n    ]\n    return get_latest_file_in_a_lst(pth_files_lst)\n</code></pre>"},{"location":"api/hydro_file/#hydroutils.hydro_file.get_latest_file_in_a_lst","title":"<code>get_latest_file_in_a_lst(lst)</code>","text":"<p>Get the most recently modified file from a list of files.</p> <p>Parameters:</p> Name Type Description Default <code>lst</code> <code>list</code> <p>List of file paths.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>Path of the most recently modified file.</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def get_latest_file_in_a_lst(lst):\n    \"\"\"Get the most recently modified file from a list of files.\n\n    Args:\n        lst (list): List of file paths.\n\n    Returns:\n        str: Path of the most recently modified file.\n    \"\"\"\n    lst_ctime = [os.path.getctime(file) for file in lst]\n    sort_idx = np.argsort(lst_ctime)\n    return lst[sort_idx[-1]]\n</code></pre>"},{"location":"api/hydro_file/#hydroutils.hydro_file.is_there_file","title":"<code>is_there_file(zipfile_path, unzip_dir)</code>","text":"<p>if a file has existed</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def is_there_file(zipfile_path, unzip_dir):\n    \"\"\"if a file has existed\"\"\"\n    if os.path.isfile(zipfile_path):\n        if os.path.isdir(unzip_dir):\n            return True\n        unzip_nested_zip(zipfile_path, unzip_dir)\n        return True\n</code></pre>"},{"location":"api/hydro_file/#hydroutils.hydro_file.serialize_json","title":"<code>serialize_json(my_dict, my_file, encoding='utf-8', ensure_ascii=True)</code>","text":"<p>Serialize a dictionary to a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>my_dict</code> <code>dict</code> <p>Dictionary to serialize.</p> required <code>my_file</code> <code>str</code> <p>Path to the output JSON file.</p> required <code>encoding</code> <code>str</code> <p>File encoding. Defaults to \"utf-8\".</p> <code>'utf-8'</code> <code>ensure_ascii</code> <code>bool</code> <p>If True, ensure ASCII output. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def serialize_json(my_dict, my_file, encoding=\"utf-8\", ensure_ascii=True):\n    \"\"\"Serialize a dictionary to a JSON file.\n\n    Args:\n        my_dict (dict): Dictionary to serialize.\n        my_file (str): Path to the output JSON file.\n        encoding (str, optional): File encoding. Defaults to \"utf-8\".\n        ensure_ascii (bool, optional): If True, ensure ASCII output. Defaults to True.\n\n    Returns:\n        None\n    \"\"\"\n    with open(my_file, \"w\", encoding=encoding) as FP:\n        json.dump(my_dict, FP, ensure_ascii=ensure_ascii, indent=4)\n</code></pre>"},{"location":"api/hydro_file/#hydroutils.hydro_file.serialize_json_np","title":"<code>serialize_json_np(my_dict, my_file)</code>","text":"<p>Serialize a dictionary containing NumPy arrays to a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>my_dict</code> <code>dict</code> <p>Dictionary containing NumPy arrays to serialize.</p> required <code>my_file</code> <code>str</code> <p>Path to the output JSON file.</p> required <p>Returns:</p> Type Description <p>None</p> Note <p>Uses NumpyArrayEncoder to handle NumPy types.</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def serialize_json_np(my_dict, my_file):\n    \"\"\"Serialize a dictionary containing NumPy arrays to a JSON file.\n\n    Args:\n        my_dict (dict): Dictionary containing NumPy arrays to serialize.\n        my_file (str): Path to the output JSON file.\n\n    Returns:\n        None\n\n    Note:\n        Uses NumpyArrayEncoder to handle NumPy types.\n    \"\"\"\n    with open(my_file, \"w\") as FP:\n        json.dump(my_dict, FP, cls=NumpyArrayEncoder)\n</code></pre>"},{"location":"api/hydro_file/#hydroutils.hydro_file.serialize_numpy","title":"<code>serialize_numpy(my_array, my_file)</code>","text":"<p>Save a NumPy array to a binary file.</p> <p>Parameters:</p> Name Type Description Default <code>my_array</code> <code>ndarray</code> <p>NumPy array to save.</p> required <code>my_file</code> <code>str</code> <p>Path to the output file.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def serialize_numpy(my_array, my_file):\n    \"\"\"Save a NumPy array to a binary file.\n\n    Args:\n        my_array (np.ndarray): NumPy array to save.\n        my_file (str): Path to the output file.\n\n    Returns:\n        None\n    \"\"\"\n    np.save(my_file, my_array)\n</code></pre>"},{"location":"api/hydro_file/#hydroutils.hydro_file.serialize_pickle","title":"<code>serialize_pickle(my_object, my_file)</code>","text":"<p>Serialize an object to a pickle file.</p> <p>Parameters:</p> Name Type Description Default <code>my_object</code> <code>object</code> <p>Python object to serialize.</p> required <code>my_file</code> <code>str</code> <p>Path to the output pickle file.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def serialize_pickle(my_object, my_file):\n    \"\"\"Serialize an object to a pickle file.\n\n    Args:\n        my_object (object): Python object to serialize.\n        my_file (str): Path to the output pickle file.\n\n    Returns:\n        None\n    \"\"\"\n    with open(my_file, \"wb\") as f:\n        pickle.dump(my_object, f)\n</code></pre>"},{"location":"api/hydro_file/#hydroutils.hydro_file.unserialize_json","title":"<code>unserialize_json(my_file)</code>","text":"<p>Load a JSON file into a Python object.</p> <p>Parameters:</p> Name Type Description Default <code>my_file</code> <code>str</code> <p>Path to the JSON file to read.</p> required <p>Returns:</p> Name Type Description <code>object</code> <p>Python object (typically dict or list) loaded from the JSON file.</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def unserialize_json(my_file):\n    \"\"\"Load a JSON file into a Python object.\n\n    Args:\n        my_file (str): Path to the JSON file to read.\n\n    Returns:\n        object: Python object (typically dict or list) loaded from the JSON file.\n    \"\"\"\n    with open(my_file, \"r\") as fp:\n        my_object = json.load(fp)\n    return my_object\n</code></pre>"},{"location":"api/hydro_file/#hydroutils.hydro_file.unserialize_json_ordered","title":"<code>unserialize_json_ordered(my_file)</code>","text":"<p>Load a JSON file into an OrderedDict, preserving key order.</p> <p>Parameters:</p> Name Type Description Default <code>my_file</code> <code>str</code> <p>Path to the JSON file to read.</p> required <p>Returns:</p> Name Type Description <code>OrderedDict</code> <p>Dictionary with preserved key order from the JSON file.</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def unserialize_json_ordered(my_file):\n    \"\"\"Load a JSON file into an OrderedDict, preserving key order.\n\n    Args:\n        my_file (str): Path to the JSON file to read.\n\n    Returns:\n        OrderedDict: Dictionary with preserved key order from the JSON file.\n    \"\"\"\n    with open(my_file, \"r\") as fp:\n        m_dict = json.load(fp, object_pairs_hook=OrderedDict)\n    return m_dict\n</code></pre>"},{"location":"api/hydro_file/#hydroutils.hydro_file.unserialize_numpy","title":"<code>unserialize_numpy(my_file)</code>","text":"<p>Load a NumPy array from a binary file.</p> <p>Parameters:</p> Name Type Description Default <code>my_file</code> <code>str</code> <p>Path to the NumPy array file.</p> required <p>Returns:</p> Type Description <p>np.ndarray: NumPy array loaded from the file.</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def unserialize_numpy(my_file):\n    \"\"\"Load a NumPy array from a binary file.\n\n    Args:\n        my_file (str): Path to the NumPy array file.\n\n    Returns:\n        np.ndarray: NumPy array loaded from the file.\n    \"\"\"\n    return np.load(my_file)\n</code></pre>"},{"location":"api/hydro_file/#hydroutils.hydro_file.unserialize_pickle","title":"<code>unserialize_pickle(my_file)</code>","text":"<p>Load an object from a pickle file.</p> <p>Parameters:</p> Name Type Description Default <code>my_file</code> <code>str</code> <p>Path to the pickle file to read.</p> required <p>Returns:</p> Name Type Description <code>object</code> <p>Python object loaded from the pickle file.</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def unserialize_pickle(my_file):\n    \"\"\"Load an object from a pickle file.\n\n    Args:\n        my_file (str): Path to the pickle file to read.\n\n    Returns:\n        object: Python object loaded from the pickle file.\n    \"\"\"\n    with open(my_file, \"rb\") as f:\n        my_object = pickle.load(f)\n    return my_object\n</code></pre>"},{"location":"api/hydro_file/#hydroutils.hydro_file.unzip_file","title":"<code>unzip_file(data_zip, path_unzip)</code>","text":"<p>Extract a zip file to the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>data_zip</code> <code>str</code> <p>Path to the zip file to extract.</p> required <code>path_unzip</code> <code>str</code> <p>Directory where the contents will be extracted.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def unzip_file(data_zip, path_unzip):\n    \"\"\"Extract a zip file to the specified directory.\n\n    Args:\n        data_zip (str): Path to the zip file to extract.\n        path_unzip (str): Directory where the contents will be extracted.\n\n    Returns:\n        None\n    \"\"\"\n    with zipfile.ZipFile(data_zip, \"r\") as zip_temp:\n        zip_temp.extractall(path_unzip)\n</code></pre>"},{"location":"api/hydro_file/#hydroutils.hydro_file.unzip_nested_zip","title":"<code>unzip_nested_zip(dataset_zip, path_unzip)</code>","text":"<p>Extract a zip file including any nested zip files If a file's name is \"xxx_\", it seems the \"extractall\" function in the \"zipfile\" lib will throw an OSError, so please check the unzipped files manually when this occurs. Parameters</p> <p>dataset_zip: the zip file path_unzip: where it is unzipped</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def unzip_nested_zip(dataset_zip, path_unzip):\n    \"\"\"\n    Extract a zip file including any nested zip files\n    If a file's name is \"xxx_\", it seems the \"extractall\" function in the \"zipfile\" lib will throw an OSError,\n    so please check the unzipped files manually when this occurs.\n    Parameters\n    ----------\n    dataset_zip: the zip file\n    path_unzip: where it is unzipped\n    \"\"\"\n\n    with zipfile.ZipFile(dataset_zip, \"r\") as zfile:\n        try:\n            zfile.extractall(path=path_unzip)\n        except OSError as e:\n            logging.warning(\n                \"Please check the unzipped files manually. There may be some missed important files.\"\n            )\n            logging.warning(f\"The directory is: {path_unzip}\")\n            logging.warning(f\"Error message: {e}\")\n    for root, dirs, files in os.walk(path_unzip):\n        for filename in files:\n            if re.search(r\"\\.zip$\", filename):\n                file_spec = os.path.join(root, filename)\n                new_dir = os.path.join(root, filename[:-4])\n                unzip_nested_zip(file_spec, new_dir)\n</code></pre>"},{"location":"api/hydro_file/#hydroutils.hydro_file.zip_extract","title":"<code>zip_extract(the_dir)</code>","text":"<p>Extract the downloaded zip files in the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>the_dir</code> <code>Path</code> <p>The directory containing zip files to extract.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def zip_extract(the_dir) -&gt; None:\n    \"\"\"Extract the downloaded zip files in the specified directory.\n\n    Args:\n        the_dir (Path): The directory containing zip files to extract.\n\n    Returns:\n        None\n    \"\"\"\n    for f in the_dir.glob(\"*.zip\"):\n        with zipfile.ZipFile(f) as zf:\n            # extract files to a directory named by f.stem\n            zf.extractall(the_dir.joinpath(f.stem))\n</code></pre>"},{"location":"api/hydro_log/","title":"hydro_log","text":"<p>The <code>hydro_log</code> module provides logging utilities with rich formatting and file/console output capabilities.</p>"},{"location":"api/hydro_log/#classes","title":"Classes","text":""},{"location":"api/hydro_log/#hydrowarning","title":"HydroWarning","text":"<p>A class for handling and displaying hydrology-related warnings and messages with rich formatting.</p> <pre><code>class HydroWarning:\n    def __init__(self)\n    def no_directory(directory_name: str, message: Text = None) -&gt; None\n    def file_not_found(file_name: str, message: Text = None) -&gt; None\n    def operation_successful(operation_detail: str, message: Text = None) -&gt; None\n</code></pre> <p>Example: <pre><code>from hydroutils import HydroWarning\n\nwarning = HydroWarning()\n\n# Display directory not found warning\nwarning.no_directory(\"/path/to/missing/dir\")\n\n# Display file not found warning\nwarning.file_not_found(\"data.csv\")\n\n# Display success message\nwarning.operation_successful(\"Data processing complete\")\n</code></pre></p>"},{"location":"api/hydro_log/#decorators","title":"Decorators","text":""},{"location":"api/hydro_log/#hydro_logger","title":"@hydro_logger","text":"<p>A class decorator that adds logging capabilities to a class.</p> <p>Example: <pre><code>from hydroutils import hydro_logger\n\n@hydro_logger\nclass MyHydroClass:\n    def process_data(self):\n        self.logger.info(\"Starting data processing...\")\n        # Processing logic here\n        self.logger.debug(\"Processing complete\")\n\n# The class now has both file and console logging\nobj = MyHydroClass()\nobj.process_data()  # Logs will be written to file and console\n</code></pre></p> <p>Features: - Automatically creates log directory in cache - Timestamps in log filenames - Both file (DEBUG level) and console (INFO level) output - Standard logging format with timestamp, module name, and log level</p>"},{"location":"api/hydro_log/#api-reference","title":"API Reference","text":"<p>Author: Wenyu Ouyang Date: 2023-10-25 20:07:14 LastEditTime: 2025-01-15 11:55:24 LastEditors: Wenyu Ouyang Description: Use rich to log: https://rich.readthedocs.io/en/latest/ FilePath: \\hydroutils\\hydroutils\\hydro_log.py Copyright (c) 2023-2024 Wenyu Ouyang. All rights reserved.</p>"},{"location":"api/hydro_log/#hydroutils.hydro_log.HydroWarning","title":"<code>HydroWarning</code>","text":"<p>A class for displaying formatted warning messages using Rich console.</p> <p>This class provides methods for displaying different types of warning messages with consistent formatting and color coding using the Rich library.</p> <p>Attributes:</p> Name Type Description <code>console</code> <code>Console</code> <p>Rich console instance for formatted output.</p> Source code in <code>hydroutils\\hydro_log.py</code> <pre><code>class HydroWarning:\n    \"\"\"A class for displaying formatted warning messages using Rich console.\n\n    This class provides methods for displaying different types of warning messages\n    with consistent formatting and color coding using the Rich library.\n\n    Attributes:\n        console (Console): Rich console instance for formatted output.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize HydroWarning with a Rich console instance.\"\"\"\n        self.console = Console()\n\n    def no_directory(self, directory_name, message=None):\n        \"\"\"Display a warning message for a missing directory.\n\n        Args:\n            directory_name (str): Name of the missing directory.\n            message (Text, optional): Custom warning message. If None, a default\n                message will be created. Defaults to None.\n        \"\"\"\n        if message is None:\n            message = Text(\n                f\"There is no such directory: {directory_name}\", style=\"bold red\"\n            )\n        self.console.print(message)\n\n    def file_not_found(self, file_name, message=None):\n        \"\"\"Display a warning message for a file that cannot be found.\n\n        Args:\n            file_name (str): Name of the file that could not be found.\n            message (Text, optional): Custom warning message. If None, a default\n                message will be created. Defaults to None.\n        \"\"\"\n        if message is None:\n            message = Text(\n                f\"We didn't find this file: {file_name}\", style=\"bold yellow\"\n            )\n        self.console.print(message)\n\n    def operation_successful(self, operation_detail, message=None):\n        \"\"\"Display a success message for a completed operation.\n\n        Args:\n            operation_detail (str): Description of the successful operation.\n            message (Text, optional): Custom success message. If None, a default\n                message will be created. Defaults to None.\n        \"\"\"\n        if message is None:\n            message = Text(f\"Operation Success: {operation_detail}\", style=\"bold green\")\n        self.console.print(message)\n</code></pre>"},{"location":"api/hydro_log/#hydroutils.hydro_log.HydroWarning.__init__","title":"<code>__init__()</code>","text":"<p>Initialize HydroWarning with a Rich console instance.</p> Source code in <code>hydroutils\\hydro_log.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize HydroWarning with a Rich console instance.\"\"\"\n    self.console = Console()\n</code></pre>"},{"location":"api/hydro_log/#hydroutils.hydro_log.HydroWarning.file_not_found","title":"<code>file_not_found(file_name, message=None)</code>","text":"<p>Display a warning message for a file that cannot be found.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>Name of the file that could not be found.</p> required <code>message</code> <code>Text</code> <p>Custom warning message. If None, a default message will be created. Defaults to None.</p> <code>None</code> Source code in <code>hydroutils\\hydro_log.py</code> <pre><code>def file_not_found(self, file_name, message=None):\n    \"\"\"Display a warning message for a file that cannot be found.\n\n    Args:\n        file_name (str): Name of the file that could not be found.\n        message (Text, optional): Custom warning message. If None, a default\n            message will be created. Defaults to None.\n    \"\"\"\n    if message is None:\n        message = Text(\n            f\"We didn't find this file: {file_name}\", style=\"bold yellow\"\n        )\n    self.console.print(message)\n</code></pre>"},{"location":"api/hydro_log/#hydroutils.hydro_log.HydroWarning.no_directory","title":"<code>no_directory(directory_name, message=None)</code>","text":"<p>Display a warning message for a missing directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory_name</code> <code>str</code> <p>Name of the missing directory.</p> required <code>message</code> <code>Text</code> <p>Custom warning message. If None, a default message will be created. Defaults to None.</p> <code>None</code> Source code in <code>hydroutils\\hydro_log.py</code> <pre><code>def no_directory(self, directory_name, message=None):\n    \"\"\"Display a warning message for a missing directory.\n\n    Args:\n        directory_name (str): Name of the missing directory.\n        message (Text, optional): Custom warning message. If None, a default\n            message will be created. Defaults to None.\n    \"\"\"\n    if message is None:\n        message = Text(\n            f\"There is no such directory: {directory_name}\", style=\"bold red\"\n        )\n    self.console.print(message)\n</code></pre>"},{"location":"api/hydro_log/#hydroutils.hydro_log.HydroWarning.operation_successful","title":"<code>operation_successful(operation_detail, message=None)</code>","text":"<p>Display a success message for a completed operation.</p> <p>Parameters:</p> Name Type Description Default <code>operation_detail</code> <code>str</code> <p>Description of the successful operation.</p> required <code>message</code> <code>Text</code> <p>Custom success message. If None, a default message will be created. Defaults to None.</p> <code>None</code> Source code in <code>hydroutils\\hydro_log.py</code> <pre><code>def operation_successful(self, operation_detail, message=None):\n    \"\"\"Display a success message for a completed operation.\n\n    Args:\n        operation_detail (str): Description of the successful operation.\n        message (Text, optional): Custom success message. If None, a default\n            message will be created. Defaults to None.\n    \"\"\"\n    if message is None:\n        message = Text(f\"Operation Success: {operation_detail}\", style=\"bold green\")\n    self.console.print(message)\n</code></pre>"},{"location":"api/hydro_log/#hydroutils.hydro_log.hydro_logger","title":"<code>hydro_logger(cls)</code>","text":"<p>Class decorator that adds a configured logger to the decorated class.</p> <p>This decorator sets up a logger with both file and console handlers. The file handler writes all logs (DEBUG and above) to a timestamped file in the cache directory, while the console handler shows INFO and above messages.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <p>The class to be decorated.</p> required <p>Returns:</p> Type Description <p>The decorated class with an added logger attribute.</p> Example <p>@hydro_logger class MyClass:     def my_method(self):         self.logger.info(\"This will be logged\")</p> Source code in <code>hydroutils\\hydro_log.py</code> <pre><code>def hydro_logger(cls):\n    \"\"\"Class decorator that adds a configured logger to the decorated class.\n\n    This decorator sets up a logger with both file and console handlers. The file\n    handler writes all logs (DEBUG and above) to a timestamped file in the cache\n    directory, while the console handler shows INFO and above messages.\n\n    Args:\n        cls: The class to be decorated.\n\n    Returns:\n        The decorated class with an added logger attribute.\n\n    Example:\n        @hydro_logger\n        class MyClass:\n            def my_method(self):\n                self.logger.info(\"This will be logged\")\n    \"\"\"\n    # Use the class name as the logger name\n    logger_name = f\"{cls.__module__}.{cls.__name__}\"\n    logger = logging.getLogger(logger_name)\n    logger.setLevel(logging.DEBUG)\n    cache_dir = get_cache_dir()\n    log_dir = os.path.join(cache_dir, \"logs\")\n    if not os.path.exists(log_dir):\n        os.makedirs(log_dir)\n    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    log_file = os.path.join(log_dir, f\"{logger_name}_{current_time}.log\")\n    # Check if handlers have already been added to avoid duplication\n    if not logger.handlers:\n        # Create a file handler to write logs to the specified file\n        file_handler = logging.FileHandler(log_file)\n        file_handler.setLevel(logging.DEBUG)\n\n        # Create a console handler to output logs to the console (optional)\n        console_handler = logging.StreamHandler()\n        console_handler.setLevel(logging.INFO)\n\n        # set the format of the log\n        formatter = logging.Formatter(\n            \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n        )\n        file_handler.setFormatter(formatter)\n        console_handler.setFormatter(formatter)\n\n        # Add handlers to the logger\n        logger.addHandler(file_handler)\n        logger.addHandler(console_handler)\n\n    # Bind the logger to the class attribute\n    cls.logger = logger\n    return cls\n</code></pre>"},{"location":"api/hydro_plot/","title":"hydro_plot - Visualization Tools","text":"<p>The <code>hydro_plot</code> module provides specialized plotting functions for visualizing hydrological data and analysis results.</p>"},{"location":"api/hydro_plot/#overview","title":"Overview","text":"<p>This module contains functions for:</p> <ul> <li>Time Series Plots: Visualize streamflow, precipitation, and other time series data</li> <li>Statistical Plots: Create plots for model performance assessment</li> <li>Flow Analysis Plots: Flow duration curves, hydrographs, and flow statistics</li> <li>Comparison Plots: Side-by-side comparisons of observed vs simulated data</li> </ul>"},{"location":"api/hydro_plot/#quick-example","title":"Quick Example","text":"<pre><code>import hydroutils as hu\nimport pandas as pd\nimport numpy as np\n\n# Sample data\ndates = pd.date_range('2020-01-01', '2020-12-31', freq='D')\nobserved = 10 + 5 * np.sin(2 * np.pi * np.arange(len(dates)) / 365.25)\nsimulated = observed * 0.95 + np.random.normal(0, 1.5, len(dates))\n\n# Create time series plot\nfig, ax = hu.plot_timeseries(\n    dates, observed, simulated,\n    labels=['Observed', 'Simulated'],\n    title='Streamflow Comparison'\n)\n\n# Create performance scatter plot\nfig, ax = hu.plot_scatter_performance(\n    observed, simulated,\n    add_stats=True,  # Add NSE, R\u00b2, etc.\n    add_1to1_line=True\n)\n</code></pre>"},{"location":"api/hydro_plot/#api-reference","title":"API Reference","text":"<p>Author: Wenyu Ouyang Date: 2022-12-02 10:59:30 LastEditTime: 2025-08-02 11:58:30 LastEditors: Wenyu Ouyang Description: Some common plots for hydrology FilePath: \\hydroutils\\hydroutils\\hydro_plot.py Copyright (c) 2021-2022 Wenyu Ouyang. All rights reserved.</p>"},{"location":"api/hydro_plot/#hydroutils.hydro_plot.create_median_labels","title":"<code>create_median_labels(ax, medians_value, percent25value=None, percent75value=None, size='small')</code>","text":"<p>\"create median labels for boxes in a boxplot Parameters</p> <p>ax : plt.AxesSubplot     an ax in a fig medians_value : np.array     description percent25value : type, optional     description, by default None percent75value : type, optional     description, by default None size : str, optional     the size of median-value labels, by default small</p> Source code in <code>hydroutils\\hydro_plot.py</code> <pre><code>def create_median_labels(\n    ax, medians_value, percent25value=None, percent75value=None, size=\"small\"\n):\n    \"\"\" \"create median labels for boxes in a boxplot\n    Parameters\n    ----------\n    ax : plt.AxesSubplot\n        an ax in a fig\n    medians_value : np.array\n        _description_\n    percent25value : _type_, optional\n        _description_, by default None\n    percent75value : _type_, optional\n        _description_, by default None\n    size : str, optional\n        the size of median-value labels, by default small\n    \"\"\"\n    decimal_places = \"2\"\n    if percent25value is None or percent75value is None:\n        vertical_offset = np.min(medians_value * 0.01)  # offset from median for display\n    else:\n        per25min = np.min(percent25value)\n        per75max = np.max(percent75value)\n        vertical_offset = (per75max - per25min) * 0.01\n    median_labels = [format(s, f\".{decimal_places}f\") for s in medians_value]\n    pos = range(len(medians_value))\n    for xtick in ax.get_xticks():\n        ax.text(\n            pos[xtick],\n            medians_value[xtick] + vertical_offset,\n            median_labels[xtick],\n            horizontalalignment=\"center\",\n            color=\"w\",\n            # https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.text.html\n            size=size,\n            weight=\"semibold\",\n        )\n</code></pre>"},{"location":"api/hydro_plot/#hydroutils.hydro_plot.plot_boxes_matplotlib","title":"<code>plot_boxes_matplotlib(data, label1=None, label2=None, leg_col=None, colorlst='rbgcmywrbgcmyw', title=None, figsize=(8, 6), sharey=False, xticklabel=None, axin=None, ylim=None, ylabel=None, notch=False, widths=0.5, subplots_adjust_wspace=0.2, show_median=True, median_line_color='black', median_font_size='small')</code>","text":"<p>Create multiple boxplots for comparing multiple indicators.</p> <p>This function creates a figure with multiple boxplots, each representing a different indicator. It supports customization of appearance, labels, and layout, and can display median values and other statistics.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>list</code> <p>List of data arrays, each element represents one indicator and can contain multiple numpy arrays for box comparison.</p> required <code>label1</code> <code>list</code> <p>Names for each subplot. Defaults to None.</p> <code>None</code> <code>label2</code> <code>list</code> <p>Legend names for boxes within each subplot. Same across all subplots. Defaults to None.</p> <code>None</code> <code>leg_col</code> <code>int</code> <p>Number of columns in the legend. Defaults to None.</p> <code>None</code> <code>colorlst</code> <code>str</code> <p>String of color characters for boxes. Defaults to \"rbgcmywrbgcmyw\".</p> <code>'rbgcmywrbgcmyw'</code> <code>title</code> <code>str</code> <p>Figure title. Defaults to None.</p> <code>None</code> <code>figsize</code> <code>tuple</code> <p>Figure size as (width, height). Defaults to (8, 6).</p> <code>(8, 6)</code> <code>sharey</code> <code>bool</code> <p>If True, all subplots share y-axis scale. Defaults to False.</p> <code>False</code> <code>xticklabel</code> <code>list</code> <p>Labels for x-axis ticks. Defaults to None.</p> <code>None</code> <code>axin</code> <code>Axes</code> <p>Existing axes to plot on. Defaults to None.</p> <code>None</code> <code>ylim</code> <code>list</code> <p>Y-axis limits [min, max]. Defaults to None.</p> <code>None</code> <code>ylabel</code> <code>list</code> <p>Y-axis labels for each subplot. Defaults to None.</p> <code>None</code> <code>notch</code> <code>bool</code> <p>If True, boxes will have notches. Defaults to False.</p> <code>False</code> <code>widths</code> <code>float</code> <p>Width of the boxes. Defaults to 0.5.</p> <code>0.5</code> <code>subplots_adjust_wspace</code> <code>float</code> <p>Width space between subplots. Defaults to 0.2.</p> <code>0.2</code> <code>show_median</code> <code>bool</code> <p>If True, show median values above boxes. Defaults to True.</p> <code>True</code> <code>median_line_color</code> <code>str</code> <p>Color of median lines. Defaults to \"black\".</p> <code>'black'</code> <code>median_font_size</code> <code>str</code> <p>Font size for median values. Defaults to \"small\".</p> <code>'small'</code> <p>Returns:</p> Type Description <p>Union[plt.Figure, Tuple[plt.Axes, dict]]: If axin is None, returns the</p> <p>figure object. Otherwise, returns a tuple of (axes, boxplot_dict).</p> Example <p>data = [np.random.normal(0, 1, 100), np.random.normal(2, 1, 100)] fig = plot_boxes_matplotlib(data, ...                           label1=['Group A'], ...                           label2=['Sample 1', 'Sample 2'], ...                           show_median=True)</p> Source code in <code>hydroutils\\hydro_plot.py</code> <pre><code>def plot_boxes_matplotlib(\n    data: list,\n    label1: list = None,\n    label2: list = None,\n    leg_col: int = None,\n    colorlst=\"rbgcmywrbgcmyw\",\n    title=None,\n    figsize=(8, 6),\n    sharey=False,\n    xticklabel=None,\n    axin=None,\n    ylim=None,\n    ylabel=None,\n    notch=False,\n    widths=0.5,\n    subplots_adjust_wspace=0.2,\n    show_median=True,\n    median_line_color=\"black\",\n    median_font_size=\"small\",\n):\n    \"\"\"Create multiple boxplots for comparing multiple indicators.\n\n    This function creates a figure with multiple boxplots, each representing a\n    different indicator. It supports customization of appearance, labels, and\n    layout, and can display median values and other statistics.\n\n    Args:\n        data (list): List of data arrays, each element represents one indicator\n            and can contain multiple numpy arrays for box comparison.\n        label1 (list, optional): Names for each subplot. Defaults to None.\n        label2 (list, optional): Legend names for boxes within each subplot.\n            Same across all subplots. Defaults to None.\n        leg_col (int, optional): Number of columns in the legend. Defaults to None.\n        colorlst (str, optional): String of color characters for boxes.\n            Defaults to \"rbgcmywrbgcmyw\".\n        title (str, optional): Figure title. Defaults to None.\n        figsize (tuple, optional): Figure size as (width, height).\n            Defaults to (8, 6).\n        sharey (bool, optional): If True, all subplots share y-axis scale.\n            Defaults to False.\n        xticklabel (list, optional): Labels for x-axis ticks. Defaults to None.\n        axin (matplotlib.axes.Axes, optional): Existing axes to plot on.\n            Defaults to None.\n        ylim (list, optional): Y-axis limits [min, max]. Defaults to None.\n        ylabel (list, optional): Y-axis labels for each subplot. Defaults to None.\n        notch (bool, optional): If True, boxes will have notches.\n            Defaults to False.\n        widths (float, optional): Width of the boxes. Defaults to 0.5.\n        subplots_adjust_wspace (float, optional): Width space between subplots.\n            Defaults to 0.2.\n        show_median (bool, optional): If True, show median values above boxes.\n            Defaults to True.\n        median_line_color (str, optional): Color of median lines.\n            Defaults to \"black\".\n        median_font_size (str, optional): Font size for median values.\n            Defaults to \"small\".\n\n    Returns:\n        Union[plt.Figure, Tuple[plt.Axes, dict]]: If axin is None, returns the\n        figure object. Otherwise, returns a tuple of (axes, boxplot_dict).\n\n    Example:\n        &gt;&gt;&gt; data = [np.random.normal(0, 1, 100), np.random.normal(2, 1, 100)]\n        &gt;&gt;&gt; fig = plot_boxes_matplotlib(data,\n        ...                           label1=['Group A'],\n        ...                           label2=['Sample 1', 'Sample 2'],\n        ...                           show_median=True)\n    \"\"\"\n    nc = len(data)\n    if axin is None:\n        fig, axes = plt.subplots(\n            ncols=nc, sharey=sharey, figsize=figsize, constrained_layout=False\n        )\n    else:\n        axes = axin\n\n    # the next few lines are for showing median values\n    decimal_places = \"2\"\n    for k in range(nc):\n        ax = axes[k] if nc &gt; 1 else axes\n        temp = data[k]\n        if type(temp) is list:\n            for kk in range(len(temp)):\n                tt = temp[kk]\n                if tt is not None and len(tt) &gt; 0:\n                    tt = tt[~np.isnan(tt)]\n                    temp[kk] = tt\n                else:\n                    temp[kk] = []\n        else:\n            temp = temp[~np.isnan(temp)]\n        bp = ax.boxplot(\n            temp, patch_artist=True, notch=notch, showfliers=False, widths=widths\n        )\n        for median in bp[\"medians\"]:\n            median.set_color(median_line_color)\n        medians_value = [np.median(tmp) for tmp in temp]\n        percent25value = [np.percentile(tmp, 25) for tmp in temp]\n        percent75value = [np.percentile(tmp, 75) for tmp in temp]\n        per25min = np.min(percent25value)\n        per75max = np.max(percent75value)\n        median_labels = [format(s, f\".{decimal_places}f\") for s in medians_value]\n        pos = range(len(medians_value))\n        if show_median:\n            for tick, label in zip(pos, ax.get_xticklabels()):\n                # params of ax.text could be seen here: https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.text.html\n                ax.text(\n                    pos[tick] + 1,\n                    medians_value[tick] + (per75max - per25min) * 0.01,\n                    median_labels[tick],\n                    horizontalalignment=\"center\",\n                    # https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.text.html\n                    size=median_font_size,\n                    weight=\"semibold\",\n                    color=median_line_color,\n                )\n        for kk in range(len(bp[\"boxes\"])):\n            plt.setp(bp[\"boxes\"][kk], facecolor=colorlst[kk])\n\n        if label1 is not None:\n            ax.set_xlabel(label1[k])\n        else:\n            ax.set_xlabel(str(k))\n        if xticklabel is None:\n            ax.set_xticks([])\n        else:\n            ax.set_xticks([y + 1 for y in range(0, len(data[k]), 2)])\n            ax.set_xticklabels(xticklabel)\n        if ylabel is not None:\n            ax.set_ylabel(ylabel[k])\n        if ylim is not None:\n            ax.set_ylim(ylim[k])\n    if label2 is not None:\n        plt.legend(\n            bp[\"boxes\"],\n            label2,\n            # explanation for bbox_to_anchor: https://zhuanlan.zhihu.com/p/101059179\n            bbox_to_anchor=(1.0, 1.02, 0.25, 0.05),\n            loc=\"upper right\",\n            borderaxespad=0,\n            ncol=len(label2) if leg_col is None else leg_col,\n            frameon=False,\n            fontsize=12,\n        )\n    if title is not None:\n        # fig.suptitle(title)\n        ax.set_title(title)\n    plt.tight_layout()\n    plt.subplots_adjust(wspace=subplots_adjust_wspace)\n    return fig if axin is None else (ax, bp)\n</code></pre>"},{"location":"api/hydro_plot/#hydroutils.hydro_plot.plot_boxs","title":"<code>plot_boxs(data, x_name, y_name, uniform_color=None, swarm_plot=False, hue=None, colormap=False, xlim=None, ylim=None, order=None, font='serif', rotation=45, show_median=False)</code>","text":"<p>plot multiple boxes in one ax with seaborn Parameters</p> <p>data : pd.DataFrame     a tidy pandas dataframe;     if you don't know what is \"tidy data\", please read: https://github.com/jizhang/pandas-tidy-data x_name : str     the names of each box y_name : str     what is shown uniform_color : str, optional     unified color for all boxes, by default None swarm_plot : bool, optional     description, by default False hue : type, optional     description, by default None colormap : bool, optional     description, by default False xlim : type, optional     description, by default None ylim : type, optional     description, by default None order : type, optional     description, by default None font : str, optional     description, by default \"serif\" rotation : int, optional     rotation for labels in x-axis, by default 45 show_median: bool, optional     if True, show median value for each box, by default False Returns</p> <p>type description</p> Source code in <code>hydroutils\\hydro_plot.py</code> <pre><code>def plot_boxs(\n    data: pd.DataFrame,\n    x_name: str,\n    y_name: str,\n    uniform_color=None,\n    swarm_plot=False,\n    hue=None,\n    colormap=False,\n    xlim=None,\n    ylim=None,\n    order=None,\n    font=\"serif\",\n    rotation=45,\n    show_median=False,\n):\n    \"\"\"plot multiple boxes in one ax with seaborn\n    Parameters\n    ----------\n    data : pd.DataFrame\n        a tidy pandas dataframe;\n        if you don't know what is \"tidy data\", please read: https://github.com/jizhang/pandas-tidy-data\n    x_name : str\n        the names of each box\n    y_name : str\n        what is shown\n    uniform_color : str, optional\n        unified color for all boxes, by default None\n    swarm_plot : bool, optional\n        _description_, by default False\n    hue : _type_, optional\n        _description_, by default None\n    colormap : bool, optional\n        _description_, by default False\n    xlim : _type_, optional\n        _description_, by default None\n    ylim : _type_, optional\n        _description_, by default None\n    order : _type_, optional\n        _description_, by default None\n    font : str, optional\n        _description_, by default \"serif\"\n    rotation : int, optional\n        rotation for labels in x-axis, by default 45\n    show_median: bool, optional\n        if True, show median value for each box, by default False\n    Returns\n    -------\n    _type_\n        _description_\n    \"\"\"\n    fig = plt.figure()\n    sns.set(style=\"ticks\", palette=\"pastel\", font=font, font_scale=1.5)\n    # Draw a nested boxplot to show bills by day and time\n    if uniform_color is not None:\n        sns_box = sns.boxplot(\n            x=x_name,\n            y=y_name,\n            data=data,\n            color=uniform_color,\n            showfliers=False,\n            order=order,\n        )\n    else:\n        sns_box = sns.boxplot(\n            x=x_name, y=y_name, data=data, showfliers=False, order=order\n        )\n    if swarm_plot:\n        if hue is None:\n            sns_box = sns.swarmplot(\n                x=x_name, y=y_name, data=data, color=\".2\", order=order\n            )\n        elif colormap:\n            # Create a matplotlib colormap from the sns seagreen color palette\n            cmap = sns.light_palette(\"seagreen\", reverse=False, as_cmap=True)\n            # Normalize to the range of possible values from df[\"c\"]\n            norm = matplotlib.colors.Normalize(\n                vmin=data[hue].min(), vmax=data[hue].max()\n            )\n            colors = {cval: cmap(norm(cval)) for cval in data[hue]}\n            # plot the swarmplot with the colors dictionary as palette, s=2 means size is 2\n            sns_box = sns.swarmplot(\n                x=x_name,\n                y=y_name,\n                hue=hue,\n                s=2,\n                data=data,\n                palette=colors,\n                order=order,\n            )\n            # remove the legend, because we want to set a colorbar instead\n            plt.gca().legend_.remove()\n            # create colorbar\n            divider = make_axes_locatable(plt.gca())\n            ax_cb = divider.new_horizontal(size=\"5%\", pad=0.05)\n            fig = sns_box.get_figure()\n            fig.add_axes(ax_cb)\n            cb1 = matplotlib.colorbar.ColorbarBase(\n                ax_cb, cmap=cmap, norm=norm, orientation=\"vertical\"\n            )\n            cb1.set_label(\"Some Units\")\n        else:\n            palette = sns.light_palette(\"seagreen\", reverse=False, n_colors=10)\n            sns_box = sns.swarmplot(\n                x=x_name,\n                y=y_name,\n                hue=hue,\n                s=2,\n                data=data,\n                palette=palette,\n                order=order,\n            )\n    if xlim is not None:\n        plt.xlim(xlim[0], xlim[1])\n    if ylim is not None:\n        plt.ylim(ylim[0], ylim[1])\n    if show_median:\n        medians = data.groupby([x_name], sort=False)[y_name].median().values\n        create_median_labels(sns_box, medians_value=medians, size=\"x-small\")\n    sns.despine()\n    locs, labels = plt.xticks()\n    plt.setp(labels, rotation=rotation)\n    # plt.show()\n    return sns_box.get_figure()\n</code></pre>"},{"location":"api/hydro_plot/#hydroutils.hydro_plot.plot_diff_boxes","title":"<code>plot_diff_boxes(data, row_and_col=None, y_col=None, x_col=None, hspace=0.3, wspace=1, title_str=None, title_font_size=14)</code>","text":"<p>plot boxplots in rows and cols</p> Source code in <code>hydroutils\\hydro_plot.py</code> <pre><code>def plot_diff_boxes(\n    data,\n    row_and_col=None,\n    y_col=None,\n    x_col=None,\n    hspace=0.3,\n    wspace=1,\n    title_str=None,\n    title_font_size=14,\n):\n    \"\"\"plot boxplots in rows and cols\"\"\"\n    # matplotlib.use('TkAgg')\n    if type(data) is not pd.DataFrame:\n        data = pd.DataFrame(data)\n    subplot_num = data.shape[1] if y_col is None else len(y_col)\n    if row_and_col is None:\n        row_num = 1\n        col_num = subplot_num\n        f, axes = plt.subplots(row_num, col_num)\n        plt.subplots_adjust(hspace=hspace, wspace=wspace)\n    else:\n        assert subplot_num &lt;= row_and_col[0] * row_and_col[1]\n        row_num = row_and_col[0]\n        col_num = row_and_col[1]\n        f, axes = plt.subplots(row_num, col_num)\n        f.tight_layout()\n    for i in range(subplot_num):\n        if y_col is None:\n            if row_num == 1 or col_num == 1:\n                sns.boxplot(\n                    y=data.columns.values[i],\n                    data=data,\n                    width=0.5,\n                    orient=\"v\",\n                    ax=axes[i],\n                    showfliers=False,\n                ).set(xlabel=data.columns.values[i], ylabel=\"\")\n            else:\n                row_idx = int(i / col_num)\n                col_idx = i % col_num\n                sns.boxplot(\n                    y=data.columns.values[i],\n                    data=data,\n                    orient=\"v\",\n                    ax=axes[row_idx, col_idx],\n                    showfliers=False,\n                )\n        else:\n            assert x_col is not None\n            if row_num == 1 or col_num == 1:\n                sns.boxplot(\n                    x=data.columns.values[x_col],\n                    y=data.columns.values[y_col[i]],\n                    data=data,\n                    orient=\"v\",\n                    ax=axes[i],\n                    showfliers=False,\n                )\n            else:\n                row_idx = int(i / col_num)\n                col_idx = i % col_num\n                sns.boxplot(\n                    x=data.columns.values[x_col],\n                    y=data.columns.values[y_col[i]],\n                    data=data,\n                    orient=\"v\",\n                    ax=axes[row_idx, col_idx],\n                    showfliers=False,\n                )\n    if title_str is not None:\n        f.suptitle(title_str, fontsize=title_font_size)\n    return f\n</code></pre>"},{"location":"api/hydro_plot/#hydroutils.hydro_plot.plot_ecdf","title":"<code>plot_ecdf(mydataframe, mycolumn, save_file=None)</code>","text":"<p>Empirical cumulative distribution function</p> Source code in <code>hydroutils\\hydro_plot.py</code> <pre><code>def plot_ecdf(mydataframe, mycolumn, save_file=None):\n    \"\"\"Empirical cumulative distribution function\"\"\"\n    x, y = ecdf(mydataframe[mycolumn])\n    df = pd.DataFrame({\"x\": x, \"y\": y})\n    sns.set_style(\"ticks\", {\"axes.grid\": True})\n    sns.lineplot(x=\"x\", y=\"y\", data=df, estimator=None).set(\n        xlim=(0, 1), xticks=np.arange(0, 1, 0.05), yticks=np.arange(0, 1, 0.05)\n    )\n    plt.show()\n    if save_file is not None:\n        plt.savefig(save_file)\n</code></pre>"},{"location":"api/hydro_plot/#hydroutils.hydro_plot.plot_ecdfs","title":"<code>plot_ecdfs(xs, ys, legends=None, style=None, case_str='case', event_str='event', x_str='x', y_str='y', ax_as_subplot=None, interval=0.1)</code>","text":"<p>Empirical cumulative distribution function</p> Source code in <code>hydroutils\\hydro_plot.py</code> <pre><code>def plot_ecdfs(\n    xs,\n    ys,\n    legends=None,\n    style=None,\n    case_str=\"case\",\n    event_str=\"event\",\n    x_str=\"x\",\n    y_str=\"y\",\n    ax_as_subplot=None,\n    interval=0.1,\n):\n    \"\"\"Empirical cumulative distribution function\"\"\"\n    assert isinstance(xs, list) and isinstance(ys, list)\n    assert len(xs) == len(ys)\n    if legends is not None:\n        assert isinstance(legends, list)\n        assert len(ys) == len(legends)\n    if style is not None:\n        assert isinstance(style, list)\n        assert len(ys) == len(style)\n    for y in ys:\n        assert all(xi &lt; yi for xi, yi in zip(y, y[1:]))\n    frames = []\n    for i in range(len(xs)):\n        str_i = x_str + str(i) if legends is None else legends[i]\n        assert all(xi &lt; yi for xi, yi in zip(xs[i], xs[i][1:]))\n        df_dict_i = {\n            x_str: xs[i],\n            y_str: ys[i],\n            case_str: np.full([xs[i].size], str_i),\n        }\n        if style is not None:\n            df_dict_i[event_str] = np.full([xs[i].size], style[i])\n        df_i = pd.DataFrame(df_dict_i)\n        frames.append(df_i)\n    df = pd.concat(frames)\n    sns.set_style(\"ticks\", {\"axes.grid\": True})\n    if style is None:\n        return (\n            sns.lineplot(x=x_str, y=y_str, hue=case_str, data=df, estimator=None).set(\n                xlim=(0, 1),\n                xticks=np.arange(0, 1, interval),\n                yticks=np.arange(0, 1, interval),\n            )\n            if ax_as_subplot is None\n            else sns.lineplot(\n                ax=ax_as_subplot,\n                x=x_str,\n                y=y_str,\n                hue=case_str,\n                data=df,\n                estimator=None,\n            ).set(\n                xlim=(0, 1),\n                xticks=np.arange(0, 1, interval),\n                yticks=np.arange(0, 1, interval),\n            )\n        )\n    elif ax_as_subplot is None:\n        return sns.lineplot(\n            x=x_str,\n            y=y_str,\n            hue=case_str,\n            style=event_str,\n            data=df,\n            estimator=None,\n        ).set(\n            xlim=(0, 1),\n            xticks=np.arange(0, 1, interval),\n            yticks=np.arange(0, 1, interval),\n        )\n    else:\n        return sns.lineplot(\n            ax=ax_as_subplot,\n            x=x_str,\n            y=y_str,\n            hue=case_str,\n            style=event_str,\n            data=df,\n            estimator=None,\n        ).set(\n            xlim=(0, 1),\n            xticks=np.arange(0, 1, interval),\n            yticks=np.arange(0, 1, interval),\n        )\n</code></pre>"},{"location":"api/hydro_plot/#hydroutils.hydro_plot.plot_ecdfs_matplot","title":"<code>plot_ecdfs_matplot(xs, ys, legends=None, colors='rbkgcmy', dash_lines=None, x_str='x', y_str='y', x_interval=0.1, y_interval=0.1, x_lim=(0, 1), y_lim=(0, 1), show_legend=True, legend_font_size=16, fig_size=(8, 6))</code>","text":"<p>Empirical cumulative distribution function with matplotlib Parameters</p> <p>xs : type description ys : type description legends : type, optional     description, by default None colors : str, optional     description, by default \"rbkgcmy\" dash_lines : type, optional     description, by default None x_str : str, optional     description, by default \"x\" y_str : str, optional     description, by default \"y\" x_interval : float, optional     description, by default 0.1 y_interval : float, optional     description, by default 0.1 x_lim : tuple, optional     description, by default (0, 1) y_lim : tuple, optional     description, by default (0, 1) show_legend : bool, optional     description, by default True legend_font_size : int, optional     description, by default 16 fig_size : tuple, optional     size of the figure, by default (8, 6) Returns</p> <p>type description</p> Source code in <code>hydroutils\\hydro_plot.py</code> <pre><code>def plot_ecdfs_matplot(\n    xs,\n    ys,\n    legends=None,\n    colors=\"rbkgcmy\",\n    dash_lines=None,\n    x_str=\"x\",\n    y_str=\"y\",\n    x_interval=0.1,\n    y_interval=0.1,\n    x_lim=(0, 1),\n    y_lim=(0, 1),\n    show_legend=True,\n    legend_font_size=16,\n    fig_size=(8, 6),\n):\n    \"\"\"Empirical cumulative distribution function with matplotlib\n    Parameters\n    ----------\n    xs : _type_\n        _description_\n    ys : _type_\n        _description_\n    legends : _type_, optional\n        _description_, by default None\n    colors : str, optional\n        _description_, by default \"rbkgcmy\"\n    dash_lines : _type_, optional\n        _description_, by default None\n    x_str : str, optional\n        _description_, by default \"x\"\n    y_str : str, optional\n        _description_, by default \"y\"\n    x_interval : float, optional\n        _description_, by default 0.1\n    y_interval : float, optional\n        _description_, by default 0.1\n    x_lim : tuple, optional\n        _description_, by default (0, 1)\n    y_lim : tuple, optional\n        _description_, by default (0, 1)\n    show_legend : bool, optional\n        _description_, by default True\n    legend_font_size : int, optional\n        _description_, by default 16\n    fig_size : tuple, optional\n        size of the figure, by default (8, 6)\n    Returns\n    -------\n    _type_\n        _description_\n    \"\"\"\n    assert isinstance(xs, list) and isinstance(ys, list)\n    assert len(xs) == len(ys)\n    if legends is not None:\n        assert isinstance(legends, list) and len(ys) == len(legends)\n    if dash_lines is not None:\n        assert isinstance(dash_lines, list)\n    else:\n        dash_lines = np.full(len(xs), False).tolist()\n    for y in ys:\n        assert all(xi &lt; yi for xi, yi in zip(y, y[1:]))\n    fig = plt.figure(figsize=fig_size)\n    ax = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n    for i in range(len(xs)):\n        if (\n            np.nanmax(np.array(xs[i])) == np.inf\n            or np.nanmin(np.array(xs[i])) == -np.inf\n        ):\n            assert all(xi &lt;= yi for xi, yi in zip(xs[i], xs[i][1:]))\n        else:\n            assert all(xi &lt;= yi for xi, yi in zip(xs[i], xs[i][1:]))\n        (line_i,) = ax.plot(xs[i], ys[i], color=colors[i], label=legends[i])\n        if dash_lines[i]:\n            line_i.set_dashes([2, 2, 10, 2])\n\n    plt.xlabel(x_str, fontsize=18)\n    plt.ylabel(y_str, fontsize=18)\n    ax.set_xlim(x_lim[0], x_lim[1])\n    ax.set_ylim(y_lim[0], y_lim[1])\n    # set x y number font size\n    plt.xticks(np.arange(x_lim[0], x_lim[1] + x_lim[1] / 100, x_interval), fontsize=16)\n    plt.yticks(np.arange(y_lim[0], y_lim[1] + y_lim[1] / 100, y_interval), fontsize=16)\n    if show_legend:\n        ax.legend()\n        plt.legend(prop={\"size\": legend_font_size})\n    plt.grid()\n    # Hide the right and top spines\n    ax.spines[\"right\"].set_visible(False)\n    ax.spines[\"top\"].set_visible(False)\n    return fig, ax\n</code></pre>"},{"location":"api/hydro_plot/#hydroutils.hydro_plot.plot_event_characteristics","title":"<code>plot_event_characteristics(event_analysis, output_folder, delta_t_hours=3.0, net_rain_key='P_eff', obs_flow_key='Q_obs_eff')</code>","text":"<p>Create and save a detailed flood event characteristics plot.</p> <p>This function creates a comprehensive visualization of a flood event, showing both the net rainfall and direct runoff, along with key event characteristics in a text box. The runoff curve is plotted on top of the rainfall bars for better visibility.</p> <p>Parameters:</p> Name Type Description Default <code>event_analysis</code> <code>Dict</code> <p>Dictionary containing flood event data and analysis: - filepath (str): Path to source data file - peak_obs (float): Peak observed flow - runoff_volume_m3 (float): Total runoff volume in m\u00b3 - runoff_duration_hours (float): Event duration in hours - total_net_rain (float): Total net rainfall in mm - lag_time_hours (float): Time lag between peak rain and peak flow - P_eff (np.ndarray): Net rainfall time series - Q_obs_eff (np.ndarray): Observed flow time series</p> required <code>output_folder</code> <code>str</code> <p>Directory where the plot will be saved.</p> required <code>delta_t_hours</code> <code>float</code> <p>Time step in hours. Defaults to 3.0.</p> <code>3.0</code> <code>net_rain_key</code> <code>str</code> <p>Key for net rainfall in event_analysis. Defaults to \"P_eff\".</p> <code>'P_eff'</code> <code>obs_flow_key</code> <code>str</code> <p>Key for observed flow in event_analysis. Defaults to \"Q_obs_eff\".</p> <code>'Q_obs_eff'</code> Note <ul> <li>Uses dual axes: left for runoff (m\u00b3/s), right for rainfall (mm)</li> <li>Rainfall is plotted as blue bars from top</li> <li>Runoff is plotted as orange line with higher z-order</li> <li>Includes grid for better readability</li> <li>Text box shows key event characteristics</li> <li>Saves plot as PNG with 150 DPI</li> <li>Uses SimSun font for Chinese characters</li> </ul> Example <p>event = { ...     'filepath': 'event_001.csv', ...     'peak_obs': 150.5, ...     'runoff_volume_m3': 2.5e6, ...     'runoff_duration_hours': 48.0, ...     'total_net_rain': 85.5, ...     'lag_time_hours': 6.0, ...     'P_eff': np.array([...]),  # rainfall data ...     'Q_obs_eff': np.array([...])  # flow data ... } plot_event_characteristics(event, 'output/plots/')</p> Credit <p>Original implementation by Zheng Zhang</p> Source code in <code>hydroutils\\hydro_plot.py</code> <pre><code>def plot_event_characteristics(\n    event_analysis: Dict,\n    output_folder: str,\n    delta_t_hours: float = 3.0,\n    net_rain_key: str = \"P_eff\",\n    obs_flow_key: str = \"Q_obs_eff\",\n):\n    \"\"\"Create and save a detailed flood event characteristics plot.\n\n    This function creates a comprehensive visualization of a flood event, showing\n    both the net rainfall and direct runoff, along with key event characteristics\n    in a text box. The runoff curve is plotted on top of the rainfall bars for\n    better visibility.\n\n    Args:\n        event_analysis (Dict): Dictionary containing flood event data and analysis:\n            - filepath (str): Path to source data file\n            - peak_obs (float): Peak observed flow\n            - runoff_volume_m3 (float): Total runoff volume in m\u00b3\n            - runoff_duration_hours (float): Event duration in hours\n            - total_net_rain (float): Total net rainfall in mm\n            - lag_time_hours (float): Time lag between peak rain and peak flow\n            - P_eff (np.ndarray): Net rainfall time series\n            - Q_obs_eff (np.ndarray): Observed flow time series\n        output_folder (str): Directory where the plot will be saved.\n        delta_t_hours (float, optional): Time step in hours. Defaults to 3.0.\n        net_rain_key (str, optional): Key for net rainfall in event_analysis.\n            Defaults to \"P_eff\".\n        obs_flow_key (str, optional): Key for observed flow in event_analysis.\n            Defaults to \"Q_obs_eff\".\n\n    Note:\n        - Uses dual axes: left for runoff (m\u00b3/s), right for rainfall (mm)\n        - Rainfall is plotted as blue bars from top\n        - Runoff is plotted as orange line with higher z-order\n        - Includes grid for better readability\n        - Text box shows key event characteristics\n        - Saves plot as PNG with 150 DPI\n        - Uses SimSun font for Chinese characters\n\n    Example:\n        &gt;&gt;&gt; event = {\n        ...     'filepath': 'event_001.csv',\n        ...     'peak_obs': 150.5,\n        ...     'runoff_volume_m3': 2.5e6,\n        ...     'runoff_duration_hours': 48.0,\n        ...     'total_net_rain': 85.5,\n        ...     'lag_time_hours': 6.0,\n        ...     'P_eff': np.array([...]),  # rainfall data\n        ...     'Q_obs_eff': np.array([...])  # flow data\n        ... }\n        &gt;&gt;&gt; plot_event_characteristics(event, 'output/plots/')\n\n    Credit:\n        Original implementation by Zheng Zhang\n    \"\"\"\n    net_rain = event_analysis[net_rain_key]\n    direct_runoff = event_analysis[obs_flow_key]\n    event_filename = os.path.basename(event_analysis[\"filepath\"])\n\n    fig, ax1 = plt.subplots(figsize=(15, 7))\n    fig.suptitle(f\"\u6d2a\u6c34\u4e8b\u4ef6\u7279\u5f81\u5206\u6790 - {event_filename}\", fontsize=16)\n\n    # \u7ed8\u5236\u5f84\u6d41\u66f2\u7ebf (\u5de6Y\u8f74)\n    x_axis = np.arange(len(direct_runoff))\n    # --- \u6838\u5fc3\u4fee\u6539\uff1a\u4e3a\u66f2\u7ebf\u8bbe\u7f6e\u4e00\u4e2a\u8f83\u9ad8\u7684 zorder ---\n    ax1.plot(\n        x_axis,\n        direct_runoff,\n        color=\"orangered\",\n        label=r\"\u5f84\u6d41 ($m^3/s$)\",\n        zorder=10,\n        linewidth=2,\n    )  # zorder=10\n\n    ax1.set_xlabel(f\"\u65f6\u6bb5 (\u0394t = {delta_t_hours}h)\", fontsize=12)\n    ax1.set_ylabel(r\"\u5f84\u6d41\u6d41\u91cf ($m^3/s$)\", color=\"orangered\", fontsize=12)\n    ax1.tick_params(axis=\"y\", labelcolor=\"orangered\")\n    ax1.set_ylim(bottom=0)\n    ax1.grid(True, which=\"major\", linestyle=\"--\", linewidth=\"0.5\", color=\"gray\")\n\n    # \u521b\u5efa\u5171\u4eabX\u8f74\u7684\u7b2c\u4e8c\u4e2aY\u8f74\n    ax2 = ax1.twinx()\n    # \u7ed8\u5236\u51c0\u96e8\u67f1\u72b6\u56fe (\u53f3Y\u8f74\uff0c\u5411\u4e0b)\n    # --- \u6838\u5fc3\u4fee\u6539\uff1a\u4e3a\u67f1\u72b6\u56fe\u8bbe\u7f6e\u4e00\u4e2a\u8f83\u4f4e\u7684 zorder (\u53ef\u9009\uff0c\u4f46\u597d\u4e60\u60ef) ---\n    ax2.bar(\n        x_axis,\n        net_rain,\n        color=\"steelblue\",\n        label=\"\u51c0\u96e8 (mm)\",\n        width=0.8,\n        zorder=5,\n    )  # zorder=5\n\n    ax2.set_ylabel(\"\u51c0\u96e8\u91cf (mm)\", color=\"steelblue\", fontsize=12)\n    ax2.tick_params(axis=\"y\", labelcolor=\"steelblue\")\n    ax2.invert_yaxis()\n    ax2.set_ylim(top=0)\n\n    # \u51c6\u5907\u5e76\u6807\u6ce8\u6587\u672c\u6846 (\u4e0e\u4e4b\u524d\u5bf9\u9f50\u7248\u672c\u76f8\u540c)\n    labels = [\n        \"\u6d2a\u5cf0\u6d41\u91cf:\",\n        \"\u6d2a   \u91cf:\",\n        \"\u6d2a\u6c34\u5386\u65f6:\",\n        \"\u603b\u51c0\u96e8\u91cf:\",\n        \"\u6d2a\u5cf0\u96e8\u5cf0\u5ef6\u8fdf:\",\n    ]\n    values = [\n        f\"{event_analysis['peak_obs']:.2f} \" + r\"$m^3/s$\",\n        f\"{event_analysis['runoff_volume_m3'] / 1e6:.2f} \" + r\"$\\times 10^6\\ m^3$\",\n        f\"{event_analysis['runoff_duration_hours']:.1f} \u5c0f\u65f6\",\n        f\"{event_analysis['total_net_rain']:.2f} mm\",\n        f\"{event_analysis['lag_time_hours']:.1f} \u5c0f\u65f6\",\n    ]\n    label_text = \"\\n\".join(labels)\n    value_text = \"\\n\".join(values)\n    bbox_props = dict(boxstyle=\"round,pad=0.5\", facecolor=\"wheat\", alpha=0.8)\n    ax1.text(\n        0.85,\n        0.95,\n        \"--- \u6c34\u6587\u7279\u5f81 ---\",\n        transform=ax1.transAxes,\n        fontsize=12,\n        verticalalignment=\"top\",\n        horizontalalignment=\"center\",\n        bbox=bbox_props,\n    )\n    ax1.text(\n        0.80,\n        0.85,\n        label_text,\n        transform=ax1.transAxes,\n        fontsize=12,\n        verticalalignment=\"top\",\n        horizontalalignment=\"right\",\n        family=\"SimSun\",\n    )\n    ax1.text(\n        0.82,\n        0.85,\n        value_text,\n        transform=ax1.transAxes,\n        fontsize=12,\n        verticalalignment=\"top\",\n        horizontalalignment=\"left\",\n    )\n\n    # \u5408\u5e76\u56fe\u4f8b\n    lines1, labels1 = ax1.get_legend_handles_labels()\n    lines2, labels2 = ax2.get_legend_handles_labels()\n    ax2.legend(\n        lines1 + lines2,\n        labels1 + labels2,\n        loc=\"upper left\",\n        bbox_to_anchor=(0.01, 0.95),\n    )\n\n    # \u4fdd\u5b58\u56fe\u5f62\n    output_filename = os.path.join(\n        output_folder, f\"{os.path.splitext(event_filename)[0]}.png\"\n    )\n    try:\n        plt.savefig(output_filename, dpi=150, bbox_inches=\"tight\")\n        print(f\"  \u5df2\u751f\u6210\u56fe\u8868: {output_filename}\")\n    except Exception as e:\n        print(f\"  \u4fdd\u5b58\u56fe\u8868\u5931\u8d25: {output_filename}, \u9519\u8bef: {e}\")\n\n    plt.close(fig)\n</code></pre>"},{"location":"api/hydro_plot/#hydroutils.hydro_plot.plot_heat_map","title":"<code>plot_heat_map(data, mask=None, fig_size=None, fmt='d', square=True, annot=True, xticklabels=True, yticklabels=True)</code>","text":"<p>Create a heatmap visualization using seaborn.</p> <p>This function creates a customizable heatmap for visualizing 2D data arrays. It uses seaborn's heatmap function with additional formatting options and supports masking specific data points.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>2D data array to visualize.</p> required <code>mask</code> <code>ndarray</code> <p>Boolean array of same shape as data. True values will be masked (not shown). Defaults to None.</p> <code>None</code> <code>fig_size</code> <code>tuple</code> <p>Figure size as (width, height). Defaults to None.</p> <code>None</code> <code>fmt</code> <code>str</code> <p>String formatting code for cell annotations. Defaults to \"d\" (integer).</p> <code>'d'</code> <code>square</code> <code>bool</code> <p>If True, set the Axes aspect to \"equal\". Defaults to True.</p> <code>True</code> <code>annot</code> <code>bool</code> <p>If True, write the data value in each cell. Defaults to True.</p> <code>True</code> <code>xticklabels</code> <code>bool</code> <p>If True, show x-axis tick labels. Defaults to True.</p> <code>True</code> <code>yticklabels</code> <code>bool</code> <p>If True, show y-axis tick labels. Defaults to True.</p> <code>True</code> Note <ul> <li>Uses \"RdBu_r\" colormap (red-blue diverging)</li> <li>Annotations are shown by default</li> <li>Cells are square by default for better visualization</li> <li>Based on seaborn's heatmap: https://seaborn.pydata.org/generated/seaborn.heatmap.html</li> </ul> Example <p>data = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) plot_heat_map(data, fig_size=(8, 6))</p> Source code in <code>hydroutils\\hydro_plot.py</code> <pre><code>def plot_heat_map(\n    data: pd.DataFrame,\n    mask=None,\n    fig_size=None,\n    fmt=\"d\",\n    square=True,\n    annot=True,\n    xticklabels=True,\n    yticklabels=True,\n):\n    \"\"\"Create a heatmap visualization using seaborn.\n\n    This function creates a customizable heatmap for visualizing 2D data arrays.\n    It uses seaborn's heatmap function with additional formatting options and\n    supports masking specific data points.\n\n    Args:\n        data (pd.DataFrame): 2D data array to visualize.\n        mask (np.ndarray, optional): Boolean array of same shape as data. True\n            values will be masked (not shown). Defaults to None.\n        fig_size (tuple, optional): Figure size as (width, height). Defaults to None.\n        fmt (str, optional): String formatting code for cell annotations.\n            Defaults to \"d\" (integer).\n        square (bool, optional): If True, set the Axes aspect to \"equal\".\n            Defaults to True.\n        annot (bool, optional): If True, write the data value in each cell.\n            Defaults to True.\n        xticklabels (bool, optional): If True, show x-axis tick labels.\n            Defaults to True.\n        yticklabels (bool, optional): If True, show y-axis tick labels.\n            Defaults to True.\n\n    Note:\n        - Uses \"RdBu_r\" colormap (red-blue diverging)\n        - Annotations are shown by default\n        - Cells are square by default for better visualization\n        - Based on seaborn's heatmap: https://seaborn.pydata.org/generated/seaborn.heatmap.html\n\n    Example:\n        &gt;&gt;&gt; data = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n        &gt;&gt;&gt; plot_heat_map(data, fig_size=(8, 6))\n    \"\"\"\n    if fig_size is not None:\n        fig = plt.figure(figsize=fig_size)\n    ax = sns.heatmap(\n        data=data,\n        square=square,\n        annot=annot,\n        fmt=fmt,\n        cmap=\"RdBu_r\",\n        mask=mask,\n        xticklabels=xticklabels,\n        yticklabels=yticklabels,\n    )\n</code></pre>"},{"location":"api/hydro_plot/#hydroutils.hydro_plot.plot_map_carto","title":"<code>plot_map_carto(data, lat, lon, fig=None, ax=None, pertile_range=None, value_range=None, fig_size=(8, 8), need_colorbar=True, colorbar_size=[0.91, 0.318, 0.02, 0.354], cmap_str='jet', idx_lst=None, markers=None, marker_size=20, is_discrete=False, colors='rbkgcmywrbkgcmyw', category_names=None, legend_font_size=None, colorbar_font_size=None)</code>","text":"<p>Create a map visualization using Cartopy with data points or categories.</p> <p>This function creates a map using Cartopy and plots data points on it. It supports both continuous and discrete data visualization, with options for customizing markers, colors, and legends/colorbars.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>1-D array of values to plot, one per point.</p> required <code>lat</code> <code>ndarray</code> <p>1-D array of latitude values.</p> required <code>lon</code> <code>ndarray</code> <p>1-D array of longitude values.</p> required <code>fig</code> <code>Figure</code> <p>Existing figure to plot on. Defaults to None.</p> <code>None</code> <code>ax</code> <code>Axes</code> <p>Existing axes to plot on. Defaults to None.</p> <code>None</code> <code>pertile_range</code> <code>list</code> <p>Percentile range for color scaling as [min_percentile, max_percentile]. Example: [25, 75] for interquartile range. Defaults to None.</p> <code>None</code> <code>value_range</code> <code>list</code> <p>Explicit value range for color scaling as [min_value, max_value]. Overrides pertile_range. Defaults to None.</p> <code>None</code> <code>fig_size</code> <code>tuple</code> <p>Figure size as (width, height). Defaults to (8, 8).</p> <code>(8, 8)</code> <code>need_colorbar</code> <code>bool</code> <p>Whether to show colorbar. Defaults to True.</p> <code>True</code> <code>colorbar_size</code> <code>list</code> <p>Colorbar position and size as [left, bottom, width, height]. Defaults to [0.91, 0.318, 0.02, 0.354].</p> <code>[0.91, 0.318, 0.02, 0.354]</code> <code>cmap_str</code> <code>Union[str, list]</code> <p>Colormap name(s). Can be single string or list for multiple point types. Defaults to \"jet\".</p> <code>'jet'</code> <code>idx_lst</code> <code>list</code> <p>List of index arrays for plotting multiple point types separately. Defaults to None.</p> <code>None</code> <code>markers</code> <code>Union[str, list]</code> <p>Marker style(s) for points. Can be single style or list. Defaults to None.</p> <code>None</code> <code>marker_size</code> <code>Union[int, list]</code> <p>Marker size(s). Can be single value or list. Defaults to 20.</p> <code>20</code> <code>is_discrete</code> <code>bool</code> <p>If True, treat data as discrete categories. Defaults to False.</p> <code>False</code> <code>colors</code> <code>str</code> <p>String of color characters for discrete categories. Defaults to \"rbkgcmywrbkgcmyw\".</p> <code>'rbkgcmywrbkgcmyw'</code> <code>category_names</code> <code>list</code> <p>Names for discrete categories. Defaults to None.</p> <code>None</code> <code>legend_font_size</code> <code>float</code> <p>Font size for legend. Defaults to None.</p> <code>None</code> <code>colorbar_font_size</code> <code>float</code> <p>Font size for colorbar. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>plt.Axes: The map axes object.</p> Note <ul> <li>Uses Cartopy's PlateCarree projection</li> <li>Automatically determines map extent from data points</li> <li>Includes state boundaries and coastlines</li> <li>Supports both continuous (colorbar) and discrete (legend) data</li> <li>Can plot multiple point types with different markers/colors</li> <li>Handles NaN values appropriately</li> </ul> Example Source code in <code>hydroutils\\hydro_plot.py</code> <pre><code>def plot_map_carto(\n    data,\n    lat,\n    lon,\n    fig=None,\n    ax=None,\n    pertile_range=None,\n    value_range=None,\n    fig_size=(8, 8),\n    need_colorbar=True,\n    colorbar_size=[0.91, 0.318, 0.02, 0.354],\n    cmap_str=\"jet\",\n    idx_lst=None,\n    markers=None,\n    marker_size=20,\n    is_discrete=False,\n    colors=\"rbkgcmywrbkgcmyw\",\n    category_names=None,\n    legend_font_size=None,\n    colorbar_font_size=None,\n):\n    \"\"\"Create a map visualization using Cartopy with data points or categories.\n\n    This function creates a map using Cartopy and plots data points on it. It supports\n    both continuous and discrete data visualization, with options for customizing\n    markers, colors, and legends/colorbars.\n\n    Args:\n        data (np.ndarray): 1-D array of values to plot, one per point.\n        lat (np.ndarray): 1-D array of latitude values.\n        lon (np.ndarray): 1-D array of longitude values.\n        fig (plt.Figure, optional): Existing figure to plot on. Defaults to None.\n        ax (plt.Axes, optional): Existing axes to plot on. Defaults to None.\n        pertile_range (list, optional): Percentile range for color scaling as\n            [min_percentile, max_percentile]. Example: [25, 75] for interquartile\n            range. Defaults to None.\n        value_range (list, optional): Explicit value range for color scaling as\n            [min_value, max_value]. Overrides pertile_range. Defaults to None.\n        fig_size (tuple, optional): Figure size as (width, height).\n            Defaults to (8, 8).\n        need_colorbar (bool, optional): Whether to show colorbar.\n            Defaults to True.\n        colorbar_size (list, optional): Colorbar position and size as\n            [left, bottom, width, height]. Defaults to [0.91, 0.318, 0.02, 0.354].\n        cmap_str (Union[str, list], optional): Colormap name(s). Can be single\n            string or list for multiple point types. Defaults to \"jet\".\n        idx_lst (list, optional): List of index arrays for plotting multiple\n            point types separately. Defaults to None.\n        markers (Union[str, list], optional): Marker style(s) for points. Can be\n            single style or list. Defaults to None.\n        marker_size (Union[int, list], optional): Marker size(s). Can be single\n            value or list. Defaults to 20.\n        is_discrete (bool, optional): If True, treat data as discrete categories.\n            Defaults to False.\n        colors (str, optional): String of color characters for discrete\n            categories. Defaults to \"rbkgcmywrbkgcmyw\".\n        category_names (list, optional): Names for discrete categories.\n            Defaults to None.\n        legend_font_size (float, optional): Font size for legend.\n            Defaults to None.\n        colorbar_font_size (float, optional): Font size for colorbar.\n            Defaults to None.\n\n    Returns:\n        plt.Axes: The map axes object.\n\n    Note:\n        - Uses Cartopy's PlateCarree projection\n        - Automatically determines map extent from data points\n        - Includes state boundaries and coastlines\n        - Supports both continuous (colorbar) and discrete (legend) data\n        - Can plot multiple point types with different markers/colors\n        - Handles NaN values appropriately\n\n    Example:\n        &gt;&gt;&gt; # Continuous data example\n        &gt;&gt;&gt; data = np.random.rand(100)\n        &gt;&gt;&gt; lat = np.random.uniform(30, 45, 100)\n        &gt;&gt;&gt; lon = np.random.uniform(-120, -100, 100)\n        &gt;&gt;&gt; ax = plot_map_carto(data, lat, lon,\n        ...                    value_range=[0, 1],\n        ...                    cmap_str='viridis')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Discrete categories example\n        &gt;&gt;&gt; categories = np.random.randint(0, 3, 100)\n        &gt;&gt;&gt; ax = plot_map_carto(categories, lat, lon,\n        ...                    is_discrete=True,\n        ...                    category_names=['Low', 'Medium', 'High'])\n    \"\"\"\n    if value_range is not None:\n        vmin = value_range[0]\n        vmax = value_range[1]\n    elif pertile_range is None:\n        # https://blog.csdn.net/chenirene510/article/details/111318539\n        mask_data = np.ma.masked_invalid(data)\n        vmin = np.min(mask_data)\n        vmax = np.max(mask_data)\n    else:\n        assert 0 &lt;= pertile_range[0] &lt; pertile_range[1] &lt;= 100\n        vmin = np.nanpercentile(data, pertile_range[0])\n        vmax = np.nanpercentile(data, pertile_range[1])\n    llcrnrlat = (np.min(lat),)\n    urcrnrlat = (np.max(lat),)\n    llcrnrlon = (np.min(lon),)\n    urcrnrlon = (np.max(lon),)\n    extent = [llcrnrlon[0], urcrnrlon[0], llcrnrlat[0], urcrnrlat[0]]\n    # Figure\n    if fig is None or ax is None:\n        fig, ax = plt.subplots(\n            1, 1, figsize=fig_size, subplot_kw={\"projection\": ccrs.PlateCarree()}\n        )\n    ax.set_extent(extent)\n    states = NaturalEarthFeature(\n        category=\"cultural\",\n        scale=\"50m\",\n        facecolor=\"none\",\n        name=\"admin_1_states_provinces_shp\",\n    )\n    ax.add_feature(states, linewidth=0.5, edgecolor=\"black\")\n    ax.coastlines(\"50m\", linewidth=0.8)\n    if idx_lst is not None:\n        if isinstance(marker_size, list):\n            assert len(marker_size) == len(idx_lst)\n        else:\n            marker_size = np.full(len(idx_lst), marker_size).tolist()\n        if not isinstance(marker_size, list):\n            markers = np.full(len(idx_lst), markers).tolist()\n        else:\n            assert len(markers) == len(idx_lst)\n        if not isinstance(cmap_str, list):\n            cmap_str = np.full(len(idx_lst), cmap_str).tolist()\n        else:\n            assert len(cmap_str) == len(idx_lst)\n        if is_discrete:\n            for i in range(len(idx_lst)):\n                ax.plot(\n                    lon[idx_lst[i]],\n                    lat[idx_lst[i]],\n                    marker=markers[i],\n                    ms=marker_size[i],\n                    label=category_names[i],\n                    c=colors[i],\n                    linestyle=\"\",\n                )\n                ax.legend(prop=dict(size=legend_font_size))\n        else:\n            scatter = []\n            for i in range(len(idx_lst)):\n                scat = ax.scatter(\n                    lon[idx_lst[i]],\n                    lat[idx_lst[i]],\n                    c=data[idx_lst[i]],\n                    marker=markers[i],\n                    s=marker_size[i],\n                    cmap=cmap_str[i],\n                    vmin=vmin,\n                    vmax=vmax,\n                )\n                scatter.append(scat)\n            if need_colorbar:\n                if colorbar_size is not None:\n                    cbar_ax = fig.add_axes(colorbar_size)\n                    cbar = fig.colorbar(scat, cax=cbar_ax, orientation=\"vertical\")\n                else:\n                    cbar = fig.colorbar(scat, ax=ax, pad=0.01)\n                if colorbar_font_size is not None:\n                    cbar.ax.tick_params(labelsize=colorbar_font_size)\n            if category_names is not None:\n                ax.legend(\n                    scatter, category_names, prop=dict(size=legend_font_size), ncol=2\n                )\n    elif is_discrete:\n        scatter = ax.scatter(lon, lat, c=data, s=marker_size)\n        # produce a legend with the unique colors from the scatter\n        legend1 = ax.legend(\n            *scatter.legend_elements(), loc=\"lower left\", title=\"Classes\"\n        )\n        ax.add_artist(legend1)\n    else:\n        scat = plt.scatter(\n            lon, lat, c=data, s=marker_size, cmap=cmap_str, vmin=vmin, vmax=vmax\n        )\n        if need_colorbar:\n            if colorbar_size is not None:\n                cbar_ax = fig.add_axes(colorbar_size)\n                cbar = fig.colorbar(scat, cax=cbar_ax, orientation=\"vertical\")\n            else:\n                cbar = fig.colorbar(scat, ax=ax, pad=0.01)\n            if colorbar_font_size is not None:\n                cbar.ax.tick_params(labelsize=colorbar_font_size)\n    return ax\n</code></pre>"},{"location":"api/hydro_plot/#hydroutils.hydro_plot.plot_map_carto--continuous-data-example","title":"Continuous data example","text":"<p>data = np.random.rand(100) lat = np.random.uniform(30, 45, 100) lon = np.random.uniform(-120, -100, 100) ax = plot_map_carto(data, lat, lon, ...                    value_range=[0, 1], ...                    cmap_str='viridis')</p>"},{"location":"api/hydro_plot/#hydroutils.hydro_plot.plot_map_carto--discrete-categories-example","title":"Discrete categories example","text":"<p>categories = np.random.randint(0, 3, 100) ax = plot_map_carto(categories, lat, lon, ...                    is_discrete=True, ...                    category_names=['Low', 'Medium', 'High'])</p>"},{"location":"api/hydro_plot/#hydroutils.hydro_plot.plot_rainfall_runoff","title":"<code>plot_rainfall_runoff(t, p, qs, fig_size=(8, 6), c_lst='rbkgcmy', leg_lst=None, dash_lines=None, title=None, xlabel=None, ylabel=None, prcp_ylabel='prcp(mm/day)', linewidth=1, prcp_interval=20)</code>","text":"<p>Create a combined rainfall-runoff plot with dual axes.</p> <p>This function creates a figure with two synchronized axes: one for streamflow (primary) and one for precipitation (secondary, inverted). The precipitation is plotted as filled areas from the top, while streamflow lines are plotted normally.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Union[array, list]</code> <p>Time values. If list, must match length of qs.</p> required <code>p</code> <code>array</code> <p>Precipitation time series.</p> required <code>qs</code> <code>Union[array, list]</code> <p>Streamflow time series. Can be single array or list of arrays for multiple series.</p> required <code>fig_size</code> <code>tuple</code> <p>Figure size as (width, height). Defaults to (8, 6).</p> <code>(8, 6)</code> <code>c_lst</code> <code>str</code> <p>String of color characters for lines. Defaults to \"rbkgcmy\".</p> <code>'rbkgcmy'</code> <code>leg_lst</code> <code>list</code> <p>Legend labels for streamflow series. Defaults to None.</p> <code>None</code> <code>dash_lines</code> <code>list[bool]</code> <p>Which streamflow lines should be dashed. Defaults to None.</p> <code>None</code> <code>title</code> <code>str</code> <p>Plot title. Defaults to None.</p> <code>None</code> <code>xlabel</code> <code>str</code> <p>X-axis label. Defaults to None.</p> <code>None</code> <code>ylabel</code> <code>str</code> <p>Primary Y-axis label (streamflow). Defaults to None.</p> <code>None</code> <code>prcp_ylabel</code> <code>str</code> <p>Secondary Y-axis label (precipitation). Defaults to \"prcp(mm/day)\".</p> <code>'prcp(mm/day)'</code> <code>linewidth</code> <code>int</code> <p>Width of streamflow lines. Defaults to 1.</p> <code>1</code> <code>prcp_interval</code> <code>int</code> <p>Interval for precipitation Y-axis ticks. Defaults to 20.</p> <code>20</code> <p>Returns:</p> Type Description <p>Tuple[plt.Figure, plt.Axes]: The figure and primary axes objects.</p> Note <ul> <li>Precipitation is plotted from top with blue fill and 0.5 alpha</li> <li>Streamflow axis range is extended by 20% at top</li> <li>Legend is placed at upper left with fontsize 16</li> <li>Grid is enabled on primary (streamflow) axis</li> <li>All tick labels use fontsize 16</li> <li>Right and top spines are hidden</li> </ul> Example <p>t = np.arange(100) p = np.random.uniform(0, 10, 100)  # precipitation q1 = np.random.uniform(0, 100, 100)  # streamflow 1 q2 = np.random.uniform(0, 80, 100)   # streamflow 2 fig, ax = plot_rainfall_runoff(t, p, [q1, q2], ...                               leg_lst=['Obs', 'Sim'], ...                               ylabel='Streamflow (m\u00b3/s)')</p> Source code in <code>hydroutils\\hydro_plot.py</code> <pre><code>def plot_rainfall_runoff(\n    t,\n    p,\n    qs,\n    fig_size=(8, 6),\n    c_lst=\"rbkgcmy\",\n    leg_lst=None,\n    dash_lines=None,\n    title=None,\n    xlabel=None,\n    ylabel=None,\n    prcp_ylabel=\"prcp(mm/day)\",\n    linewidth=1,\n    prcp_interval=20,\n):\n    \"\"\"Create a combined rainfall-runoff plot with dual axes.\n\n    This function creates a figure with two synchronized axes: one for streamflow\n    (primary) and one for precipitation (secondary, inverted). The precipitation\n    is plotted as filled areas from the top, while streamflow lines are plotted\n    normally.\n\n    Args:\n        t (Union[np.array, list]): Time values. If list, must match length of qs.\n        p (np.array): Precipitation time series.\n        qs (Union[np.array, list]): Streamflow time series. Can be single array\n            or list of arrays for multiple series.\n        fig_size (tuple, optional): Figure size as (width, height).\n            Defaults to (8, 6).\n        c_lst (str, optional): String of color characters for lines.\n            Defaults to \"rbkgcmy\".\n        leg_lst (list, optional): Legend labels for streamflow series.\n            Defaults to None.\n        dash_lines (list[bool], optional): Which streamflow lines should be\n            dashed. Defaults to None.\n        title (str, optional): Plot title. Defaults to None.\n        xlabel (str, optional): X-axis label. Defaults to None.\n        ylabel (str, optional): Primary Y-axis label (streamflow).\n            Defaults to None.\n        prcp_ylabel (str, optional): Secondary Y-axis label (precipitation).\n            Defaults to \"prcp(mm/day)\".\n        linewidth (int, optional): Width of streamflow lines. Defaults to 1.\n        prcp_interval (int, optional): Interval for precipitation Y-axis ticks.\n            Defaults to 20.\n\n    Returns:\n        Tuple[plt.Figure, plt.Axes]: The figure and primary axes objects.\n\n    Note:\n        - Precipitation is plotted from top with blue fill and 0.5 alpha\n        - Streamflow axis range is extended by 20% at top\n        - Legend is placed at upper left with fontsize 16\n        - Grid is enabled on primary (streamflow) axis\n        - All tick labels use fontsize 16\n        - Right and top spines are hidden\n\n    Example:\n        &gt;&gt;&gt; t = np.arange(100)\n        &gt;&gt;&gt; p = np.random.uniform(0, 10, 100)  # precipitation\n        &gt;&gt;&gt; q1 = np.random.uniform(0, 100, 100)  # streamflow 1\n        &gt;&gt;&gt; q2 = np.random.uniform(0, 80, 100)   # streamflow 2\n        &gt;&gt;&gt; fig, ax = plot_rainfall_runoff(t, p, [q1, q2],\n        ...                               leg_lst=['Obs', 'Sim'],\n        ...                               ylabel='Streamflow (m\u00b3/s)')\n    \"\"\"\n    fig, ax = plt.subplots(figsize=fig_size)\n    if dash_lines is not None:\n        assert isinstance(dash_lines, list)\n    else:\n        dash_lines = np.full(len(qs), False).tolist()\n    for k in range(len(qs)):\n        tt = t[k] if type(t) is list else t\n        q = qs[k]\n        leg_str = None\n        if leg_lst is not None:\n            leg_str = leg_lst[k]\n        (line_i,) = ax.plot(tt, q, color=c_lst[k], label=leg_str, linewidth=linewidth)\n        if dash_lines[k]:\n            line_i.set_dashes([2, 2, 10, 2])\n\n    ax.set_ylim(ax.get_ylim()[0], ax.get_ylim()[1] * 1.2)\n    # Create second axes, in order to get the bars from the top you can multiply by -1\n    ax2 = ax.twinx()\n    # ax2.bar(tt, -p, color=\"b\")\n    ax2.fill_between(tt, 0, -p, step=\"mid\", color=\"b\", alpha=0.5)\n    # ax2.plot(tt, -p, color=\"b\", alpha=0.7, linewidth=1.5)\n\n    # Now need to fix the axis labels\n    # max_pre = max(p)\n    max_pre = p.max().item()\n    ax2.set_ylim(-max_pre * 5, 0)\n    y2_ticks = np.arange(0, max_pre, prcp_interval)\n    y2_ticklabels = [str(i) for i in y2_ticks]\n    ax2.set_yticks(-1 * y2_ticks)\n    ax2.set_yticklabels(y2_ticklabels, fontsize=16)\n    # ax2.set_yticklabels([lab.get_text()[1:] for lab in ax2.get_yticklabels()])\n    if title is not None:\n        ax.set_title(title, loc=\"center\", fontdict={\"fontsize\": 17})\n    if ylabel is not None:\n        ax.set_ylabel(ylabel, fontsize=18)\n    if xlabel is not None:\n        ax.set_xlabel(xlabel, fontsize=18)\n    ax2.set_ylabel(prcp_ylabel, fontsize=8, loc=\"top\")\n    # ax2.set_ylabel(\"precipitation (mm/day)\", fontsize=12, loc='top')\n    # https://github.com/matplotlib/matplotlib/issues/12318\n    ax.tick_params(axis=\"x\", labelsize=16)\n    ax.tick_params(axis=\"y\", labelsize=16)\n    ax.legend(bbox_to_anchor=(0.01, 0.85), loc=\"upper left\", fontsize=16)\n    ax.grid()\n    return fig, ax\n</code></pre>"},{"location":"api/hydro_plot/#hydroutils.hydro_plot.plot_scatter_with_11line","title":"<code>plot_scatter_with_11line(x, y, point_color='blue', line_color='black', xlim=[0.0, 1.0], ylim=[0.0, 1.0], xlabel=None, ylabel=None)</code>","text":"<p>Create a scatter plot with a 1:1 line for comparing two variables.</p> <p>This function creates a scatter plot comparing two variables and adds a 1:1 line to show the perfect correlation line. The plot includes customizable colors, axis limits, and labels.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array</code> <p>First variable to plot (x-axis).</p> required <code>y</code> <code>array</code> <p>Second variable to plot (y-axis).</p> required <code>point_color</code> <code>str</code> <p>Color of scatter points. Defaults to \"blue\".</p> <code>'blue'</code> <code>line_color</code> <code>str</code> <p>Color of 1:1 line. Defaults to \"black\".</p> <code>'black'</code> <code>xlim</code> <code>list</code> <p>X-axis limits [min, max]. Defaults to [0.0, 1.0].</p> <code>[0.0, 1.0]</code> <code>ylim</code> <code>list</code> <p>Y-axis limits [min, max]. Defaults to [0.0, 1.0].</p> <code>[0.0, 1.0]</code> <code>xlabel</code> <code>str</code> <p>X-axis label. Defaults to None.</p> <code>None</code> <code>ylabel</code> <code>str</code> <p>Y-axis label. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>tuple[plt.Figure, plt.Axes]: Matplotlib figure and axes objects.</p> Note <ul> <li>The plot uses a whitesmoke background</li> <li>Right and top spines are hidden</li> <li>Tick labels use font size 16</li> <li>The 1:1 line is dashed</li> </ul> Example <p>x = np.array([0.1, 0.2, 0.3, 0.4]) y = np.array([0.15, 0.25, 0.35, 0.45]) fig, ax = plot_scatter_with_11line(x, y, ...                                    xlabel='Predicted', ...                                    ylabel='Observed')</p> Source code in <code>hydroutils\\hydro_plot.py</code> <pre><code>def plot_scatter_with_11line(\n    x: np.array,\n    y: np.array,\n    point_color=\"blue\",\n    line_color=\"black\",\n    xlim=[0.0, 1.0],\n    ylim=[0.0, 1.0],\n    xlabel=None,\n    ylabel=None,\n):\n    \"\"\"Create a scatter plot with a 1:1 line for comparing two variables.\n\n    This function creates a scatter plot comparing two variables and adds a 1:1\n    line to show the perfect correlation line. The plot includes customizable\n    colors, axis limits, and labels.\n\n    Args:\n        x (np.array): First variable to plot (x-axis).\n        y (np.array): Second variable to plot (y-axis).\n        point_color (str, optional): Color of scatter points. Defaults to \"blue\".\n        line_color (str, optional): Color of 1:1 line. Defaults to \"black\".\n        xlim (list, optional): X-axis limits [min, max]. Defaults to [0.0, 1.0].\n        ylim (list, optional): Y-axis limits [min, max]. Defaults to [0.0, 1.0].\n        xlabel (str, optional): X-axis label. Defaults to None.\n        ylabel (str, optional): Y-axis label. Defaults to None.\n\n    Returns:\n        tuple[plt.Figure, plt.Axes]: Matplotlib figure and axes objects.\n\n    Note:\n        - The plot uses a whitesmoke background\n        - Right and top spines are hidden\n        - Tick labels use font size 16\n        - The 1:1 line is dashed\n\n    Example:\n        &gt;&gt;&gt; x = np.array([0.1, 0.2, 0.3, 0.4])\n        &gt;&gt;&gt; y = np.array([0.15, 0.25, 0.35, 0.45])\n        &gt;&gt;&gt; fig, ax = plot_scatter_with_11line(x, y,\n        ...                                    xlabel='Predicted',\n        ...                                    ylabel='Observed')\n    \"\"\"\n    fig, ax = plt.subplots()\n    # set background color for ax\n    ax.set_facecolor(\"whitesmoke\")\n    # plot the grid of the figure\n    # plt.grid(color=\"whitesmoke\")\n    ax.scatter(x, y, c=point_color, s=10)\n    line = mlines.Line2D([0, 1], [0, 1], color=line_color, linestyle=\"--\")\n    transform = ax.transAxes\n    line.set_transform(transform)\n    ax.add_line(line)\n    ax.set_xlim(xlim)\n    ax.set_ylim(ylim)\n    plt.xticks(np.arange(xlim[0], xlim[1], 0.1), fontsize=16)\n    plt.yticks(np.arange(ylim[0], ylim[1], 0.1), fontsize=16)\n    # set xlable and ylabel\n    if xlabel is not None:\n        plt.xlabel(xlabel, fontsize=16)\n    if ylabel is not None:\n        plt.ylabel(ylabel, fontsize=16)\n    ax.spines[\"right\"].set_visible(False)\n    ax.spines[\"top\"].set_visible(False)\n    ax.spines[\"left\"].set_visible(False)\n    ax.spines[\"bottom\"].set_visible(False)\n    return fig, ax\n</code></pre>"},{"location":"api/hydro_plot/#hydroutils.hydro_plot.plot_scatter_xyc","title":"<code>plot_scatter_xyc(x_label, x, y_label, y, c_label=None, c=None, size=20, is_reg=False, xlim=None, ylim=None, quadrant=None)</code>","text":"<p>scatter plot: x-y relationship with c as colorbar Parameters</p> <p>x_label : type description x : type description y_label : type description y : type description c_label : type, optional     description, by default None c : type, optional     description, by default None size : int, optional     size of points, by default 20 is_reg : bool, optional     description, by default False xlim : type, optional     description, by default None ylim : type, optional     description, by default None quadrant: list, optional     if it is not None, it should be a list like [0.0,0.0],     the first means we put a new axis in x=0.0, second for y=0.0,     so that we can build a 4-quadrant plot</p> Source code in <code>hydroutils\\hydro_plot.py</code> <pre><code>def plot_scatter_xyc(\n    x_label,\n    x,\n    y_label,\n    y,\n    c_label=None,\n    c=None,\n    size=20,\n    is_reg=False,\n    xlim=None,\n    ylim=None,\n    quadrant=None,\n):\n    \"\"\"\n    scatter plot: x-y relationship with c as colorbar\n    Parameters\n    ----------\n    x_label : _type_\n        _description_\n    x : _type_\n        _description_\n    y_label : _type_\n        _description_\n    y : _type_\n        _description_\n    c_label : _type_, optional\n        _description_, by default None\n    c : _type_, optional\n        _description_, by default None\n    size : int, optional\n        size of points, by default 20\n    is_reg : bool, optional\n        _description_, by default False\n    xlim : _type_, optional\n        _description_, by default None\n    ylim : _type_, optional\n        _description_, by default None\n    quadrant: list, optional\n        if it is not None, it should be a list like [0.0,0.0],\n        the first means we put a new axis in x=0.0, second for y=0.0,\n        so that we can build a 4-quadrant plot\n    \"\"\"\n    fig, ax = plt.subplots()\n    if type(x) is list:\n        for i in range(len(x)):\n            ax.plot(\n                x[i], y[i], marker=\"o\", linestyle=\"\", ms=size, label=c_label[i], c=c[i]\n            )\n        ax.legend()\n\n    elif c is None:\n        df = pd.DataFrame({x_label: x, y_label: y})\n        points = plt.scatter(df[x_label], df[y_label], s=size)\n        if quadrant is not None:\n            plt.axvline(quadrant[0], c=\"grey\", lw=1, linestyle=\"--\")\n            plt.axhline(quadrant[1], c=\"grey\", lw=1, linestyle=\"--\")\n            q2 = df[(df[x_label] &lt; 0) &amp; (df[y_label] &gt; 0)].shape[0]\n            q3 = df[(df[x_label] &lt; 0) &amp; (df[y_label] &lt; 0)].shape[0]\n            q4 = df[(df[x_label] &gt; 0) &amp; (df[y_label] &lt; 0)].shape[0]\n            q5 = df[(df[x_label] == 0) &amp; (df[y_label] == 0)].shape[0]\n            q1 = df[(df[x_label] &gt; 0) &amp; (df[y_label] &gt; 0)].shape[0]\n            q = q1 + q2 + q3 + q4 + q5\n            r1 = int(round(q1 / q, 2) * 100)\n            r2 = int(round(q2 / q, 2) * 100)\n            r3 = int(round(q3 / q, 2) * 100)\n            r4 = int(round(q4 / q, 2) * 100)\n            r5 = 100 - r1 - r2 - r3 - r4\n            plt.text(\n                xlim[1] - (xlim[1] - xlim[0]) * 0.1,\n                ylim[1] - (ylim[1] - ylim[0]) * 0.1,\n                f\"{r1}%\",\n                fontsize=16,\n            )\n            plt.text(\n                xlim[0] + (xlim[1] - xlim[0]) * 0.1,\n                ylim[1] - (ylim[1] - ylim[0]) * 0.1,\n                f\"{r2}%\",\n                fontsize=16,\n            )\n            plt.text(\n                xlim[0] + (xlim[1] - xlim[0]) * 0.1,\n                ylim[0] + (ylim[1] - ylim[0]) * 0.1,\n                f\"{r3}%\",\n                fontsize=16,\n            )\n            plt.text(\n                xlim[1] - (xlim[1] - xlim[0]) * 0.1,\n                ylim[0] + (ylim[1] - ylim[0]) * 0.1,\n                f\"{r4}%\",\n                fontsize=16,\n            )\n            plt.text(0.2, 0.02, f\"{str(r5)}%\", fontsize=16)\n    else:\n        df = pd.DataFrame({x_label: x, y_label: y, c_label: c})\n        points = plt.scatter(\n            df[x_label], df[y_label], c=df[c_label], s=size, cmap=\"Spectral\"\n        )  # set style options\n        # add a color bar\n        plt.colorbar(points)\n\n    # set limits\n    if xlim is not None:\n        plt.xlim(xlim[0], xlim[1])\n    if ylim is not None:\n        plt.ylim(ylim[0], ylim[1])\n    # Hide the right and top spines\n    ax.spines[\"right\"].set_visible(False)\n    ax.spines[\"top\"].set_visible(False)\n    # build the regression plot\n    if is_reg:\n        plot = sns.regplot(x_label, y_label, data=df, scatter=False)  # , color=\".1\"\n        plot = plot.set(xlabel=x_label, ylabel=y_label)  # add labels\n    else:\n        plt.xlabel(x_label, fontsize=18)\n        plt.ylabel(y_label, fontsize=18)\n        plt.xticks(fontsize=16)\n        plt.yticks(fontsize=16)\n</code></pre>"},{"location":"api/hydro_plot/#hydroutils.hydro_plot.plot_ts","title":"<code>plot_ts(t, y, ax=None, t_bar=None, title=None, xlabel=None, ylabel=None, fig_size=(12, 4), c_lst='rbkgcmyrbkgcmyrbkgcmy', leg_lst=None, marker_lst=None, linewidth=2, linespec=None, dash_lines=None, alpha=1)</code>","text":"<p>Plot multiple time series with customizable styling.</p> <p>This function creates a time series plot that can handle multiple series, with extensive customization options for appearance and formatting. It supports both continuous lines and scatter plots, with optional vertical bars and legends.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Union[list, array]</code> <p>Time values. Can be dates, numbers, or a list of arrays (one per series).</p> required <code>y</code> <code>Union[list, array]</code> <p>Data values to plot. Can be a single array or list of arrays for multiple series.</p> required <code>ax</code> <code>Axes</code> <p>Existing axes to plot on. Defaults to None.</p> <code>None</code> <code>t_bar</code> <code>Union[float, list]</code> <p>Position(s) for vertical bars. Defaults to None.</p> <code>None</code> <code>title</code> <code>str</code> <p>Plot title. Defaults to None.</p> <code>None</code> <code>xlabel</code> <code>str</code> <p>X-axis label. Defaults to None.</p> <code>None</code> <code>ylabel</code> <code>str</code> <p>Y-axis label. Defaults to None.</p> <code>None</code> <code>fig_size</code> <code>tuple</code> <p>Figure size as (width, height). Defaults to (12, 4).</p> <code>(12, 4)</code> <code>c_lst</code> <code>str</code> <p>String of color characters for lines. Defaults to \"rbkgcmyrbkgcmyrbkgcmy\".</p> <code>'rbkgcmyrbkgcmyrbkgcmy'</code> <code>leg_lst</code> <code>list</code> <p>Legend labels for each series. Defaults to None.</p> <code>None</code> <code>marker_lst</code> <code>list</code> <p>Marker styles for each series. Defaults to None.</p> <code>None</code> <code>linewidth</code> <code>Union[int, list]</code> <p>Line width(s). Can be single value or list. Defaults to 2.</p> <code>2</code> <code>linespec</code> <code>list</code> <p>Line style specifications. Defaults to None.</p> <code>None</code> <code>dash_lines</code> <code>list[bool]</code> <p>Which lines should be dashed. Defaults to None.</p> <code>None</code> <code>alpha</code> <code>Union[float, list]</code> <p>Opacity value(s) between 0 and 1. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <p>Union[Tuple[plt.Figure, plt.Axes], plt.Axes]: If ax is None, returns</p> <p>(figure, axes), otherwise returns just the axes.</p> Note <ul> <li>Automatically handles NaN values by plotting points instead of lines</li> <li>Supports multiple line styles including solid, dashed, and markers</li> <li>Right and top spines are hidden for cleaner appearance</li> <li>Grid is enabled by default</li> <li>Font size is set to 16 for tick labels</li> <li>Legend is placed in upper right if provided</li> </ul> Example <p>t = np.arange(100) y1 = np.sin(t/10) y2 = np.cos(t/10) fig, ax = plot_ts(t, [y1, y2], ...                  leg_lst=['sin', 'cos'], ...                  xlabel='Time', ...                  ylabel='Value')</p> Source code in <code>hydroutils\\hydro_plot.py</code> <pre><code>def plot_ts(\n    t: Union[list, np.array],\n    y: Union[list, np.array],\n    ax=None,\n    t_bar=None,\n    title=None,\n    xlabel: str = None,\n    ylabel: str = None,\n    fig_size=(12, 4),\n    c_lst=\"rbkgcmyrbkgcmyrbkgcmy\",\n    leg_lst=None,\n    marker_lst=None,\n    linewidth=2,\n    linespec=None,\n    dash_lines=None,\n    alpha=1,\n):\n    \"\"\"Plot multiple time series with customizable styling.\n\n    This function creates a time series plot that can handle multiple series,\n    with extensive customization options for appearance and formatting. It supports\n    both continuous lines and scatter plots, with optional vertical bars and\n    legends.\n\n    Args:\n        t (Union[list, np.array]): Time values. Can be dates, numbers, or a list\n            of arrays (one per series).\n        y (Union[list, np.array]): Data values to plot. Can be a single array or\n            list of arrays for multiple series.\n        ax (matplotlib.axes.Axes, optional): Existing axes to plot on.\n            Defaults to None.\n        t_bar (Union[float, list], optional): Position(s) for vertical bars.\n            Defaults to None.\n        title (str, optional): Plot title. Defaults to None.\n        xlabel (str, optional): X-axis label. Defaults to None.\n        ylabel (str, optional): Y-axis label. Defaults to None.\n        fig_size (tuple, optional): Figure size as (width, height).\n            Defaults to (12, 4).\n        c_lst (str, optional): String of color characters for lines.\n            Defaults to \"rbkgcmyrbkgcmyrbkgcmy\".\n        leg_lst (list, optional): Legend labels for each series. Defaults to None.\n        marker_lst (list, optional): Marker styles for each series.\n            Defaults to None.\n        linewidth (Union[int, list], optional): Line width(s). Can be single value\n            or list. Defaults to 2.\n        linespec (list, optional): Line style specifications. Defaults to None.\n        dash_lines (list[bool], optional): Which lines should be dashed.\n            Defaults to None.\n        alpha (Union[float, list], optional): Opacity value(s) between 0 and 1.\n            Defaults to 1.\n\n    Returns:\n        Union[Tuple[plt.Figure, plt.Axes], plt.Axes]: If ax is None, returns\n        (figure, axes), otherwise returns just the axes.\n\n    Note:\n        - Automatically handles NaN values by plotting points instead of lines\n        - Supports multiple line styles including solid, dashed, and markers\n        - Right and top spines are hidden for cleaner appearance\n        - Grid is enabled by default\n        - Font size is set to 16 for tick labels\n        - Legend is placed in upper right if provided\n\n    Example:\n        &gt;&gt;&gt; t = np.arange(100)\n        &gt;&gt;&gt; y1 = np.sin(t/10)\n        &gt;&gt;&gt; y2 = np.cos(t/10)\n        &gt;&gt;&gt; fig, ax = plot_ts(t, [y1, y2],\n        ...                  leg_lst=['sin', 'cos'],\n        ...                  xlabel='Time',\n        ...                  ylabel='Value')\n    \"\"\"\n    is_new_fig = False\n    if ax is None:\n        fig = plt.figure(figsize=fig_size)\n        ax = fig.subplots()\n        is_new_fig = True\n    if dash_lines is not None:\n        assert isinstance(dash_lines, list)\n    else:\n        dash_lines = np.full(len(t), False).tolist()\n        # dash_lines[-1] = True\n    if type(y) is np.ndarray:\n        y = [y]\n    if type(linewidth) is not list:\n        linewidth = [linewidth] * len(y)\n    if type(alpha) is not list:\n        alpha = [alpha] * len(y)\n    for k in range(len(y)):\n        tt = t[k] if type(t) is list else t\n        yy = y[k]\n        leg_str = None\n        if leg_lst is not None:\n            leg_str = leg_lst[k]\n        if marker_lst is None:\n            (line_i,) = (\n                ax.plot(tt, yy, \"*\", color=c_lst[k], label=leg_str, alpha=alpha[k])\n                if True in np.isnan(yy)\n                else ax.plot(\n                    tt,\n                    yy,\n                    color=c_lst[k],\n                    label=leg_str,\n                    linewidth=linewidth[k],\n                    alpha=alpha[k],\n                )\n            )\n        elif marker_lst[k] == \"-\":\n            if linespec is not None:\n                (line_i,) = ax.plot(\n                    tt,\n                    yy,\n                    color=c_lst[k],\n                    label=leg_str,\n                    linestyle=linespec[k],\n                    lw=linewidth[k],\n                    alpha=alpha[k],\n                )\n            else:\n                (line_i,) = ax.plot(\n                    tt,\n                    yy,\n                    color=c_lst[k],\n                    label=leg_str,\n                    lw=linewidth[k],\n                    alpha=alpha[k],\n                )\n        else:\n            (line_i,) = ax.plot(\n                tt,\n                yy,\n                color=c_lst[k],\n                label=leg_str,\n                marker=marker_lst[k],\n                lw=linewidth[k],\n                alpha=alpha[k],\n            )\n        if dash_lines[k]:\n            line_i.set_dashes([2, 2, 10, 2])\n        if ylabel is not None:\n            ax.set_ylabel(ylabel, fontsize=18)\n        if xlabel is not None:\n            ax.set_xlabel(xlabel, fontsize=18)\n    if t_bar is not None:\n        ylim = ax.get_ylim()\n        t_bar = [t_bar] if type(t_bar) is not list else t_bar\n        for tt in t_bar:\n            ax.plot([tt, tt], ylim, \"-k\")\n\n    if leg_lst is not None:\n        ax.legend(loc=\"upper right\", frameon=False)\n        plt.legend(prop={\"size\": 16})\n    if title is not None:\n        ax.set_title(title, loc=\"center\", fontdict={\"fontsize\": 17})\n    # plot the grid of the figure\n    plt.grid()\n    plt.xticks(fontsize=16)\n    plt.yticks(fontsize=16)\n    # Hide the right and top spines\n    ax.spines[\"right\"].set_visible(False)\n    ax.spines[\"top\"].set_visible(False)\n    plt.tight_layout()\n    return (fig, ax) if is_new_fig else ax\n</code></pre>"},{"location":"api/hydro_plot/#hydroutils.hydro_plot.plot_unit_hydrograph","title":"<code>plot_unit_hydrograph(U_optimized, title, smoothing_factor=None, peak_violation_weight=None, delta_t_hours=3.0)</code>","text":"<p>Create a unit hydrograph plot with optimization parameters.</p> <p>This function visualizes a unit hydrograph (UH) as a line plot with markers. If optimization parameters are provided, they are included in the title. The plot includes a grid and appropriate axis labels.</p> <p>Parameters:</p> Name Type Description Default <code>U_optimized</code> <code>ndarray</code> <p>Optimized unit hydrograph ordinates.</p> required <code>title</code> <code>str</code> <p>Base title for the plot.</p> required <code>smoothing_factor</code> <code>float</code> <p>Smoothing factor used in optimization. Defaults to None.</p> <code>None</code> <code>peak_violation_weight</code> <code>float</code> <p>Weight for peak violation penalty in optimization. Defaults to None.</p> <code>None</code> <code>delta_t_hours</code> <code>float</code> <p>Time step in hours. Defaults to 3.0.</p> <code>3.0</code> Note <ul> <li>Uses markers ('o') at each UH ordinate</li> <li>Includes dashed grid lines with 0.7 alpha</li> <li>X-axis shows time in hours</li> <li>Y-axis shows UH ordinates in mm/3h</li> <li>If optimization parameters are provided, they are shown in parentheses   after the title</li> </ul> Example <p>uh = np.array([0.1, 0.3, 0.4, 0.2, 0.0]) plot_unit_hydrograph(uh, \"Test Basin UH\", ...                     smoothing_factor=0.1, ...                     peak_violation_weight=0.5)</p> Source code in <code>hydroutils\\hydro_plot.py</code> <pre><code>def plot_unit_hydrograph(\n    U_optimized,\n    title,\n    smoothing_factor=None,\n    peak_violation_weight=None,\n    delta_t_hours=3.0,\n):\n    \"\"\"Create a unit hydrograph plot with optimization parameters.\n\n    This function visualizes a unit hydrograph (UH) as a line plot with markers.\n    If optimization parameters are provided, they are included in the title.\n    The plot includes a grid and appropriate axis labels.\n\n    Args:\n        U_optimized (np.ndarray): Optimized unit hydrograph ordinates.\n        title (str): Base title for the plot.\n        smoothing_factor (float, optional): Smoothing factor used in optimization.\n            Defaults to None.\n        peak_violation_weight (float, optional): Weight for peak violation penalty\n            in optimization. Defaults to None.\n        delta_t_hours (float, optional): Time step in hours. Defaults to 3.0.\n\n    Note:\n        - Uses markers ('o') at each UH ordinate\n        - Includes dashed grid lines with 0.7 alpha\n        - X-axis shows time in hours\n        - Y-axis shows UH ordinates in mm/3h\n        - If optimization parameters are provided, they are shown in parentheses\n          after the title\n\n    Example:\n        &gt;&gt;&gt; uh = np.array([0.1, 0.3, 0.4, 0.2, 0.0])\n        &gt;&gt;&gt; plot_unit_hydrograph(uh, \"Test Basin UH\",\n        ...                     smoothing_factor=0.1,\n        ...                     peak_violation_weight=0.5)\n    \"\"\"\n    if U_optimized is None:\n        print(f\"\u26a0\ufe0f \u65e0\u6cd5\u7ed8\u5236\u5355\u4f4d\u7ebf\uff1a{title} - \u4f18\u5316\u5931\u8d25\")\n        return\n\n    time_axis_uh = np.arange(1, len(U_optimized) + 1) * delta_t_hours\n\n    plt.figure(figsize=(12, 6))\n    plt.plot(time_axis_uh, U_optimized, marker=\"o\", linestyle=\"-\")\n\n    # \u6784\u5efa\u5b8c\u6574\u6807\u9898\n    full_title = title\n    if smoothing_factor is not None and peak_violation_weight is not None:\n        full_title += f\" (\u5e73\u6ed1={smoothing_factor}, \u5355\u5cf0\u7f5a={peak_violation_weight})\"\n\n    plt.title(full_title)\n    plt.xlabel(f\"\u65f6\u95f4 (\u5c0f\u65f6, \u0394t={delta_t_hours}h)\")\n    plt.ylabel(\"1mm\u51c0\u96e8\u5355\u4f4d\u7ebf\u7eb5\u5750\u6807 (mm/3h)\")\n    plt.grid(True, linestyle=\"--\", alpha=0.7)\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"api/hydro_plot/#hydroutils.hydro_plot.setup_matplotlib_chinese","title":"<code>setup_matplotlib_chinese()</code>","text":"<p>Configure matplotlib for Chinese font support and math rendering.</p> <p>This function sets up matplotlib to properly display Chinese characters and mathematical expressions. It configures the font family, handles negative signs, and sets up math rendering to work harmoniously with Chinese text.</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if configuration was successful, False if there was an error (usually due to missing SimHei font).</p> Note <ul> <li>Uses SimHei as the primary Chinese font</li> <li>Sets up STIX fonts for math rendering to match Times New Roman</li> <li>Handles negative sign display in Chinese context</li> <li>Configures sans-serif as the default font family</li> </ul> Example <p>if setup_matplotlib_chinese(): ...     plt.title(\"\u4e2d\u6587\u6807\u9898\") ...     plt.xlabel(\"\u65f6\u95f4 (s)\") ... else: ...     print(\"Chinese font setup failed\")</p> Source code in <code>hydroutils\\hydro_plot.py</code> <pre><code>def setup_matplotlib_chinese():\n    \"\"\"Configure matplotlib for Chinese font support and math rendering.\n\n    This function sets up matplotlib to properly display Chinese characters and\n    mathematical expressions. It configures the font family, handles negative signs,\n    and sets up math rendering to work harmoniously with Chinese text.\n\n    Returns:\n        bool: True if configuration was successful, False if there was an error\n            (usually due to missing SimHei font).\n\n    Note:\n        - Uses SimHei as the primary Chinese font\n        - Sets up STIX fonts for math rendering to match Times New Roman\n        - Handles negative sign display in Chinese context\n        - Configures sans-serif as the default font family\n\n    Example:\n        &gt;&gt;&gt; if setup_matplotlib_chinese():\n        ...     plt.title(\"\u4e2d\u6587\u6807\u9898\")\n        ...     plt.xlabel(\"\u65f6\u95f4 (s)\")\n        ... else:\n        ...     print(\"Chinese font setup failed\")\n    \"\"\"\n    try:\n        plt.rcParams[\"font.sans-serif\"] = [\"SimHei\"]\n        plt.rcParams[\"axes.unicode_minus\"] = False\n        # --- \u65b0\u589e\uff1a\u4e3aLaTeX\u6570\u5b66\u516c\u5f0f\u6e32\u67d3\u914d\u7f6e\u5b57\u4f53 ---\n        # \u8fd9\u53ef\u4ee5\u5e2e\u52a9\u786e\u4fdd\u6570\u5b66\u7b26\u53f7\u548c\u4e2d\u6587\u5b57\u4f53\u770b\u8d77\u6765\u66f4\u548c\u8c10\n        plt.rcParams[\"mathtext.fontset\"] = (\n            \"stix\"  # 'stix' \u662f\u4e00\u79cd\u4e0eTimes New Roman\u76f8\u4f3c\u7684\u79d1\u5b66\u5b57\u4f53\n        )\n        plt.rcParams[\"font.family\"] = \"sans-serif\"  # \u4fdd\u6301\u5176\u4ed6\u6587\u672c\u4e3a\u65e0\u886c\u7ebf\u5b57\u4f53\n        return True\n    except Exception as e:\n        warnings.warn(\n            f\"Warning: Chinese font 'SimHei' not found, Chinese text may not display correctly. Error: {e}\"\n        )\n        return False\n</code></pre>"},{"location":"api/hydro_s3/","title":"hydro_s3","text":"<p>The <code>hydro_s3</code> module provides utilities for interacting with S3-compatible storage services, supporting both MinIO and AWS S3.</p>"},{"location":"api/hydro_s3/#minio-functions","title":"MinIO Functions","text":""},{"location":"api/hydro_s3/#minio_upload_file","title":"minio_upload_file","text":"<pre><code>def minio_upload_file(client: Minio, bucket_name: str, object_name: str, file_path: str) -&gt; list\n</code></pre> <p>Uploads a file to MinIO S3-compatible storage.</p> <p>Example: <pre><code>from minio import Minio\nclient = Minio('play.min.io', access_key='...', secret_key='...')\nobjects = minio_upload_file(client, 'mybucket', 'data.csv', './data.csv')\nprint(f\"Bucket contents: {objects}\")\n</code></pre></p>"},{"location":"api/hydro_s3/#minio_download_file","title":"minio_download_file","text":"<pre><code>def minio_download_file(client: Minio, bucket_name: str, object_name: str, file_path: str, version_id: str = None) -&gt; None\n</code></pre> <p>Downloads a file from MinIO S3-compatible storage.</p> <p>Example: <pre><code>from minio import Minio\nclient = Minio('play.min.io', access_key='...', secret_key='...')\nminio_download_file(client, 'mybucket', 'data.csv', './downloaded.csv')\n</code></pre></p>"},{"location":"api/hydro_s3/#aws-s3-functions","title":"AWS S3 Functions","text":""},{"location":"api/hydro_s3/#boto3_upload_file","title":"boto3_upload_file","text":"<pre><code>def boto3_upload_file(client, bucket_name: str, object_name: str, file_path: str) -&gt; list\n</code></pre> <p>Uploads a file to AWS S3 using boto3.</p> <p>Example: <pre><code>import boto3\nclient = boto3.client('s3')\nobjects = boto3_upload_file(client, 'mybucket', 'data.csv', './data.csv')\nprint(f\"Bucket contents: {objects}\")\n</code></pre></p>"},{"location":"api/hydro_s3/#boto3_download_file","title":"boto3_download_file","text":"<pre><code>def boto3_download_file(client, bucket_name: str, object_name: str, file_path: str) -&gt; None\n</code></pre> <p>Downloads a file from AWS S3 using boto3.</p> <p>Example: <pre><code>import boto3\nclient = boto3.client('s3')\nboto3_download_file(client, 'mybucket', 'data.csv', './downloaded.csv')\n</code></pre></p>"},{"location":"api/hydro_s3/#common-features","title":"Common Features","text":"<ul> <li>Automatic bucket creation if not exists</li> <li>UTF-8 text file handling</li> <li>Version control support (MinIO)</li> <li>List bucket contents after upload</li> <li>Simple and consistent API for both services</li> </ul>"},{"location":"api/hydro_s3/#api-reference","title":"API Reference","text":"<p>Author: Wenyu Ouyang Date: 2023-10-27 15:08:16 LastEditTime: 2023-10-27 15:31:13 LastEditors: Wenyu Ouyang Description: Some functions to deal with s3 file system FilePath: /hydroutils/hydroutils/hydro_s3.py Copyright (c) 2023-2024 Wenyu Ouyang. All rights reserved.</p>"},{"location":"api/hydro_s3/#hydroutils.hydro_s3.boto3_download_file","title":"<code>boto3_download_file(client, bucket_name, object_name, file_path)</code>","text":"<p>Download a file from S3 using boto3.</p> <p>This function downloads an object from S3 storage to a local file using the boto3 client. It provides a simple wrapper around boto3's download_file method.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>client</code> <p>Initialized boto3 S3 client instance.</p> required <code>bucket_name</code> <code>str</code> <p>Name of the bucket containing the object.</p> required <code>object_name</code> <code>str</code> <p>Name of the object to download.</p> required <code>file_path</code> <code>str</code> <p>Local path where the file should be saved.</p> required Example <p>import boto3 client = boto3.client('s3', ...                      endpoint_url='http://localhost:9000', ...                      aws_access_key_id='access_key', ...                      aws_secret_access_key='secret_key') boto3_download_file(client, ...                    'mybucket', ...                    'data/file.csv', ...                    '/local/path/file.csv')</p> Source code in <code>hydroutils\\hydro_s3.py</code> <pre><code>def boto3_download_file(client, bucket_name, object_name, file_path: str):\n    \"\"\"Download a file from S3 using boto3.\n\n    This function downloads an object from S3 storage to a local file using\n    the boto3 client. It provides a simple wrapper around boto3's download_file\n    method.\n\n    Args:\n        client (boto3.client): Initialized boto3 S3 client instance.\n        bucket_name (str): Name of the bucket containing the object.\n        object_name (str): Name of the object to download.\n        file_path (str): Local path where the file should be saved.\n\n    Example:\n        &gt;&gt;&gt; import boto3\n        &gt;&gt;&gt; client = boto3.client('s3',\n        ...                      endpoint_url='http://localhost:9000',\n        ...                      aws_access_key_id='access_key',\n        ...                      aws_secret_access_key='secret_key')\n        &gt;&gt;&gt; boto3_download_file(client,\n        ...                    'mybucket',\n        ...                    'data/file.csv',\n        ...                    '/local/path/file.csv')\n    \"\"\"\n    client.download_file(bucket_name, object_name, file_path)\n</code></pre>"},{"location":"api/hydro_s3/#hydroutils.hydro_s3.boto3_upload_file","title":"<code>boto3_upload_file(client, bucket_name, object_name, file_path)</code>","text":"<p>Upload a file to S3 using boto3.</p> <p>This function uploads a local file to S3 storage using the boto3 client. If the specified bucket doesn't exist, it will be created automatically. After upload, it returns a list of all objects in the bucket.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>client</code> <p>Initialized boto3 S3 client instance.</p> required <code>bucket_name</code> <code>str</code> <p>Name of the bucket to upload to.</p> required <code>object_name</code> <code>str</code> <p>Name to give the object in S3 storage.</p> required <code>file_path</code> <code>str</code> <p>Path to the local file to upload.</p> required <p>Returns:</p> Type Description <p>list[str]: List of all object keys in the bucket after upload.</p> Note <ul> <li>Creates bucket if it doesn't exist</li> <li>Uses upload_file for efficient file upload</li> <li>Lists all objects in bucket after upload</li> <li>Handles bucket listing and creation using boto3's API</li> </ul> Example <p>import boto3 client = boto3.client('s3', ...                      endpoint_url='http://localhost:9000', ...                      aws_access_key_id='access_key', ...                      aws_secret_access_key='secret_key') objects = boto3_upload_file(client, ...                            'mybucket', ...                            'data/file.csv', ...                            '/local/path/file.csv') print(objects) ['data/file.csv', 'data/other.csv']</p> Source code in <code>hydroutils\\hydro_s3.py</code> <pre><code>def boto3_upload_file(client, bucket_name, object_name, file_path):\n    \"\"\"Upload a file to S3 using boto3.\n\n    This function uploads a local file to S3 storage using the boto3 client.\n    If the specified bucket doesn't exist, it will be created automatically.\n    After upload, it returns a list of all objects in the bucket.\n\n    Args:\n        client (boto3.client): Initialized boto3 S3 client instance.\n        bucket_name (str): Name of the bucket to upload to.\n        object_name (str): Name to give the object in S3 storage.\n        file_path (str): Path to the local file to upload.\n\n    Returns:\n        list[str]: List of all object keys in the bucket after upload.\n\n    Note:\n        - Creates bucket if it doesn't exist\n        - Uses upload_file for efficient file upload\n        - Lists all objects in bucket after upload\n        - Handles bucket listing and creation using boto3's API\n\n    Example:\n        &gt;&gt;&gt; import boto3\n        &gt;&gt;&gt; client = boto3.client('s3',\n        ...                      endpoint_url='http://localhost:9000',\n        ...                      aws_access_key_id='access_key',\n        ...                      aws_secret_access_key='secret_key')\n        &gt;&gt;&gt; objects = boto3_upload_file(client,\n        ...                            'mybucket',\n        ...                            'data/file.csv',\n        ...                            '/local/path/file.csv')\n        &gt;&gt;&gt; print(objects)\n        ['data/file.csv', 'data/other.csv']\n    \"\"\"\n    # Make a bucket\n    bucket_names = [dic[\"Name\"] for dic in client.list_buckets()[\"Buckets\"]]\n    if bucket_name not in bucket_names:\n        client.create_bucket(Bucket=bucket_name)\n    # Upload an object\n    client.upload_file(file_path, bucket_name, object_name)\n    return [dic[\"Key\"] for dic in client.list_objects(Bucket=bucket_name)[\"Contents\"]]\n</code></pre>"},{"location":"api/hydro_s3/#hydroutils.hydro_s3.minio_download_file","title":"<code>minio_download_file(client, bucket_name, object_name, file_path, version_id=None)</code>","text":"<p>Download a file from MinIO S3-compatible storage.</p> <p>This function downloads an object from MinIO storage to a local file. It supports versioned objects and handles UTF-8 encoded text files. The function ensures proper cleanup of resources after download.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Minio</code> <p>Initialized MinIO client instance.</p> required <code>bucket_name</code> <code>str</code> <p>Name of the bucket containing the object.</p> required <code>object_name</code> <code>str</code> <p>Name of the object to download.</p> required <code>file_path</code> <code>str</code> <p>Local path where the file should be saved.</p> required <code>version_id</code> <code>str</code> <p>Version ID for versioned objects. Defaults to None.</p> <code>None</code> Note <ul> <li>Assumes UTF-8 encoding for text files</li> <li>Properly closes and releases connection after download</li> <li>Uses context managers for file handling</li> <li>Handles cleanup in finally block for robustness</li> </ul> Example <p>client = Minio('play.min.io', ...               access_key='access_key', ...               secret_key='secret_key') minio_download_file(client, ...                    'mybucket', ...                    'data/file.csv', ...                    '/local/path/file.csv')</p> Source code in <code>hydroutils\\hydro_s3.py</code> <pre><code>def minio_download_file(\n    client: Minio, bucket_name, object_name, file_path: str, version_id=None\n):\n    \"\"\"Download a file from MinIO S3-compatible storage.\n\n    This function downloads an object from MinIO storage to a local file. It\n    supports versioned objects and handles UTF-8 encoded text files. The function\n    ensures proper cleanup of resources after download.\n\n    Args:\n        client (Minio): Initialized MinIO client instance.\n        bucket_name (str): Name of the bucket containing the object.\n        object_name (str): Name of the object to download.\n        file_path (str): Local path where the file should be saved.\n        version_id (str, optional): Version ID for versioned objects.\n            Defaults to None.\n\n    Note:\n        - Assumes UTF-8 encoding for text files\n        - Properly closes and releases connection after download\n        - Uses context managers for file handling\n        - Handles cleanup in finally block for robustness\n\n    Example:\n        &gt;&gt;&gt; client = Minio('play.min.io',\n        ...               access_key='access_key',\n        ...               secret_key='secret_key')\n        &gt;&gt;&gt; minio_download_file(client,\n        ...                    'mybucket',\n        ...                    'data/file.csv',\n        ...                    '/local/path/file.csv')\n    \"\"\"\n    try:\n        response = client.get_object(bucket_name, object_name, version_id)\n        res_csv: str = response.data.decode(\"utf8\")\n        with open(file_path, \"w+\") as fp:\n            fp.write(res_csv)\n    finally:\n        response.close()\n        response.release_conn()\n</code></pre>"},{"location":"api/hydro_s3/#hydroutils.hydro_s3.minio_upload_file","title":"<code>minio_upload_file(client, bucket_name, object_name, file_path)</code>","text":"<p>Upload a file to MinIO S3-compatible storage.</p> <p>This function uploads a local file to MinIO storage. If the specified bucket doesn't exist, it will be created automatically. After upload, it returns a list of all objects in the bucket.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Minio</code> <p>Initialized MinIO client instance.</p> required <code>bucket_name</code> <code>str</code> <p>Name of the bucket to upload to.</p> required <code>object_name</code> <code>str</code> <p>Name to give the object in MinIO storage.</p> required <code>file_path</code> <code>str</code> <p>Path to the local file to upload.</p> required <p>Returns:</p> Type Description <p>list[str]: List of all object names in the bucket after upload.</p> Note <ul> <li>Creates bucket if it doesn't exist</li> <li>Uses fput_object for efficient file upload</li> <li>Lists all objects recursively after upload</li> </ul> Example <p>client = Minio('play.min.io', ...               access_key='access_key', ...               secret_key='secret_key') objects = minio_upload_file(client, ...                            'mybucket', ...                            'data/file.csv', ...                            '/local/path/file.csv') print(objects) ['data/file.csv', 'data/other.csv']</p> Source code in <code>hydroutils\\hydro_s3.py</code> <pre><code>def minio_upload_file(client, bucket_name, object_name, file_path):\n    \"\"\"Upload a file to MinIO S3-compatible storage.\n\n    This function uploads a local file to MinIO storage. If the specified bucket\n    doesn't exist, it will be created automatically. After upload, it returns a\n    list of all objects in the bucket.\n\n    Args:\n        client (minio.Minio): Initialized MinIO client instance.\n        bucket_name (str): Name of the bucket to upload to.\n        object_name (str): Name to give the object in MinIO storage.\n        file_path (str): Path to the local file to upload.\n\n    Returns:\n        list[str]: List of all object names in the bucket after upload.\n\n    Note:\n        - Creates bucket if it doesn't exist\n        - Uses fput_object for efficient file upload\n        - Lists all objects recursively after upload\n\n    Example:\n        &gt;&gt;&gt; client = Minio('play.min.io',\n        ...               access_key='access_key',\n        ...               secret_key='secret_key')\n        &gt;&gt;&gt; objects = minio_upload_file(client,\n        ...                            'mybucket',\n        ...                            'data/file.csv',\n        ...                            '/local/path/file.csv')\n        &gt;&gt;&gt; print(objects)\n        ['data/file.csv', 'data/other.csv']\n    \"\"\"\n    # Make a bucket\n    bucket_names = [bucket.name for bucket in client.list_buckets()]\n    if bucket_name not in bucket_names:\n        client.make_bucket(bucket_name)\n    # Upload an object\n    client.fput_object(bucket_name, object_name, file_path)\n    # List objects\n    objects = client.list_objects(bucket_name, recursive=True)\n    return [obj.object_name for obj in objects]\n</code></pre>"},{"location":"api/hydro_stat/","title":"hydro_stat","text":"<p>The <code>hydro_stat</code> module provides statistical functions for hydrological data analysis, including error metrics, flow duration curves, and data transformations.</p>"},{"location":"api/hydro_stat/#error-metrics","title":"Error Metrics","text":""},{"location":"api/hydro_stat/#stat_error","title":"stat_error","text":"<pre><code>def stat_error(target: np.ndarray, pred: np.ndarray, fill_nan: str = \"no\") -&gt; dict\n</code></pre> <p>Calculates multiple error metrics between predicted and target values.</p> <p>Example: <pre><code>obs = np.array([[1, 2, 3], [4, 5, 6]])  # 2 basins, 3 timesteps\npred = np.array([[1.1, 2.1, 3.1], [4.2, 5.1, 5.9]])\nmetrics = stat_error(obs, pred)\nprint(f\"Mean RMSE: {np.mean(metrics['RMSE']):.2f}\")\n</code></pre></p>"},{"location":"api/hydro_stat/#stat_errors","title":"stat_errors","text":"<pre><code>def stat_errors(target: np.ndarray, pred: np.ndarray, fill_nan: list = None) -&gt; list\n</code></pre> <p>Similar to stat_error but handles 3D arrays for multiple variables.</p>"},{"location":"api/hydro_stat/#kge","title":"KGE","text":"<pre><code>def KGE(xs: np.ndarray, xo: np.ndarray) -&gt; float\n</code></pre> <p>Calculates Kling-Gupta Efficiency between simulated and observed values.</p>"},{"location":"api/hydro_stat/#flow-duration-curves","title":"Flow Duration Curves","text":""},{"location":"api/hydro_stat/#cal_fdc","title":"cal_fdc","text":"<pre><code>def cal_fdc(data: np.ndarray, quantile_num: int = 100) -&gt; np.ndarray\n</code></pre> <p>Calculates flow duration curves for multiple time series.</p> <p>Example: <pre><code>flows = np.random.lognormal(0, 1, (2, 365))  # 2 locations, 365 days\nfdcs = cal_fdc(flows, quantile_num=100)\n</code></pre></p>"},{"location":"api/hydro_stat/#fms","title":"fms","text":"<pre><code>def fms(obs: np.ndarray, sim: np.ndarray, lower: float = 0.2, upper: float = 0.7) -&gt; float\n</code></pre> <p>Calculates the slope of the middle section of the flow duration curve.</p>"},{"location":"api/hydro_stat/#peak-analysis","title":"Peak Analysis","text":""},{"location":"api/hydro_stat/#mean_peak_timing","title":"mean_peak_timing","text":"<pre><code>def mean_peak_timing(\n    obs: np.ndarray,\n    sim: np.ndarray,\n    window: int = None,\n    resolution: str = \"1D\",\n    datetime_coord: str = None\n) -&gt; float\n</code></pre> <p>Calculates mean difference in peak flow timing between observed and simulated flows.</p>"},{"location":"api/hydro_stat/#statistical-tests","title":"Statistical Tests","text":""},{"location":"api/hydro_stat/#wilcoxon_t_test","title":"wilcoxon_t_test","text":"<pre><code>def wilcoxon_t_test(xs: np.ndarray, xo: np.ndarray) -&gt; tuple\n</code></pre> <p>Performs Wilcoxon signed-rank test on paired samples.</p>"},{"location":"api/hydro_stat/#wilcoxon_t_test_for_lst","title":"wilcoxon_t_test_for_lst","text":"<pre><code>def wilcoxon_t_test_for_lst(x_lst: list, rnd_num: int = 2) -&gt; tuple\n</code></pre> <p>Performs pairwise Wilcoxon tests between all arrays in a list.</p>"},{"location":"api/hydro_stat/#data-transformations","title":"Data Transformations","text":""},{"location":"api/hydro_stat/#cal_stat_gamma","title":"cal_stat_gamma","text":"<pre><code>def cal_stat_gamma(x: np.ndarray) -&gt; list\n</code></pre> <p>Transforms data to approximate normal distribution and calculates statistics.</p>"},{"location":"api/hydro_stat/#cal_stat_prcp_norm","title":"cal_stat_prcp_norm","text":"<pre><code>def cal_stat_prcp_norm(x: np.ndarray, meanprep: np.ndarray) -&gt; list\n</code></pre> <p>Normalizes data by mean precipitation and calculates statistics.</p>"},{"location":"api/hydro_stat/#trans_norm","title":"trans_norm","text":"<pre><code>def trans_norm(\n    x: np.ndarray,\n    var_lst: Union[str, list],\n    stat_dict: dict,\n    *,\n    to_norm: bool\n) -&gt; np.ndarray\n</code></pre> <p>Normalizes or denormalizes data using pre-computed statistics.</p>"},{"location":"api/hydro_stat/#basic-statistics","title":"Basic Statistics","text":""},{"location":"api/hydro_stat/#cal_4_stat_inds","title":"cal_4_stat_inds","text":"<pre><code>def cal_4_stat_inds(b: np.ndarray) -&gt; list\n</code></pre> <p>Calculates four basic statistical indices for an array.</p>"},{"location":"api/hydro_stat/#cal_stat","title":"cal_stat","text":"<pre><code>def cal_stat(x: np.ndarray) -&gt; list\n</code></pre> <p>Calculates basic statistics for an array, ignoring NaN values.</p>"},{"location":"api/hydro_stat/#data-processing","title":"Data Processing","text":""},{"location":"api/hydro_stat/#remove_abnormal_data","title":"remove_abnormal_data","text":"<pre><code>def remove_abnormal_data(\n    data: np.ndarray,\n    *,\n    q1: float = 0.00001,\n    q2: float = 0.99999\n) -&gt; np.ndarray\n</code></pre> <p>Removes extreme values from data using quantile thresholds.</p>"},{"location":"api/hydro_stat/#month_stat_for_daily_df","title":"month_stat_for_daily_df","text":"<pre><code>def month_stat_for_daily_df(df: pd.DataFrame) -&gt; pd.DataFrame\n</code></pre> <p>Calculates monthly statistics from daily data.</p>"},{"location":"api/hydro_stat/#distribution-functions","title":"Distribution Functions","text":""},{"location":"api/hydro_stat/#ecdf","title":"ecdf","text":"<pre><code>def ecdf(data: np.ndarray) -&gt; tuple\n</code></pre> <p>Computes empirical cumulative distribution function (ECDF).</p>"},{"location":"api/hydro_stat/#api-reference","title":"API Reference","text":"<p>Author: MHPI group, Wenyu Ouyang Date: 2021-12-31 11:08:29 LastEditTime: 2025-08-04 09:13:42 LastEditors: Wenyu Ouyang Description: statistics calculation FilePath: \\hydroutils\\hydroutils\\hydro_stat.py Copyright (c) 2021-2022 MHPI group, Wenyu Ouyang. All rights reserved.</p>"},{"location":"api/hydro_stat/#hydroutils.hydro_stat.KGE","title":"<code>KGE(xs, xo)</code>","text":"<p>Kling Gupta Efficiency (Gupta et al., 2009, http://dx.doi.org/10.1016/j.jhydrol.2009.08.003) input:     xs: simulated     xo: observed output:     KGE: Kling Gupta Efficiency</p> Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def KGE(xs: np.ndarray, xo: np.ndarray) -&gt; float:\n    \"\"\"\n    Kling Gupta Efficiency (Gupta et al., 2009, http://dx.doi.org/10.1016/j.jhydrol.2009.08.003)\n    input:\n        xs: simulated\n        xo: observed\n    output:\n        KGE: Kling Gupta Efficiency\n    \"\"\"\n    r = np.corrcoef(xo, xs)[0, 1]\n    alpha = np.std(xs) / np.std(xo)\n    beta = np.mean(xs) / np.mean(xo)\n    return 1 - np.sqrt((r - 1) ** 2 + (alpha - 1) ** 2 + (beta - 1) ** 2)\n</code></pre>"},{"location":"api/hydro_stat/#hydroutils.hydro_stat.add_metric","title":"<code>add_metric(func_name, he_func_name, description)</code>","text":"<p>\u6dfb\u52a0\u65b0\u7684\u6307\u6807\u51fd\u6570</p>"},{"location":"api/hydro_stat/#hydroutils.hydro_stat.add_metric--parameters","title":"Parameters","text":"<p>func_name : str     \u65b0\u51fd\u6570\u7684\u540d\u79f0 he_func_name : str     HydroErr\u4e2d\u5bf9\u5e94\u51fd\u6570\u7684\u540d\u79f0 description : str     \u51fd\u6570\u63cf\u8ff0</p> Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def add_metric(func_name: str, he_func_name: str, description: str) -&gt; None:\n    \"\"\"\n    \u6dfb\u52a0\u65b0\u7684\u6307\u6807\u51fd\u6570\n\n    Parameters\n    ----------\n    func_name : str\n        \u65b0\u51fd\u6570\u7684\u540d\u79f0\n    he_func_name : str\n        HydroErr\u4e2d\u5bf9\u5e94\u51fd\u6570\u7684\u540d\u79f0\n    description : str\n        \u51fd\u6570\u63cf\u8ff0\n    \"\"\"\n    if hasattr(he, he_func_name):\n        metric_func = _create_metric_function(he_func_name, description)\n        setattr(current_module, func_name, metric_func)\n        HYDRO_METRICS[func_name] = (he_func_name, description)\n        print(f\"\u5df2\u6dfb\u52a0\u6307\u6807\u51fd\u6570: {func_name}\")\n    else:\n        print(f\"\u8b66\u544a: HydroErr\u4e2d\u4e0d\u5b58\u5728\u51fd\u6570 {he_func_name}\")\n</code></pre>"},{"location":"api/hydro_stat/#hydroutils.hydro_stat.cal_4_stat_inds","title":"<code>cal_4_stat_inds(b)</code>","text":"<p>Calculate four basic statistical indices for an array.</p> <p>This function computes four common statistical measures: 10th and 90th percentiles, mean, and standard deviation. If the standard deviation is very small (&lt; 0.001), it is set to 1 to avoid numerical issues.</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>ndarray</code> <p>Input array of numerical values.</p> required <p>Returns:</p> Type Description <code>List[float]</code> <p>List[float]: Four statistical measures in order: - p10: 10th percentile - p90: 90th percentile - mean: Arithmetic mean - std: Standard deviation (minimum 0.001)</p> Note <ul> <li>NaN values should be removed before calling this function</li> <li>If std &lt; 0.001, it is set to 1 to avoid division issues</li> <li>All returned values are cast to float type</li> </ul> Example <p>data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) p10, p90, mean, std = cal_4_stat_inds(data) print(f\"P10: {p10}, P90: {p90}, Mean: {mean}, Std: {std}\") P10: 1.9, P90: 9.1, Mean: 5.5, Std: 2.87</p> Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def cal_4_stat_inds(b: np.ndarray) -&gt; List[float]:\n    \"\"\"Calculate four basic statistical indices for an array.\n\n    This function computes four common statistical measures: 10th and 90th\n    percentiles, mean, and standard deviation. If the standard deviation is\n    very small (&lt; 0.001), it is set to 1 to avoid numerical issues.\n\n    Args:\n        b (np.ndarray): Input array of numerical values.\n\n    Returns:\n        List[float]: Four statistical measures in order:\n            - p10: 10th percentile\n            - p90: 90th percentile\n            - mean: Arithmetic mean\n            - std: Standard deviation (minimum 0.001)\n\n    Note:\n        - NaN values should be removed before calling this function\n        - If std &lt; 0.001, it is set to 1 to avoid division issues\n        - All returned values are cast to float type\n\n    Example:\n        &gt;&gt;&gt; data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        &gt;&gt;&gt; p10, p90, mean, std = cal_4_stat_inds(data)\n        &gt;&gt;&gt; print(f\"P10: {p10}, P90: {p90}, Mean: {mean}, Std: {std}\")\n        P10: 1.9, P90: 9.1, Mean: 5.5, Std: 2.87\n    \"\"\"\n    p10: float = np.percentile(b, 10).astype(float)\n    p90: float = np.percentile(b, 90).astype(float)\n    mean: float = np.mean(b).astype(float)\n    std: float = np.std(b).astype(float)\n    if std &lt; 0.001:\n        std = 1\n    return [p10, p90, mean, std]\n</code></pre>"},{"location":"api/hydro_stat/#hydroutils.hydro_stat.cal_fdc","title":"<code>cal_fdc(data, quantile_num=100)</code>","text":"<p>Calculate Flow Duration Curves (FDC) for multiple time series.</p> <p>This function computes flow duration curves for multiple time series data, typically used for analyzing streamflow characteristics. It handles NaN values and provides a specified number of quantile points.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array</code> <p>2D array of shape [n_grid, n_day] containing time series data for multiple locations/grids.</p> required <code>quantile_num</code> <code>int</code> <p>Number of quantile points to compute for each FDC. Defaults to 100.</p> <code>100</code> <p>Returns:</p> Type Description <p>np.ndarray: Array of shape [n_grid, quantile_num] containing FDC values for each location/grid.</p> Note <ul> <li>Data is sorted from high to low flow</li> <li>NaN values are removed before processing</li> <li>Empty series are filled with zeros</li> <li>Quantiles are evenly spaced from 0 to 1</li> <li>Output shape is always [n_grid, quantile_num]</li> </ul> <p>Raises:</p> Type Description <code>Exception</code> <p>If output flow array length doesn't match quantile_num.</p> Example <p>data = np.array([ ...     [10, 8, 6, 4, 2],  # First location ...     [20, 16, 12, 8, 4]  # Second location ... ]) fdc = cal_fdc(data, quantile_num=5) print(fdc) array([[10.,  8.,  6.,  4.,  2.],        [20., 16., 12.,  8.,  4.]])</p> Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def cal_fdc(data: np.array, quantile_num=100):\n    \"\"\"Calculate Flow Duration Curves (FDC) for multiple time series.\n\n    This function computes flow duration curves for multiple time series data,\n    typically used for analyzing streamflow characteristics. It handles NaN\n    values and provides a specified number of quantile points.\n\n    Args:\n        data (np.array): 2D array of shape [n_grid, n_day] containing time\n            series data for multiple locations/grids.\n        quantile_num (int, optional): Number of quantile points to compute\n            for each FDC. Defaults to 100.\n\n    Returns:\n        np.ndarray: Array of shape [n_grid, quantile_num] containing FDC\n            values for each location/grid.\n\n    Note:\n        - Data is sorted from high to low flow\n        - NaN values are removed before processing\n        - Empty series are filled with zeros\n        - Quantiles are evenly spaced from 0 to 1\n        - Output shape is always [n_grid, quantile_num]\n\n    Raises:\n        Exception: If output flow array length doesn't match quantile_num.\n\n    Example:\n        &gt;&gt;&gt; data = np.array([\n        ...     [10, 8, 6, 4, 2],  # First location\n        ...     [20, 16, 12, 8, 4]  # Second location\n        ... ])\n        &gt;&gt;&gt; fdc = cal_fdc(data, quantile_num=5)\n        &gt;&gt;&gt; print(fdc)\n        array([[10.,  8.,  6.,  4.,  2.],\n               [20., 16., 12.,  8.,  4.]])\n    \"\"\"\n    # data = n_grid * n_day\n    n_grid, n_day = data.shape\n    fdc = np.full([n_grid, quantile_num], np.nan)\n    for ii in range(n_grid):\n        temp_data0 = data[ii, :]\n        temp_data = temp_data0[~np.isnan(temp_data0)]\n        # deal with no data case for some gages\n        if len(temp_data) == 0:\n            temp_data = np.full(n_day, 0)\n        # sort from large to small\n        temp_sort = np.sort(temp_data)[::-1]\n        # select quantile_num quantile points\n        n_len = len(temp_data)\n        ind = (np.arange(quantile_num) / quantile_num * n_len).astype(int)\n        fdc_flow = temp_sort[ind]\n        if len(fdc_flow) != quantile_num:\n            raise Exception(\"unknown assimilation variable\")\n        else:\n            fdc[ii, :] = fdc_flow\n\n    return fdc\n</code></pre>"},{"location":"api/hydro_stat/#hydroutils.hydro_stat.cal_stat","title":"<code>cal_stat(x)</code>","text":"<p>Calculate basic statistics for an array, handling NaN values.</p> <p>This function computes four basic statistical measures (10th and 90th percentiles, mean, and standard deviation) while properly handling NaN values. If the array is empty after removing NaN values, a zero value is used for calculations.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input array, may contain NaN values.</p> required <p>Returns:</p> Type Description <code>List[float]</code> <p>List[float]: Four statistical measures in order: - p10: 10th percentile - p90: 90th percentile - mean: Arithmetic mean - std: Standard deviation (minimum 0.001)</p> Note <ul> <li>NaN values are automatically removed before calculations</li> <li>If all values are NaN, returns statistics for [0]</li> <li>Uses cal_4_stat_inds for actual calculations</li> <li>If std &lt; 0.001, it is set to 1 to avoid division issues</li> </ul> Example <p>data = np.array([1.0, 2.0, np.nan, 4.0, 5.0]) p10, p90, mean, std = cal_stat(data) print(f\"P10: {p10}, P90: {p90}, Mean: {mean}, Std: {std}\") P10: 1.3, P90: 4.7, Mean: 3.0, Std: 1.58</p> Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def cal_stat(x: np.ndarray) -&gt; List[float]:\n    \"\"\"Calculate basic statistics for an array, handling NaN values.\n\n    This function computes four basic statistical measures (10th and 90th\n    percentiles, mean, and standard deviation) while properly handling NaN\n    values. If the array is empty after removing NaN values, a zero value\n    is used for calculations.\n\n    Args:\n        x (np.ndarray): Input array, may contain NaN values.\n\n    Returns:\n        List[float]: Four statistical measures in order:\n            - p10: 10th percentile\n            - p90: 90th percentile\n            - mean: Arithmetic mean\n            - std: Standard deviation (minimum 0.001)\n\n    Note:\n        - NaN values are automatically removed before calculations\n        - If all values are NaN, returns statistics for [0]\n        - Uses cal_4_stat_inds for actual calculations\n        - If std &lt; 0.001, it is set to 1 to avoid division issues\n\n    Example:\n        &gt;&gt;&gt; data = np.array([1.0, 2.0, np.nan, 4.0, 5.0])\n        &gt;&gt;&gt; p10, p90, mean, std = cal_stat(data)\n        &gt;&gt;&gt; print(f\"P10: {p10}, P90: {p90}, Mean: {mean}, Std: {std}\")\n        P10: 1.3, P90: 4.7, Mean: 3.0, Std: 1.58\n    \"\"\"\n    a = x.flatten()\n    b = a[~np.isnan(a)]\n    if b.size == 0:\n        # if b is [], then give it a 0 value\n        b = np.array([0])\n    return cal_4_stat_inds(b)\n</code></pre>"},{"location":"api/hydro_stat/#hydroutils.hydro_stat.cal_stat_gamma","title":"<code>cal_stat_gamma(x)</code>","text":"<p>Transform time series data to approximate normal distribution.</p> <p>This function applies a transformation to hydrological time series data (streamflow, precipitation, evapotranspiration) to make it more normally distributed. The transformation is: log10(sqrt(x) + 0.1).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Time series data, typically daily values of: - Streamflow - Precipitation - Evapotranspiration</p> required <p>Returns:</p> Type Description <code>List[float]</code> <p>List[float]: Four statistical measures of transformed data: - p10: 10th percentile - p90: 90th percentile - mean: Arithmetic mean - std: Standard deviation (minimum 0.001)</p> Note <ul> <li>NaN values are automatically removed before transformation</li> <li>Transformation: log10(sqrt(x) + 0.1)</li> <li>This transformation helps handle gamma-distributed data</li> <li>If std &lt; 0.001, it is set to 1 to avoid division issues</li> </ul> Example <p>data = np.array([0.0, 0.1, 1.0, 10.0, np.nan, 100.0]) p10, p90, mean, std = cal_stat_gamma(data) print(f\"P10: {p10:.2f}, P90: {p90:.2f}\") P10: -0.52, P90: 1.01</p> Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def cal_stat_gamma(x: np.ndarray) -&gt; List[float]:\n    \"\"\"Transform time series data to approximate normal distribution.\n\n    This function applies a transformation to hydrological time series data\n    (streamflow, precipitation, evapotranspiration) to make it more normally\n    distributed. The transformation is: log10(sqrt(x) + 0.1).\n\n    Args:\n        x (np.ndarray): Time series data, typically daily values of:\n            - Streamflow\n            - Precipitation\n            - Evapotranspiration\n\n    Returns:\n        List[float]: Four statistical measures of transformed data:\n            - p10: 10th percentile\n            - p90: 90th percentile\n            - mean: Arithmetic mean\n            - std: Standard deviation (minimum 0.001)\n\n    Note:\n        - NaN values are automatically removed before transformation\n        - Transformation: log10(sqrt(x) + 0.1)\n        - This transformation helps handle gamma-distributed data\n        - If std &lt; 0.001, it is set to 1 to avoid division issues\n\n    Example:\n        &gt;&gt;&gt; data = np.array([0.0, 0.1, 1.0, 10.0, np.nan, 100.0])\n        &gt;&gt;&gt; p10, p90, mean, std = cal_stat_gamma(data)\n        &gt;&gt;&gt; print(f\"P10: {p10:.2f}, P90: {p90:.2f}\")\n        P10: -0.52, P90: 1.01\n    \"\"\"\n    a = x.flatten()\n    b = a[~np.isnan(a)]  # kick out Nan\n    b = np.log10(\n        np.sqrt(b) + 0.1\n    )  # do some tranformation to change gamma characteristics\n    return cal_4_stat_inds(b)\n</code></pre>"},{"location":"api/hydro_stat/#hydroutils.hydro_stat.cal_stat_prcp_norm","title":"<code>cal_stat_prcp_norm(x, meanprep)</code>","text":"<p>Normalize variables by precipitation and calculate gamma statistics.</p> <p>This function normalizes a variable (e.g., streamflow) by mean precipitation to remove the influence of rainfall magnitude, making statistics comparable between dry and wet basins. After normalization, gamma transformation is applied.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Data to be normalized, typically streamflow or other hydrological variables.</p> required <code>meanprep</code> <code>ndarray</code> <p>Mean precipitation values for normalization. Usually obtained from basin attributes (e.g., p_mean).</p> required <p>Returns:</p> Type Description <p>List[float]: Four statistical measures of normalized data: - p10: 10th percentile - p90: 90th percentile - mean: Arithmetic mean - std: Standard deviation (minimum 0.001)</p> Note <ul> <li>Normalization: x / meanprep (unit: mm/day / mm/day)</li> <li>After normalization, gamma transformation is applied</li> <li>Helps compare basins with different precipitation regimes</li> <li>If std &lt; 0.001, it is set to 1 to avoid division issues</li> </ul> Example <p>data = np.array([[10.0, 20.0], [30.0, 40.0]])  # 2 basins, 2 timesteps mean_prep = np.array([100.0, 200.0])  # Mean prep for 2 basins p10, p90, mean, std = cal_stat_prcp_norm(data, mean_prep) print(f\"P10: {p10:.3f}, P90: {p90:.3f}\") P10: -0.523, P90: -0.398</p> Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def cal_stat_prcp_norm(x, meanprep):\n    \"\"\"Normalize variables by precipitation and calculate gamma statistics.\n\n    This function normalizes a variable (e.g., streamflow) by mean precipitation\n    to remove the influence of rainfall magnitude, making statistics comparable\n    between dry and wet basins. After normalization, gamma transformation is\n    applied.\n\n    Args:\n        x (np.ndarray): Data to be normalized, typically streamflow or other\n            hydrological variables.\n        meanprep (np.ndarray): Mean precipitation values for normalization.\n            Usually obtained from basin attributes (e.g., p_mean).\n\n    Returns:\n        List[float]: Four statistical measures of normalized data:\n            - p10: 10th percentile\n            - p90: 90th percentile\n            - mean: Arithmetic mean\n            - std: Standard deviation (minimum 0.001)\n\n    Note:\n        - Normalization: x / meanprep (unit: mm/day / mm/day)\n        - After normalization, gamma transformation is applied\n        - Helps compare basins with different precipitation regimes\n        - If std &lt; 0.001, it is set to 1 to avoid division issues\n\n    Example:\n        &gt;&gt;&gt; data = np.array([[10.0, 20.0], [30.0, 40.0]])  # 2 basins, 2 timesteps\n        &gt;&gt;&gt; mean_prep = np.array([100.0, 200.0])  # Mean prep for 2 basins\n        &gt;&gt;&gt; p10, p90, mean, std = cal_stat_prcp_norm(data, mean_prep)\n        &gt;&gt;&gt; print(f\"P10: {p10:.3f}, P90: {p90:.3f}\")\n        P10: -0.523, P90: -0.398\n    \"\"\"\n    # meanprep = readAttr(gageDict['id'], ['q_mean'])\n    tempprep = np.tile(meanprep, (1, x.shape[1]))\n    # unit (mm/day)/(mm/day)\n    flowua = x / tempprep\n    return cal_stat_gamma(flowua)\n</code></pre>"},{"location":"api/hydro_stat/#hydroutils.hydro_stat.ecdf","title":"<code>ecdf(data)</code>","text":"<p>Compute Empirical Cumulative Distribution Function (ECDF).</p> <p>This function calculates the empirical CDF for a given dataset. The ECDF shows the fraction of observations less than or equal to each data point.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input data array.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray]: Two arrays: - x: Sorted input data - y: Cumulative probabilities (0 to 1)</p> Note <ul> <li>Data is sorted in ascending order</li> <li>Probabilities are calculated as (i)/(n) for i=1..n</li> <li>No special handling of NaN values - remove them before calling</li> </ul> Example <p>data = np.array([1, 2, 2, 3, 3, 3, 4, 4, 5]) x, y = ecdf(data) print(\"Values:\", x) Values: [1 2 2 3 3 3 4 4 5] print(\"Probabilities:\", y) Probabilities: [0.111 0.222 0.333 0.444 0.556 0.667 0.778 0.889 1.000]</p> Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def ecdf(data: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Compute Empirical Cumulative Distribution Function (ECDF).\n\n    This function calculates the empirical CDF for a given dataset. The ECDF\n    shows the fraction of observations less than or equal to each data point.\n\n    Args:\n        data (np.ndarray): Input data array.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: Two arrays:\n            - x: Sorted input data\n            - y: Cumulative probabilities (0 to 1)\n\n    Note:\n        - Data is sorted in ascending order\n        - Probabilities are calculated as (i)/(n) for i=1..n\n        - No special handling of NaN values - remove them before calling\n\n    Example:\n        &gt;&gt;&gt; data = np.array([1, 2, 2, 3, 3, 3, 4, 4, 5])\n        &gt;&gt;&gt; x, y = ecdf(data)\n        &gt;&gt;&gt; print(\"Values:\", x)\n        Values: [1 2 2 3 3 3 4 4 5]\n        &gt;&gt;&gt; print(\"Probabilities:\", y)\n        Probabilities: [0.111 0.222 0.333 0.444 0.556 0.667 0.778 0.889 1.000]\n    \"\"\"\n    x = np.sort(data)\n    n = x.size\n    y = np.arange(1, n + 1) / n\n    return (x, y)\n</code></pre>"},{"location":"api/hydro_stat/#hydroutils.hydro_stat.flood_peak_error","title":"<code>flood_peak_error(Q_obs, Q_sim)</code>","text":"<p>Calculate relative flood peak error.</p>"},{"location":"api/hydro_stat/#hydroutils.hydro_stat.flood_peak_error--parameters","title":"Parameters","text":"<p>Q_obs : array-like     Observed streamflow. Q_sim : array-like     Simulated streamflow.</p>"},{"location":"api/hydro_stat/#hydroutils.hydro_stat.flood_peak_error--returns","title":"Returns","text":"<p>float     Relative flood peak error (%).</p> Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def flood_peak_error(Q_obs, Q_sim):\n    \"\"\"\n    Calculate relative flood peak error.\n\n    Parameters\n    ----------\n    Q_obs : array-like\n        Observed streamflow.\n    Q_sim : array-like\n        Simulated streamflow.\n\n    Returns\n    -------\n    float\n        Relative flood peak error (%).\n    \"\"\"\n    peak_obs = np.max(Q_obs)\n    peak_sim = np.max(Q_sim)\n\n    if peak_obs &gt; 1e-6:\n        return ((peak_sim - peak_obs) / peak_obs) * 100.0\n    else:\n        return np.nan\n</code></pre>"},{"location":"api/hydro_stat/#hydroutils.hydro_stat.flood_peak_timing","title":"<code>flood_peak_timing(obs, sim, window=None, resolution='1D', datetime_coord=None)</code>","text":"<p>Calculate mean difference in peak flow timing (simplified version for numpy arrays).</p> <p>Uses scipy.find_peaks to find peaks in the observed time series. Starting with all observed peaks, those with a prominence of less than the standard deviation of the observed time series are discarded. Next, the lowest peaks are subsequently discarded until all remaining peaks have a distance of at least 100 steps. Finally, the corresponding peaks in the simulated time series are searched in a window of size <code>window</code> on either side of the observed peaks and the absolute time differences between observed and simulated peaks is calculated. The final metric is the mean absolute time difference across all peaks (in time steps).</p>"},{"location":"api/hydro_stat/#hydroutils.hydro_stat.flood_peak_timing--parameters","title":"Parameters","text":"<p>obs : np.ndarray     Observed time series. sim : np.ndarray     Simulated time series. window : int, optional     Size of window to consider on each side of the observed peak for finding the simulated peak. That is, the total     window length to find the peak in the simulations is 2 * window + 1 centered at the observed     peak. The default depends on the temporal resolution, e.g. for a resolution of '1D', a window of 3 is used and     for a resolution of '1H' the window size is 12. resolution : str, optional     Temporal resolution of the time series in pandas format, e.g. '1D' for daily and '1H' for hourly.     Currently used only for determining default window size. datetime_coord : str, optional     Name of datetime coordinate. Currently unused in this simplified implementation.</p>"},{"location":"api/hydro_stat/#hydroutils.hydro_stat.flood_peak_timing--returns","title":"Returns","text":"<p>float     Mean peak time difference in time steps. Returns NaN if no peaks are found.</p>"},{"location":"api/hydro_stat/#hydroutils.hydro_stat.flood_peak_timing--references","title":"References","text":"<p>.. [#] Kratzert, F., Klotz, D., Hochreiter, S., and Nearing, G. S.: A note on leveraging synergy in multiple     meteorological datasets with deep learning for rainfall-runoff modeling, Hydrol. Earth Syst. Sci.,     https://doi.org/10.5194/hess-2020-221</p> Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def flood_peak_timing(\n    obs: np.ndarray,\n    sim: np.ndarray,\n    window: Optional[int] = None,\n    resolution: str = \"1D\",\n    datetime_coord: Optional[str] = None,\n) -&gt; float:\n    \"\"\"\n    Calculate mean difference in peak flow timing (simplified version for numpy arrays).\n\n    Uses scipy.find_peaks to find peaks in the observed time series. Starting with all observed peaks, those with a\n    prominence of less than the standard deviation of the observed time series are discarded. Next, the lowest peaks\n    are subsequently discarded until all remaining peaks have a distance of at least 100 steps. Finally, the\n    corresponding peaks in the simulated time series are searched in a window of size `window` on either side of the\n    observed peaks and the absolute time differences between observed and simulated peaks is calculated.\n    The final metric is the mean absolute time difference across all peaks (in time steps).\n\n    Parameters\n    ----------\n    obs : np.ndarray\n        Observed time series.\n    sim : np.ndarray\n        Simulated time series.\n    window : int, optional\n        Size of window to consider on each side of the observed peak for finding the simulated peak. That is, the total\n        window length to find the peak in the simulations is 2 * window + 1 centered at the observed\n        peak. The default depends on the temporal resolution, e.g. for a resolution of '1D', a window of 3 is used and\n        for a resolution of '1H' the window size is 12.\n    resolution : str, optional\n        Temporal resolution of the time series in pandas format, e.g. '1D' for daily and '1H' for hourly.\n        Currently used only for determining default window size.\n    datetime_coord : str, optional\n        Name of datetime coordinate. Currently unused in this simplified implementation.\n\n    Returns\n    -------\n    float\n        Mean peak time difference in time steps. Returns NaN if no peaks are found.\n\n    References\n    ----------\n    .. [#] Kratzert, F., Klotz, D., Hochreiter, S., and Nearing, G. S.: A note on leveraging synergy in multiple\n        meteorological datasets with deep learning for rainfall-runoff modeling, Hydrol. Earth Syst. Sci.,\n        https://doi.org/10.5194/hess-2020-221\n    \"\"\"\n    # verify inputs\n    _validate_inputs(obs, sim)\n\n    # get time series with only valid observations (scipy's find_peaks doesn't guarantee correctness with NaNs)\n    obs_clean, sim_clean = _mask_valid(obs, sim)\n\n    if len(obs_clean) &lt; 3:\n        return np.nan\n\n    # determine default window size based on resolution\n    if window is None:\n        # infer a reasonable window size based on resolution\n        window = max(int(_get_frequency_factor(\"12H\", resolution)), 3)\n\n    # heuristic to get indices of peaks and their corresponding height.\n    # Use prominence based on standard deviation to filter significant peaks\n    prominence_threshold = np.std(obs_clean)\n    if prominence_threshold == 0:  # Handle constant time series\n        prominence_threshold = (\n            0.01 * np.mean(obs_clean) if np.mean(obs_clean) != 0 else 0.01\n        )\n\n    peaks, _ = signal.find_peaks(\n        obs_clean, distance=100, prominence=prominence_threshold\n    )\n\n    if len(peaks) == 0:\n        return np.nan\n\n    # evaluate timing\n    timing_errors = []\n    for idx in peaks:\n        # skip peaks at the start and end of the sequence\n        if (idx - window &lt; 0) or (idx + window &gt;= len(obs_clean)):\n            continue\n\n        # find the corresponding peak in simulated data within the window\n        window_start = max(0, idx - window)\n        window_end = min(len(sim_clean), idx + window + 1)\n        sim_window = sim_clean[window_start:window_end]\n\n        # find the index of maximum value in the window\n        local_peak_idx = np.argmax(sim_window)\n        global_peak_idx = window_start + local_peak_idx\n\n        # calculate the time difference between the peaks (in time steps)\n        timing_error = abs(idx - global_peak_idx)\n        timing_errors.append(timing_error)\n\n    return np.mean(timing_errors) if timing_errors else np.nan\n</code></pre>"},{"location":"api/hydro_stat/#hydroutils.hydro_stat.flood_volume_error","title":"<code>flood_volume_error(Q_obs, Q_sim, delta_t_seconds=10800)</code>","text":"<p>Calculate relative flood volume error.</p>"},{"location":"api/hydro_stat/#hydroutils.hydro_stat.flood_volume_error--parameters","title":"Parameters","text":"<p>Q_obs : array-like     Observed streamflow. Q_sim : array-like     Simulated streamflow. delta_t_seconds : int, optional     Time step in seconds, by default 10800 (3 hours).</p>"},{"location":"api/hydro_stat/#hydroutils.hydro_stat.flood_volume_error--returns","title":"Returns","text":"<p>float     Relative flood volume error (%).</p> Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def flood_volume_error(Q_obs, Q_sim, delta_t_seconds=10800):\n    \"\"\"\n    Calculate relative flood volume error.\n\n    Parameters\n    ----------\n    Q_obs : array-like\n        Observed streamflow.\n    Q_sim : array-like\n        Simulated streamflow.\n    delta_t_seconds : int, optional\n        Time step in seconds, by default 10800 (3 hours).\n\n    Returns\n    -------\n    float\n        Relative flood volume error (%).\n    \"\"\"\n    vol_obs = np.sum(Q_obs) * delta_t_seconds\n    vol_sim = np.sum(Q_sim) * delta_t_seconds\n\n    if vol_obs &gt; 1e-6:\n        return ((vol_sim - vol_obs) / vol_obs) * 100.0\n    else:\n        return np.nan\n</code></pre>"},{"location":"api/hydro_stat/#hydroutils.hydro_stat.fms","title":"<code>fms(obs, sim, lower=0.2, upper=0.7)</code>","text":"<p>TODO: not fully tested Calculate the slope of the middle section of the flow duration curve [#]_</p> <p>.. math::     \\%\\text{BiasFMS} = \\frac{\\left | \\log(Q_{s,\\text{lower}}) - \\log(Q_{s,\\text{upper}}) \\right | -         \\left | \\log(Q_{o,\\text{lower}}) - \\log(Q_{o,\\text{upper}}) \\right |}{\\left |         \\log(Q_{s,\\text{lower}}) - \\log(Q_{s,\\text{upper}}) \\right |} \\times 100,</p> <p>where :math:<code>Q_{s,\\text{lower/upper}}</code> corresponds to the FDC of the simulations (here, <code>sim</code>) at the <code>lower</code> and <code>upper</code> bound of the middle section and :math:<code>Q_{o,\\text{lower/upper}}</code> similarly for the observations (here, <code>obs</code>).</p>"},{"location":"api/hydro_stat/#hydroutils.hydro_stat.fms--parameters","title":"Parameters","text":"<p>obs : DataArray     Observed time series. sim : DataArray     Simulated time series. lower : float, optional     Lower bound of the middle section in range ]0,1[, by default 0.2 upper : float, optional     Upper bound of the middle section in range ]0,1[, by default 0.7</p>"},{"location":"api/hydro_stat/#hydroutils.hydro_stat.fms--returns","title":"Returns","text":"<p>float     Slope of the middle section of the flow duration curve.</p>"},{"location":"api/hydro_stat/#hydroutils.hydro_stat.fms--references","title":"References","text":"<p>.. [#] Yilmaz, K. K., Gupta, H. V., and Wagener, T. ( 2008), A process-based diagnostic approach to model     evaluation: Application to the NWS distributed hydrologic model, Water Resour. Res., 44, W09417,     doi:10.1029/2007WR006716.</p> Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def fms(\n    obs: np.ndarray, sim: np.ndarray, lower: float = 0.2, upper: float = 0.7\n) -&gt; float:\n    r\"\"\"\n    TODO: not fully tested\n    Calculate the slope of the middle section of the flow duration curve [#]_\n\n    .. math::\n        \\%\\text{BiasFMS} = \\frac{\\left | \\log(Q_{s,\\text{lower}}) - \\log(Q_{s,\\text{upper}}) \\right | -\n            \\left | \\log(Q_{o,\\text{lower}}) - \\log(Q_{o,\\text{upper}}) \\right |}{\\left |\n            \\log(Q_{s,\\text{lower}}) - \\log(Q_{s,\\text{upper}}) \\right |} \\times 100,\n\n    where :math:`Q_{s,\\text{lower/upper}}` corresponds to the FDC of the simulations (here, `sim`) at the `lower` and\n    `upper` bound of the middle section and :math:`Q_{o,\\text{lower/upper}}` similarly for the observations (here,\n    `obs`).\n\n    Parameters\n    ----------\n    obs : DataArray\n        Observed time series.\n    sim : DataArray\n        Simulated time series.\n    lower : float, optional\n        Lower bound of the middle section in range ]0,1[, by default 0.2\n    upper : float, optional\n        Upper bound of the middle section in range ]0,1[, by default 0.7\n\n    Returns\n    -------\n    float\n        Slope of the middle section of the flow duration curve.\n\n    References\n    ----------\n    .. [#] Yilmaz, K. K., Gupta, H. V., and Wagener, T. ( 2008), A process-based diagnostic approach to model\n        evaluation: Application to the NWS distributed hydrologic model, Water Resour. Res., 44, W09417,\n        doi:10.1029/2007WR006716.\n    \"\"\"\n    if len(obs) &lt; 1:\n        return np.nan\n\n    if any((x &lt;= 0) or (x &gt;= 1) for x in [upper, lower]):\n        raise ValueError(\"upper and lower have to be in range ]0,1[\")\n\n    if lower &gt;= upper:\n        raise ValueError(\"The lower threshold has to be smaller than the upper.\")\n\n    # get arrays of sorted (descending) discharges\n    obs = np.sort(obs)\n    sim = np.sort(sim)\n\n    # for numerical reasons change 0s to 1e-6. Simulations can still contain negatives, so also reset those.\n    sim[sim &lt;= 0] = 1e-6\n    obs[obs == 0] = 1e-6\n\n    # calculate fms part by part\n    qsm_lower = np.log(sim[np.round(lower * len(sim)).astype(int)])\n    qsm_upper = np.log(sim[np.round(upper * len(sim)).astype(int)])\n    qom_lower = np.log(obs[np.round(lower * len(obs)).astype(int)])\n    qom_upper = np.log(obs[np.round(upper * len(obs)).astype(int)])\n\n    fms = ((qsm_lower - qsm_upper) - (qom_lower - qom_upper)) / (\n        qom_lower - qom_upper + 1e-6\n    )\n\n    return fms * 100\n</code></pre>"},{"location":"api/hydro_stat/#hydroutils.hydro_stat.month_stat_for_daily_df","title":"<code>month_stat_for_daily_df(df)</code>","text":"<p>Calculate monthly statistics from daily data.</p> <p>This function resamples daily data to monthly frequency by computing the mean value for each month. It ensures the input DataFrame has a datetime index before resampling.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing daily data with datetime index or index that can be converted to datetime.</p> required <p>Returns:</p> Type Description <p>pd.DataFrame: DataFrame containing monthly statistics (means). Index is the start of each month.</p> Note <ul> <li>Uses pandas resample with 'MS' (month start) frequency</li> <li>Automatically converts index to datetime if needed</li> <li>Computes mean value for each month</li> <li>Handles missing values according to pandas defaults</li> </ul> Example <p>dates = pd.date_range('2020-01-01', '2020-12-31', freq='D') data = pd.DataFrame({'value': range(366)}, index=dates) monthly = month_stat_for_daily_df(data) print(monthly.head())                value 2020-01-01  15.0 2020-02-01  45.5 2020-03-01  74.0 2020-04-01  105.0 2020-05-01  135.5</p> Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def month_stat_for_daily_df(df):\n    \"\"\"Calculate monthly statistics from daily data.\n\n    This function resamples daily data to monthly frequency by computing the\n    mean value for each month. It ensures the input DataFrame has a datetime\n    index before resampling.\n\n    Args:\n        df (pd.DataFrame): DataFrame containing daily data with datetime index\n            or index that can be converted to datetime.\n\n    Returns:\n        pd.DataFrame: DataFrame containing monthly statistics (means).\n            Index is the start of each month.\n\n    Note:\n        - Uses pandas resample with 'MS' (month start) frequency\n        - Automatically converts index to datetime if needed\n        - Computes mean value for each month\n        - Handles missing values according to pandas defaults\n\n    Example:\n        &gt;&gt;&gt; dates = pd.date_range('2020-01-01', '2020-12-31', freq='D')\n        &gt;&gt;&gt; data = pd.DataFrame({'value': range(366)}, index=dates)\n        &gt;&gt;&gt; monthly = month_stat_for_daily_df(data)\n        &gt;&gt;&gt; print(monthly.head())\n                       value\n        2020-01-01  15.0\n        2020-02-01  45.5\n        2020-03-01  74.0\n        2020-04-01  105.0\n        2020-05-01  135.5\n    \"\"\"\n    # guarantee the index is datetime\n    df.index = pd.to_datetime(df.index)\n    return df.resample(\"MS\").mean()\n</code></pre>"},{"location":"api/hydro_stat/#hydroutils.hydro_stat.pbias","title":"<code>pbias(observed, simulated)</code>","text":"<p>Calculate Percent Bias (PBIAS)</p>"},{"location":"api/hydro_stat/#hydroutils.hydro_stat.pbias--parameters","title":"Parameters","text":"<p>observed : array-like     Observed values simulated : array-like     Simulated values</p>"},{"location":"api/hydro_stat/#hydroutils.hydro_stat.pbias--returns","title":"Returns","text":"<p>float     Percent bias value</p> Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def pbias(\n    observed: Union[np.ndarray, List[float]], simulated: Union[np.ndarray, List[float]]\n) -&gt; float:\n    \"\"\"\n    Calculate Percent Bias (PBIAS)\n\n    Parameters\n    ----------\n    observed : array-like\n        Observed values\n    simulated : array-like\n        Simulated values\n\n    Returns\n    -------\n    float\n        Percent bias value\n    \"\"\"\n    observed = np.asarray(observed)\n    simulated = np.asarray(simulated)\n\n    # Remove NaN values\n    mask = ~(np.isnan(observed) | np.isnan(simulated))\n    observed = observed[mask]\n    simulated = simulated[mask]\n\n    if len(observed) == 0:\n        return np.nan\n\n    return np.sum(simulated - observed) / np.sum(observed) * 100\n</code></pre>"},{"location":"api/hydro_stat/#hydroutils.hydro_stat.remove_abnormal_data","title":"<code>remove_abnormal_data(data, *, q1=1e-05, q2=0.99999)</code>","text":"<p>Remove extreme values from data using quantile thresholds.</p> <p>This function removes data points that fall outside specified quantile ranges by replacing them with NaN values. This is useful for removing outliers or extreme values that might affect analysis.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input data array.</p> required <code>q1</code> <code>float</code> <p>Lower quantile threshold. Values below this quantile will be replaced with NaN. Defaults to 0.00001.</p> <code>1e-05</code> <code>q2</code> <code>float</code> <p>Upper quantile threshold. Values above this quantile will be replaced with NaN. Defaults to 0.99999.</p> <code>0.99999</code> <p>Returns:</p> Type Description <p>np.ndarray: Data array with extreme values replaced by NaN.</p> Note <ul> <li>Uses numpy.quantile for threshold calculation</li> <li>Values equal to thresholds are kept</li> <li>Original array shape is preserved</li> <li>NaN values in input are preserved</li> <li>Default thresholds keep 99.998% of data</li> </ul> Example <p>data = np.array([1, 2, 3, 100, 4, 5, 0.001, 6]) cleaned = remove_abnormal_data(data, q1=0.1, q2=0.9) print(cleaned) array([nan,  2.,  3.,  nan,  4.,  5.,  nan,  6.])</p> Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def remove_abnormal_data(data, *, q1=0.00001, q2=0.99999):\n    \"\"\"Remove extreme values from data using quantile thresholds.\n\n    This function removes data points that fall outside specified quantile\n    ranges by replacing them with NaN values. This is useful for removing\n    outliers or extreme values that might affect analysis.\n\n    Args:\n        data (np.ndarray): Input data array.\n        q1 (float, optional): Lower quantile threshold. Values below this\n            quantile will be replaced with NaN. Defaults to 0.00001.\n        q2 (float, optional): Upper quantile threshold. Values above this\n            quantile will be replaced with NaN. Defaults to 0.99999.\n\n    Returns:\n        np.ndarray: Data array with extreme values replaced by NaN.\n\n    Note:\n        - Uses numpy.quantile for threshold calculation\n        - Values equal to thresholds are kept\n        - Original array shape is preserved\n        - NaN values in input are preserved\n        - Default thresholds keep 99.998% of data\n\n    Example:\n        &gt;&gt;&gt; data = np.array([1, 2, 3, 100, 4, 5, 0.001, 6])\n        &gt;&gt;&gt; cleaned = remove_abnormal_data(data, q1=0.1, q2=0.9)\n        &gt;&gt;&gt; print(cleaned)\n        array([nan,  2.,  3.,  nan,  4.,  5.,  nan,  6.])\n    \"\"\"\n    # remove abnormal data\n    data[data &lt; np.quantile(data, q1)] = np.nan\n    data[data &gt; np.quantile(data, q2)] = np.nan\n    return data\n</code></pre>"},{"location":"api/hydro_stat/#hydroutils.hydro_stat.stat_error","title":"<code>stat_error(target, pred, fill_nan='no')</code>","text":"<p>Calculate statistical metrics for 2D arrays with NaN handling options.</p> <p>This function computes multiple statistical metrics comparing predicted values against target (observed) values for multiple time series (e.g., multiple basins). It provides different options for handling NaN values.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>ndarray</code> <p>Target (observed) values. 2D array [basin, sequence].</p> required <code>pred</code> <code>ndarray</code> <p>Predicted values. Same shape as target.</p> required <code>fill_nan</code> <code>str</code> <p>Method for handling NaN values. Options: - \"no\": Ignore NaN values (default) - \"sum\": Sum values in NaN locations - \"mean\": Average values in NaN locations</p> <code>'no'</code> <p>Returns:</p> Type Description <code>Union[Dict[str, ndarray], Dict[str, List[float]]]</code> <p>Union[Dict[str, np.ndarray], Dict[str, List[float]]]: Dictionary with metrics: - Bias: Mean error - RMSE: Root mean square error - ubRMSE: Unbiased root mean square error - Corr: Pearson correlation coefficient - R2: Coefficient of determination - NSE: Nash-Sutcliffe efficiency - KGE: Kling-Gupta efficiency - FHV: Peak flow bias (top 2%) - FLV: Low flow bias (bottom 30%)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input arrays have wrong dimensions or incompatible shapes.</p> Note <p>For fill_nan options: - \"no\": [1, nan, nan, 2] vs [0.3, 0.3, 0.3, 1.5] becomes [1, 2] vs [0.3, 1.5] - \"sum\": [1, nan, nan, 2] vs [0.3, 0.3, 0.3, 1.5] becomes [1, 2] vs [0.9, 1.5] - \"mean\": Similar to \"sum\" but takes average instead of sum</p> Example <p>target = np.array([[1.0, np.nan, np.nan, 2.0], ...                    [3.0, 4.0, np.nan, 6.0]]) pred = np.array([[1.1, 0.3, 0.3, 1.9], ...                  [3.2, 3.8, 0.5, 5.8]]) metrics = stat_error(target, pred, fill_nan=\"sum\") print(metrics['RMSE'])  # Example output array([0.158, 0.245])</p> Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def stat_error(\n    target: np.ndarray, pred: np.ndarray, fill_nan: str = \"no\"\n) -&gt; Union[Dict[str, np.ndarray], Dict[str, List[float]]]:\n    \"\"\"Calculate statistical metrics for 2D arrays with NaN handling options.\n\n    This function computes multiple statistical metrics comparing predicted values\n    against target (observed) values for multiple time series (e.g., multiple\n    basins). It provides different options for handling NaN values.\n\n    Args:\n        target (np.ndarray): Target (observed) values. 2D array [basin, sequence].\n        pred (np.ndarray): Predicted values. Same shape as target.\n        fill_nan (str, optional): Method for handling NaN values. Options:\n            - \"no\": Ignore NaN values (default)\n            - \"sum\": Sum values in NaN locations\n            - \"mean\": Average values in NaN locations\n\n    Returns:\n        Union[Dict[str, np.ndarray], Dict[str, List[float]]]: Dictionary with metrics:\n            - Bias: Mean error\n            - RMSE: Root mean square error\n            - ubRMSE: Unbiased root mean square error\n            - Corr: Pearson correlation coefficient\n            - R2: Coefficient of determination\n            - NSE: Nash-Sutcliffe efficiency\n            - KGE: Kling-Gupta efficiency\n            - FHV: Peak flow bias (top 2%)\n            - FLV: Low flow bias (bottom 30%)\n\n    Raises:\n        ValueError: If input arrays have wrong dimensions or incompatible shapes.\n\n    Note:\n        For fill_nan options:\n        - \"no\": [1, nan, nan, 2] vs [0.3, 0.3, 0.3, 1.5] becomes [1, 2] vs [0.3, 1.5]\n        - \"sum\": [1, nan, nan, 2] vs [0.3, 0.3, 0.3, 1.5] becomes [1, 2] vs [0.9, 1.5]\n        - \"mean\": Similar to \"sum\" but takes average instead of sum\n\n    Example:\n        &gt;&gt;&gt; target = np.array([[1.0, np.nan, np.nan, 2.0],\n        ...                    [3.0, 4.0, np.nan, 6.0]])\n        &gt;&gt;&gt; pred = np.array([[1.1, 0.3, 0.3, 1.9],\n        ...                  [3.2, 3.8, 0.5, 5.8]])\n        &gt;&gt;&gt; metrics = stat_error(target, pred, fill_nan=\"sum\")\n        &gt;&gt;&gt; print(metrics['RMSE'])  # Example output\n        array([0.158, 0.245])\n    \"\"\"\n    if len(target.shape) == 3:\n        raise ValueError(\n            \"The input data should be 2-dim, not 3-dim. If you want to calculate metrics for 3-d arrays, please use stat_errors function.\"\n        )\n    if type(fill_nan) is not str:\n        raise ValueError(\"fill_nan should be a string.\")\n    if target.shape != pred.shape:\n        raise ValueError(\"The shape of target and pred should be the same.\")\n    if fill_nan != \"no\":\n        each_non_nan_idx = []\n        all_non_nan_idx: list[int] = []\n        for i in range(target.shape[0]):\n            tmp = target[i]\n            non_nan_idx_tmp = [j for j in range(tmp.size) if not np.isnan(tmp[j])]\n            each_non_nan_idx.append(non_nan_idx_tmp)\n            # TODO: now all_non_nan_idx is only set for ET, because of its irregular nan values\n            all_non_nan_idx = all_non_nan_idx + non_nan_idx_tmp\n            non_nan_idx = np.unique(all_non_nan_idx).tolist()\n        # some NaN data appear in different dates in different basins, so we have to calculate the metric for each basin\n        # but for ET, it is not very resonable to calculate the metric for each basin in this way, for example,\n        # the non_nan_idx: [1, 9, 17, 33, 41], then there are 16 elements in 17 -&gt; 33, so use all_non_nan_idx is better\n        # hence we don't use each_non_nan_idx finally\n        out_dict: Dict[str, List[float]] = dict(\n            Bias=[],\n            RMSE=[],\n            ubRMSE=[],\n            Corr=[],\n            R2=[],\n            NSE=[],\n            KGE=[],\n            FHV=[],\n            FLV=[],\n        )\n    if fill_nan == \"sum\":\n        for i in range(target.shape[0]):\n            tmp = target[i]\n            # non_nan_idx = each_non_nan_idx[i]\n            targ_i = tmp[non_nan_idx]\n            pred_i = np.add.reduceat(pred[i], non_nan_idx)\n            dict_i = stat_error_i(targ_i, pred_i)\n            out_dict[\"Bias\"].append(dict_i[\"Bias\"])\n            out_dict[\"RMSE\"].append(dict_i[\"RMSE\"])\n            out_dict[\"ubRMSE\"].append(dict_i[\"ubRMSE\"])\n            out_dict[\"Corr\"].append(dict_i[\"Corr\"])\n            out_dict[\"R2\"].append(dict_i[\"R2\"])\n            out_dict[\"NSE\"].append(dict_i[\"NSE\"])\n            out_dict[\"KGE\"].append(dict_i[\"KGE\"])\n            out_dict[\"FHV\"].append(dict_i[\"FHV\"])\n            out_dict[\"FLV\"].append(dict_i[\"FLV\"])\n        return out_dict\n    elif fill_nan == \"mean\":\n        for i in range(target.shape[0]):\n            tmp = target[i]\n            # non_nan_idx = each_non_nan_idx[i]\n            targ_i = tmp[non_nan_idx]\n            pred_i_sum = np.add.reduceat(pred[i], non_nan_idx)\n            if non_nan_idx[-1] &lt; len(pred[i]):\n                idx4mean = non_nan_idx + [len(pred[i])]\n            else:\n                idx4mean = copy.copy(non_nan_idx)\n            idx_interval = [y - x for x, y in zip(idx4mean, idx4mean[1:])]\n            pred_i = pred_i_sum / idx_interval\n            dict_i = stat_error_i(targ_i, pred_i)\n            out_dict[\"Bias\"].append(dict_i[\"Bias\"])\n            out_dict[\"RMSE\"].append(dict_i[\"RMSE\"])\n            out_dict[\"ubRMSE\"].append(dict_i[\"ubRMSE\"])\n            out_dict[\"Corr\"].append(dict_i[\"Corr\"])\n            out_dict[\"R2\"].append(dict_i[\"R2\"])\n            out_dict[\"NSE\"].append(dict_i[\"NSE\"])\n            out_dict[\"KGE\"].append(dict_i[\"KGE\"])\n            out_dict[\"FHV\"].append(dict_i[\"FHV\"])\n            out_dict[\"FLV\"].append(dict_i[\"FLV\"])\n        return out_dict\n    ngrid, nt = pred.shape\n    # Bias\n    Bias = np.nanmean(pred - target, axis=1)\n    # RMSE\n    RMSE = np.sqrt(np.nanmean((pred - target) ** 2, axis=1))\n    # ubRMSE\n    predMean = np.tile(np.nanmean(pred, axis=1), (nt, 1)).transpose()\n    targetMean = np.tile(np.nanmean(target, axis=1), (nt, 1)).transpose()\n    predAnom = pred - predMean\n    targetAnom = target - targetMean\n    ubRMSE = np.sqrt(np.nanmean((predAnom - targetAnom) ** 2, axis=1))\n    # rho R2 NSE\n    Corr = np.full(ngrid, np.nan)\n    R2 = np.full(ngrid, np.nan)\n    NSE = np.full(ngrid, np.nan)\n    KGe = np.full(ngrid, np.nan)\n    PBiaslow = np.full(ngrid, np.nan)\n    PBiashigh = np.full(ngrid, np.nan)\n    PBias = np.full(ngrid, np.nan)\n    num_lowtarget_zero = 0\n    for k in range(ngrid):\n        x = pred[k, :]\n        y = target[k, :]\n        ind = np.where(np.logical_and(~np.isnan(x), ~np.isnan(y)))[0]\n        if ind.shape[0] &gt; 0:\n            xx = x[ind]\n            yy = y[ind]\n            # percent bias\n            PBias[k] = np.sum(xx - yy) / np.sum(yy) * 100\n            if ind.shape[0] &gt; 1:\n                # Theoretically at least two points for correlation\n                Corr[k] = scipy.stats.pearsonr(xx, yy)[0]\n                yymean = yy.mean()\n                SST: float = np.sum((yy - yymean) ** 2)\n                SSReg: float = np.sum((xx - yymean) ** 2)\n                SSRes: float = np.sum((yy - xx) ** 2)\n                R2[k] = 1 - SSRes / SST\n                NSE[k] = 1 - SSRes / SST\n                KGe[k] = KGE(xx, yy)\n            # FHV the peak flows bias 2%\n            # FLV the low flows bias bottom 30%, log space\n            pred_sort = np.sort(xx)\n            target_sort = np.sort(yy)\n            indexlow = round(0.3 * len(pred_sort))\n            indexhigh = round(0.98 * len(pred_sort))\n            lowpred = pred_sort[:indexlow]\n            highpred = pred_sort[indexhigh:]\n            lowtarget = target_sort[:indexlow]\n            hightarget = target_sort[indexhigh:]\n            if np.sum(lowtarget) == 0:\n                num_lowtarget_zero = num_lowtarget_zero + 1\n            with warnings.catch_warnings():\n                # Sometimes the lowtarget is all 0, which will cause a warning\n                # but I know it is not an error, so I ignore it\n                warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n                PBiaslow[k] = np.sum(lowpred - lowtarget) / np.sum(lowtarget) * 100\n            PBiashigh[k] = np.sum(highpred - hightarget) / np.sum(hightarget) * 100\n    outDict = dict(\n        Bias=Bias,\n        RMSE=RMSE,\n        ubRMSE=ubRMSE,\n        Corr=Corr,\n        R2=R2,\n        NSE=NSE,\n        KGE=KGe,\n        FHV=PBiashigh,\n        FLV=PBiaslow,\n    )\n    # \"The CDF of BFLV will not reach 1.0 because some basins have all zero flow observations for the \"\n    # \"30% low flow interval, the percent bias can be infinite\\n\"\n    # \"The number of these cases is \" + str(num_lowtarget_zero)\n    return outDict\n</code></pre>"},{"location":"api/hydro_stat/#hydroutils.hydro_stat.stat_error_i","title":"<code>stat_error_i(targ_i, pred_i)</code>","text":"<p>Calculate multiple statistical metrics for one-dimensional arrays.</p> <p>This function computes a comprehensive set of statistical metrics comparing predicted values against target (observed) values. It handles NaN values and requires at least two valid data points for correlation-based metrics.</p> <p>Parameters:</p> Name Type Description Default <code>targ_i</code> <code>ndarray</code> <p>Target (observed) values.</p> required <code>pred_i</code> <code>ndarray</code> <p>Predicted values.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: Dictionary containing the following metrics: - Bias: Mean error - RMSE: Root mean square error - ubRMSE: Unbiased root mean square error - Corr: Pearson correlation coefficient - R2: Coefficient of determination - NSE: Nash-Sutcliffe efficiency - KGE: Kling-Gupta efficiency - FHV: Peak flow bias (top 2%) - FLV: Low flow bias (bottom 30%)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If there are fewer than 2 valid data points for correlation.</p> Note <ul> <li>NaN values are automatically handled (removed from calculations)</li> <li>FHV and FLV are calculated in percentage</li> <li>All metrics are calculated on valid (non-NaN) data points only</li> </ul> Example <p>target = np.array([1.0, 2.0, 3.0, np.nan, 5.0]) predicted = np.array([1.1, 2.2, 2.9, np.nan, 4.8]) metrics = stat_error_i(target, predicted) print(metrics['RMSE'])  # Example output 0.173</p> Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def stat_error_i(targ_i: np.ndarray, pred_i: np.ndarray) -&gt; Dict[str, float]:\n    \"\"\"Calculate multiple statistical metrics for one-dimensional arrays.\n\n    This function computes a comprehensive set of statistical metrics comparing\n    predicted values against target (observed) values. It handles NaN values\n    and requires at least two valid data points for correlation-based metrics.\n\n    Args:\n        targ_i (np.ndarray): Target (observed) values.\n        pred_i (np.ndarray): Predicted values.\n\n    Returns:\n        Dict[str, float]: Dictionary containing the following metrics:\n            - Bias: Mean error\n            - RMSE: Root mean square error\n            - ubRMSE: Unbiased root mean square error\n            - Corr: Pearson correlation coefficient\n            - R2: Coefficient of determination\n            - NSE: Nash-Sutcliffe efficiency\n            - KGE: Kling-Gupta efficiency\n            - FHV: Peak flow bias (top 2%)\n            - FLV: Low flow bias (bottom 30%)\n\n    Raises:\n        ValueError: If there are fewer than 2 valid data points for correlation.\n\n    Note:\n        - NaN values are automatically handled (removed from calculations)\n        - FHV and FLV are calculated in percentage\n        - All metrics are calculated on valid (non-NaN) data points only\n\n    Example:\n        &gt;&gt;&gt; target = np.array([1.0, 2.0, 3.0, np.nan, 5.0])\n        &gt;&gt;&gt; predicted = np.array([1.1, 2.2, 2.9, np.nan, 4.8])\n        &gt;&gt;&gt; metrics = stat_error_i(target, predicted)\n        &gt;&gt;&gt; print(metrics['RMSE'])  # Example output\n        0.173\n    \"\"\"\n    ind = np.where(np.logical_and(~np.isnan(pred_i), ~np.isnan(targ_i)))[0]\n    # Theoretically at least two points for correlation\n    if ind.shape[0] &gt; 1:\n        xx = pred_i[ind]\n        yy = targ_i[ind]\n        bias = he.me(xx, yy)\n        # RMSE\n        rmse = he.rmse(xx, yy)\n        # ubRMSE\n        pred_mean = np.nanmean(xx)\n        target_mean = np.nanmean(yy)\n        pred_anom = xx - pred_mean\n        target_anom = yy - target_mean\n        ubrmse = np.sqrt(np.nanmean((pred_anom - target_anom) ** 2))\n        # rho R2 NSE\n        corr = he.pearson_r(xx, yy)\n        r2 = he.r_squared(xx, yy)\n        nse = he.nse(xx, yy)\n        kge = he.kge_2009(xx, yy)\n        # percent bias\n        pbias = np.sum(xx - yy) / np.sum(yy) * 100\n        # FHV the peak flows bias 2%\n        # FLV the low flows bias bottom 30%, log space\n        pred_sort = np.sort(xx)\n        target_sort = np.sort(yy)\n        indexlow = round(0.3 * len(pred_sort))\n        indexhigh = round(0.98 * len(pred_sort))\n        lowpred = pred_sort[:indexlow]\n        highpred = pred_sort[indexhigh:]\n        lowtarget = target_sort[:indexlow]\n        hightarget = target_sort[indexhigh:]\n        pbiaslow = np.sum(lowpred - lowtarget) / np.sum(lowtarget) * 100\n        pbiashigh = np.sum(highpred - hightarget) / np.sum(hightarget) * 100\n        return dict(\n            Bias=bias,\n            RMSE=rmse,\n            ubRMSE=ubrmse,\n            Corr=corr,\n            R2=r2,\n            NSE=nse,\n            KGE=kge,\n            FHV=pbiashigh,\n            FLV=pbiaslow,\n        )\n    else:\n        raise ValueError(\n            \"The number of data is less than 2, we don't calculate the statistics.\"\n        )\n</code></pre>"},{"location":"api/hydro_stat/#hydroutils.hydro_stat.stat_errors","title":"<code>stat_errors(target, pred, fill_nan=None)</code>","text":"<p>Calculate statistical metrics for 3D arrays with multiple variables.</p> <p>This function extends stat_error to handle 3D arrays where the third dimension represents different variables. Each variable can have its own NaN handling method.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>ndarray</code> <p>Target (observed) values. 3D array [basin, sequence, variable].</p> required <code>pred</code> <code>ndarray</code> <p>Predicted values. Same shape as target.</p> required <code>fill_nan</code> <code>List[str]</code> <p>List of NaN handling methods, one per variable. Each element can be \"no\", \"sum\", or \"mean\". Defaults to [\"no\"].</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, ndarray]]</code> <p>List[Dict[str, np.ndarray]]: List of dictionaries, one per variable. Each dictionary contains: - Bias: Mean error - RMSE: Root mean square error - ubRMSE: Unbiased root mean square error - Corr: Pearson correlation coefficient - R2: Coefficient of determination - NSE: Nash-Sutcliffe efficiency - KGE: Kling-Gupta efficiency - FHV: Peak flow bias (top 2%) - FLV: Low flow bias (bottom 30%)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If: - Input arrays are not 3D - Arrays have incompatible shapes - fill_nan length doesn't match number of variables</p> Example <p>target = np.array([[[1.0, 2.0], [np.nan, 4.0], [5.0, 6.0]]])  # 1x3x2 pred = np.array([[[1.1, 2.1], [3.0, 3.9], [4.9, 5.8]]]) metrics = stat_errors(target, pred, fill_nan=[\"no\", \"sum\"]) print(len(metrics))  # Number of variables 2 print(metrics[0]['RMSE'])  # RMSE for first variable array([0.141])</p> Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def stat_errors(\n    target: np.ndarray, pred: np.ndarray, fill_nan: Optional[List[str]] = None\n) -&gt; List[Dict[str, np.ndarray]]:\n    \"\"\"Calculate statistical metrics for 3D arrays with multiple variables.\n\n    This function extends stat_error to handle 3D arrays where the third dimension\n    represents different variables. Each variable can have its own NaN handling\n    method.\n\n    Args:\n        target (np.ndarray): Target (observed) values. 3D array [basin, sequence, variable].\n        pred (np.ndarray): Predicted values. Same shape as target.\n        fill_nan (List[str], optional): List of NaN handling methods, one per variable.\n            Each element can be \"no\", \"sum\", or \"mean\". Defaults to [\"no\"].\n\n    Returns:\n        List[Dict[str, np.ndarray]]: List of dictionaries, one per variable.\n            Each dictionary contains:\n            - Bias: Mean error\n            - RMSE: Root mean square error\n            - ubRMSE: Unbiased root mean square error\n            - Corr: Pearson correlation coefficient\n            - R2: Coefficient of determination\n            - NSE: Nash-Sutcliffe efficiency\n            - KGE: Kling-Gupta efficiency\n            - FHV: Peak flow bias (top 2%)\n            - FLV: Low flow bias (bottom 30%)\n\n    Raises:\n        ValueError: If:\n            - Input arrays are not 3D\n            - Arrays have incompatible shapes\n            - fill_nan length doesn't match number of variables\n\n    Example:\n        &gt;&gt;&gt; target = np.array([[[1.0, 2.0], [np.nan, 4.0], [5.0, 6.0]]])  # 1x3x2\n        &gt;&gt;&gt; pred = np.array([[[1.1, 2.1], [3.0, 3.9], [4.9, 5.8]]])\n        &gt;&gt;&gt; metrics = stat_errors(target, pred, fill_nan=[\"no\", \"sum\"])\n        &gt;&gt;&gt; print(len(metrics))  # Number of variables\n        2\n        &gt;&gt;&gt; print(metrics[0]['RMSE'])  # RMSE for first variable\n        array([0.141])\n    \"\"\"\n    if fill_nan is None:\n        fill_nan = [\"no\"]\n    if len(target.shape) != 3:\n        raise ValueError(\n            \"The input data should be 3-dim, not 2-dim. If you want to calculate \"\n            \"metrics for 2-d arrays, please use stat_error function.\"\n        )\n    if target.shape != pred.shape:\n        raise ValueError(\"The shape of target and pred should be the same.\")\n    if type(fill_nan) is not list or len(fill_nan) != target.shape[-1]:\n        raise ValueError(\n            \"Please give same length of fill_nan as the number of variables.\"\n        )\n    dict_list = []\n    for k in range(target.shape[-1]):\n        k_dict = stat_error(target[:, :, k], pred[:, :, k], fill_nan=fill_nan[k])\n        dict_list.append(k_dict)\n    return dict_list\n</code></pre>"},{"location":"api/hydro_stat/#hydroutils.hydro_stat.trans_norm","title":"<code>trans_norm(x, var_lst, stat_dict, *, to_norm)</code>","text":"<p>Normalize or denormalize data using statistical parameters.</p> <p>This function performs normalization or denormalization on 2D or 3D data arrays using pre-computed statistical parameters. It supports multiple variables and can handle both site-based and time series data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input data array: - 2D: [sites, variables] - 3D: [sites, time, variables]</p> required <code>var_lst</code> <code>Union[str, List[str]]</code> <p>Variable name(s) to process.</p> required <code>stat_dict</code> <code>Dict[str, List[float]]</code> <p>Dictionary containing statistics for each variable. Each value is [p10, p90, mean, std].</p> required <code>to_norm</code> <code>bool</code> <p>If True, normalize data; if False, denormalize data.</p> required <p>Returns:</p> Type Description <p>np.ndarray: Normalized or denormalized data with same shape as input.</p> Note <ul> <li>Normalization: (x - mean) / std</li> <li>Denormalization: x * std + mean</li> <li>Statistics should be pre-computed for each variable</li> <li>Handles single variable (str) or multiple variables (list)</li> <li>Preserves input array dimensions</li> </ul> Example Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def trans_norm(x, var_lst, stat_dict, *, to_norm):\n    \"\"\"Normalize or denormalize data using statistical parameters.\n\n    This function performs normalization or denormalization on 2D or 3D data\n    arrays using pre-computed statistical parameters. It supports multiple\n    variables and can handle both site-based and time series data.\n\n    Args:\n        x (np.ndarray): Input data array:\n            - 2D: [sites, variables]\n            - 3D: [sites, time, variables]\n        var_lst (Union[str, List[str]]): Variable name(s) to process.\n        stat_dict (Dict[str, List[float]]): Dictionary containing statistics\n            for each variable. Each value is [p10, p90, mean, std].\n        to_norm (bool): If True, normalize data; if False, denormalize data.\n\n    Returns:\n        np.ndarray: Normalized or denormalized data with same shape as input.\n\n    Note:\n        - Normalization: (x - mean) / std\n        - Denormalization: x * std + mean\n        - Statistics should be pre-computed for each variable\n        - Handles single variable (str) or multiple variables (list)\n        - Preserves input array dimensions\n\n    Example:\n        &gt;&gt;&gt; # Normalization example\n        &gt;&gt;&gt; data = np.array([[1.0, 2.0], [3.0, 4.0]])  # 2 sites, 2 variables\n        &gt;&gt;&gt; stats = {'var1': [0, 2, 1, 0.5], 'var2': [1, 5, 3, 1.0]}\n        &gt;&gt;&gt; vars = ['var1', 'var2']\n        &gt;&gt;&gt; normalized = trans_norm(data, vars, stats, to_norm=True)\n        &gt;&gt;&gt; print(normalized)  # Example output\n        array([[0. , -1.],\n               [4. ,  1.]])\n    \"\"\"\n    if type(var_lst) is str:\n        var_lst = [var_lst]\n    out = np.zeros(x.shape)\n    for k in range(len(var_lst)):\n        var = var_lst[k]\n        stat = stat_dict[var]\n        if to_norm is True:\n            if len(x.shape) == 3:\n                out[:, :, k] = (x[:, :, k] - stat[2]) / stat[3]\n            elif len(x.shape) == 2:\n                out[:, k] = (x[:, k] - stat[2]) / stat[3]\n        elif len(x.shape) == 3:\n            out[:, :, k] = x[:, :, k] * stat[3] + stat[2]\n        elif len(x.shape) == 2:\n            out[:, k] = x[:, k] * stat[3] + stat[2]\n    return out\n</code></pre>"},{"location":"api/hydro_stat/#hydroutils.hydro_stat.trans_norm--normalization-example","title":"Normalization example","text":"<p>data = np.array([[1.0, 2.0], [3.0, 4.0]])  # 2 sites, 2 variables stats = {'var1': [0, 2, 1, 0.5], 'var2': [1, 5, 3, 1.0]} vars = ['var1', 'var2'] normalized = trans_norm(data, vars, stats, to_norm=True) print(normalized)  # Example output array([[0. , -1.],        [4. ,  1.]])</p>"},{"location":"api/hydro_stat/#hydroutils.hydro_stat.wilcoxon_t_test","title":"<code>wilcoxon_t_test(xs, xo)</code>","text":"<p>Perform Wilcoxon signed-rank test on paired samples.</p> <p>This function performs a Wilcoxon signed-rank test to determine whether two related samples have the same distribution. It's particularly useful for comparing model predictions against observations.</p> <p>Parameters:</p> Name Type Description Default <code>xs</code> <code>ndarray</code> <p>First sample (typically simulated/predicted values).</p> required <code>xo</code> <code>ndarray</code> <p>Second sample (typically observed values).</p> required <p>Returns:</p> Type Description <code>Tuple[float, float]</code> <p>Tuple[float, float]: Test statistics: - w: Wilcoxon test statistic - p: p-value for the test</p> Note <ul> <li>Non-parametric alternative to paired t-test</li> <li>Assumes samples are paired and same length</li> <li>Direction of difference (xs-xo vs xo-xs) doesn't affect results</li> <li>Uses scipy.stats.wilcoxon under the hood</li> </ul> Example <p>sim = np.array([102, 104, 98, 101, 96, 103, 95]) obs = np.array([100, 102, 95, 100, 93, 101, 94]) w, p = wilcoxon_t_test(sim, obs) print(f\"W-statistic: {w:.2f}, p-value: {p:.4f}\") W-statistic: 26.50, p-value: 0.0234</p> Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def wilcoxon_t_test(xs: np.ndarray, xo: np.ndarray) -&gt; Tuple[float, float]:\n    \"\"\"Perform Wilcoxon signed-rank test on paired samples.\n\n    This function performs a Wilcoxon signed-rank test to determine whether two\n    related samples have the same distribution. It's particularly useful for\n    comparing model predictions against observations.\n\n    Args:\n        xs (np.ndarray): First sample (typically simulated/predicted values).\n        xo (np.ndarray): Second sample (typically observed values).\n\n    Returns:\n        Tuple[float, float]: Test statistics:\n            - w: Wilcoxon test statistic\n            - p: p-value for the test\n\n    Note:\n        - Non-parametric alternative to paired t-test\n        - Assumes samples are paired and same length\n        - Direction of difference (xs-xo vs xo-xs) doesn't affect results\n        - Uses scipy.stats.wilcoxon under the hood\n\n    Example:\n        &gt;&gt;&gt; sim = np.array([102, 104, 98, 101, 96, 103, 95])\n        &gt;&gt;&gt; obs = np.array([100, 102, 95, 100, 93, 101, 94])\n        &gt;&gt;&gt; w, p = wilcoxon_t_test(sim, obs)\n        &gt;&gt;&gt; print(f\"W-statistic: {w:.2f}, p-value: {p:.4f}\")\n        W-statistic: 26.50, p-value: 0.0234\n    \"\"\"\n    diff = xs - xo  # same result when using xo-xs\n    w, p = wilcoxon(diff)\n    return w, p\n</code></pre>"},{"location":"api/hydro_stat/#hydroutils.hydro_stat.wilcoxon_t_test_for_lst","title":"<code>wilcoxon_t_test_for_lst(x_lst, rnd_num=2)</code>","text":"<p>Perform pairwise Wilcoxon tests on multiple arrays.</p> <p>This function performs Wilcoxon signed-rank tests on every possible pair of arrays in a list of arrays. Results are rounded to specified precision.</p> <p>Parameters:</p> Name Type Description Default <code>x_lst</code> <code>List[ndarray]</code> <p>List of arrays to compare pairwise.</p> required <code>rnd_num</code> <code>int</code> <p>Number of decimal places to round results to. Defaults to 2.</p> <code>2</code> <p>Returns:</p> Type Description <p>Tuple[List[float], List[float]]: Two lists: - w: List of Wilcoxon test statistics for each pair - p: List of p-values for each pair</p> Note <ul> <li>Generates all possible pairs using itertools.combinations</li> <li>Results are ordered by pair combinations</li> <li>Number of pairs = n*(n-1)/2 where n is number of arrays</li> <li>All test statistics and p-values are rounded</li> </ul> Example <p>arrays = [ ...     np.array([1, 2, 3, 4]), ...     np.array([2, 3, 4, 5]), ...     np.array([3, 4, 5, 6]) ... ] w, p = wilcoxon_t_test_for_lst(arrays) print(f\"W-statistics: {w}\") W-statistics: [0.00, 0.00, 0.00] print(f\"p-values: {p}\") p-values: [0.07, 0.07, 0.07]</p> Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def wilcoxon_t_test_for_lst(x_lst, rnd_num=2):\n    \"\"\"Perform pairwise Wilcoxon tests on multiple arrays.\n\n    This function performs Wilcoxon signed-rank tests on every possible pair\n    of arrays in a list of arrays. Results are rounded to specified precision.\n\n    Args:\n        x_lst (List[np.ndarray]): List of arrays to compare pairwise.\n        rnd_num (int, optional): Number of decimal places to round results to.\n            Defaults to 2.\n\n    Returns:\n        Tuple[List[float], List[float]]: Two lists:\n            - w: List of Wilcoxon test statistics for each pair\n            - p: List of p-values for each pair\n\n    Note:\n        - Generates all possible pairs using itertools.combinations\n        - Results are ordered by pair combinations\n        - Number of pairs = n*(n-1)/2 where n is number of arrays\n        - All test statistics and p-values are rounded\n\n    Example:\n        &gt;&gt;&gt; arrays = [\n        ...     np.array([1, 2, 3, 4]),\n        ...     np.array([2, 3, 4, 5]),\n        ...     np.array([3, 4, 5, 6])\n        ... ]\n        &gt;&gt;&gt; w, p = wilcoxon_t_test_for_lst(arrays)\n        &gt;&gt;&gt; print(f\"W-statistics: {w}\")\n        W-statistics: [0.00, 0.00, 0.00]\n        &gt;&gt;&gt; print(f\"p-values: {p}\")\n        p-values: [0.07, 0.07, 0.07]\n    \"\"\"\n    arr_lst = np.asarray(x_lst)\n    w, p = [], []\n    arr_lst_pair = list(itertools.combinations(arr_lst, 2))\n    for arr_pair in arr_lst_pair:\n        wi, pi = wilcoxon_t_test(arr_pair[0], arr_pair[1])\n        w.append(round(wi, rnd_num))\n        p.append(round(pi, rnd_num))\n    return w, p\n</code></pre>"},{"location":"api/hydro_time/","title":"hydro_time","text":"<p>The <code>hydro_time</code> module provides utilities for handling time-related operations in hydrological data processing.</p>"},{"location":"api/hydro_time/#core-functions","title":"Core Functions","text":""},{"location":"api/hydro_time/#t2str","title":"t2str","text":"<pre><code>def t2str(t_: Union[str, datetime.datetime]) -&gt; Union[datetime.datetime, str]\n</code></pre> <p>Converts between datetime string and datetime object.</p> <p>Example: <pre><code>import hydroutils as hu\nfrom datetime import datetime\n\n# String to datetime\ndt = hu.t2str('2023-01-01')  # Returns datetime(2023, 1, 1)\n\n# Datetime to string\ns = hu.t2str(datetime(2023, 1, 1))  # Returns '2023-01-01'\n</code></pre></p>"},{"location":"api/hydro_time/#date_to_julian","title":"date_to_julian","text":"<pre><code>def date_to_julian(a_time: Union[str, datetime.datetime]) -&gt; int\n</code></pre> <p>Converts a date to its Julian day (day of year).</p>"},{"location":"api/hydro_time/#t_range_days","title":"t_range_days","text":"<pre><code>def t_range_days(t_range: list, *, step: np.timedelta64 = np.timedelta64(1, \"D\")) -&gt; np.array\n</code></pre> <p>Creates a uniformly-spaced array of dates from a date range.</p> <p>Example: <pre><code>import hydroutils as hu\nimport numpy as np\n\ndates = hu.t_range_days(['2000-01-01', '2000-01-05'])\n# Returns array(['2000-01-01', '2000-01-02', '2000-01-03', '2000-01-04'])\n</code></pre></p>"},{"location":"api/hydro_time/#t_range_days_timedelta","title":"t_range_days_timedelta","text":"<pre><code>def t_range_days_timedelta(t_array: np.array, td: int = 12, td_type: str = \"h\") -&gt; np.array\n</code></pre> <p>Adds a time delta to each date in an array.</p>"},{"location":"api/hydro_time/#generate_start0101_time_range","title":"generate_start0101_time_range","text":"<pre><code>def generate_start0101_time_range(\n    start_time: Union[str, pd.Timestamp],\n    end_time: Union[str, pd.Timestamp],\n    freq: str = \"8D\"\n) -&gt; pd.DatetimeIndex\n</code></pre> <p>Generates a time range that resets to January 1st each year.</p>"},{"location":"api/hydro_time/#t_days_lst2range","title":"t_days_lst2range","text":"<pre><code>def t_days_lst2range(t_array: list) -&gt; list\n</code></pre> <p>Converts a list of dates to a date range [start, end].</p>"},{"location":"api/hydro_time/#assign_time_start_end","title":"assign_time_start_end","text":"<pre><code>def assign_time_start_end(time_ranges: list, assign_way: str = \"intersection\") -&gt; tuple\n</code></pre> <p>Determines overall start and end times from multiple time ranges.</p>"},{"location":"api/hydro_time/#calculate_utc_offset","title":"calculate_utc_offset","text":"<pre><code>def calculate_utc_offset(lat: float, lng: float, date: datetime.datetime = None) -&gt; int\n</code></pre> <p>Calculates UTC offset for a location using tzfpy.</p>"},{"location":"api/hydro_time/#get_year","title":"get_year","text":"<pre><code>def get_year(a_time: Union[datetime.date, np.datetime64, str]) -&gt; int\n</code></pre> <p>Extracts the year from various date formats.</p>"},{"location":"api/hydro_time/#intersect","title":"intersect","text":"<pre><code>def intersect(t_lst1: np.array, t_lst2: np.array) -&gt; tuple\n</code></pre> <p>Finds indices where two time arrays intersect.</p>"},{"location":"api/hydro_time/#api-reference","title":"API Reference","text":"<p>Author: Wenyu Ouyang Date: 2022-12-02 11:03:04 LastEditTime: 2024-09-14 13:57:36 LastEditors: Wenyu Ouyang Description: some functions to deal with time FilePath: \\hydroutils\\hydroutils\\hydro_time.py Copyright (c) 2023-2024 Wenyu Ouyang. All rights reserved.</p>"},{"location":"api/hydro_time/#hydroutils.hydro_time.assign_time_start_end","title":"<code>assign_time_start_end(time_ranges, assign_way='intersection')</code>","text":"<p>Determine start and end times from multiple time ranges.</p> <p>Parameters:</p> Name Type Description Default <code>time_ranges</code> <code>list</code> <p>List of time range pairs [[start1, end1], [start2, end2], ...]. Each start/end can be any comparable type (datetime, string, etc.).</p> required <code>assign_way</code> <code>str</code> <p>Method to determine the final range. Defaults to \"intersection\". - \"intersection\": Use latest start time and earliest end time. - \"union\": Use earliest start time and latest end time.</p> <code>'intersection'</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>(time_start, time_end) The determined start and end times.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If assign_way is not \"intersection\" or \"union\".</p> Example <p>ranges = [[\"2020-01-01\", \"2020-12-31\"], [\"2020-03-01\", \"2021-02-28\"]] assign_time_start_end(ranges, \"intersection\") (\"2020-03-01\", \"2020-12-31\") assign_time_start_end(ranges, \"union\") (\"2020-01-01\", \"2021-02-28\")</p> Source code in <code>hydroutils\\hydro_time.py</code> <pre><code>def assign_time_start_end(time_ranges, assign_way=\"intersection\"):\n    \"\"\"Determine start and end times from multiple time ranges.\n\n    Args:\n        time_ranges (list): List of time range pairs [[start1, end1], [start2, end2], ...].\n            Each start/end can be any comparable type (datetime, string, etc.).\n        assign_way (str, optional): Method to determine the final range. Defaults to \"intersection\".\n            - \"intersection\": Use latest start time and earliest end time.\n            - \"union\": Use earliest start time and latest end time.\n\n    Returns:\n        tuple: (time_start, time_end) The determined start and end times.\n\n    Raises:\n        NotImplementedError: If assign_way is not \"intersection\" or \"union\".\n\n    Example:\n        &gt;&gt;&gt; ranges = [[\"2020-01-01\", \"2020-12-31\"], [\"2020-03-01\", \"2021-02-28\"]]\n        &gt;&gt;&gt; assign_time_start_end(ranges, \"intersection\")\n        (\"2020-03-01\", \"2020-12-31\")\n        &gt;&gt;&gt; assign_time_start_end(ranges, \"union\")\n        (\"2020-01-01\", \"2021-02-28\")\n    \"\"\"\n    if assign_way == \"intersection\":\n        time_start = max(t[0] for t in time_ranges)\n        time_end = min(t[1] for t in time_ranges)\n    elif assign_way == \"union\":\n        time_start = min(t[0] for t in time_ranges)\n        time_end = max(t[1] for t in time_ranges)\n    else:\n        raise NotImplementedError(\"We don't support this assign_way yet\")\n    return time_start, time_end\n</code></pre>"},{"location":"api/hydro_time/#hydroutils.hydro_time.calculate_utc_offset","title":"<code>calculate_utc_offset(lat, lng, date=None)</code>","text":"<p>Calculate the UTC offset for a geographic location.</p> <p>This function determines the timezone and UTC offset for a given latitude and longitude coordinate pair using the tzfpy library, which provides accurate timezone data based on geographic location.</p> <p>Parameters:</p> Name Type Description Default <code>lat</code> <code>float</code> <p>Latitude in decimal degrees (-90 to 90).</p> required <code>lng</code> <code>float</code> <p>Longitude in decimal degrees (-180 to 180).</p> required <code>date</code> <code>datetime</code> <p>The date to calculate the offset for. Defaults to current UTC time. Important for handling daylight saving time.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>int</code> <p>UTC offset in hours, or None if timezone cannot be determined.</p> Example <p>calculate_utc_offset(35.6762, 139.6503)  # Tokyo, Japan 9 calculate_utc_offset(51.5074, -0.1278)   # London, UK 0  # or 1 during DST</p> Source code in <code>hydroutils\\hydro_time.py</code> <pre><code>def calculate_utc_offset(lat, lng, date=None):\n    \"\"\"Calculate the UTC offset for a geographic location.\n\n    This function determines the timezone and UTC offset for a given latitude and\n    longitude coordinate pair using the tzfpy library, which provides accurate\n    timezone data based on geographic location.\n\n    Args:\n        lat (float): Latitude in decimal degrees (-90 to 90).\n        lng (float): Longitude in decimal degrees (-180 to 180).\n        date (datetime.datetime, optional): The date to calculate the offset for.\n            Defaults to current UTC time. Important for handling daylight saving time.\n\n    Returns:\n        int: UTC offset in hours, or None if timezone cannot be determined.\n\n    Example:\n        &gt;&gt;&gt; calculate_utc_offset(35.6762, 139.6503)  # Tokyo, Japan\n        9\n        &gt;&gt;&gt; calculate_utc_offset(51.5074, -0.1278)   # London, UK\n        0  # or 1 during DST\n    \"\"\"\n    if date is None:\n        date = datetime.datetime.utcnow()\n\n    if timezone_str := tzfpy.get_tz(lng, lat):\n        # Get the timezone object using pytz\n        tz = pytz.timezone(timezone_str)\n        # Get the UTC offset for the specified date\n        offset = tz.utcoffset(date)\n        if offset is not None:\n            return int(offset.total_seconds() / 3600)\n    return None\n</code></pre>"},{"location":"api/hydro_time/#hydroutils.hydro_time.date_to_julian","title":"<code>date_to_julian(a_time)</code>","text":"<p>Convert a date to Julian day of the year.</p> <p>Parameters:</p> Name Type Description Default <code>a_time</code> <code>Union[str, datetime]</code> <p>Date to convert. If string, must be in format 'YYYY-MM-DD'.</p> required <p>Returns:</p> Name Type Description <code>int</code> <p>Day of the year (1-366).</p> Source code in <code>hydroutils\\hydro_time.py</code> <pre><code>def date_to_julian(a_time):\n    \"\"\"Convert a date to Julian day of the year.\n\n    Args:\n        a_time (Union[str, datetime.datetime]): Date to convert.\n            If string, must be in format 'YYYY-MM-DD'.\n\n    Returns:\n        int: Day of the year (1-366).\n    \"\"\"\n    if type(a_time) == str:\n        fmt = \"%Y-%m-%d\"\n        dt = datetime.datetime.strptime(a_time, fmt)\n    else:\n        dt = a_time\n    tt = dt.timetuple()\n    return tt.tm_yday\n</code></pre>"},{"location":"api/hydro_time/#hydroutils.hydro_time.generate_start0101_time_range","title":"<code>generate_start0101_time_range(start_time, end_time, freq='8D')</code>","text":"<p>Generate a time range with annual reset to January 1st.</p> <p>This function creates a time range with a specified frequency, but with the special behavior that each year starts from January 1st regardless of the frequency interval. This is particularly useful for creating time series that need to align with calendar years while maintaining a regular interval pattern within each year.</p> <p>Parameters:</p> Name Type Description Default <code>start_time</code> <code>Union[str, Timestamp]</code> <p>Start date of the range. Can be string ('YYYY-MM-DD') or pandas Timestamp.</p> required <code>end_time</code> <code>Union[str, Timestamp]</code> <p>End date of the range. Can be string ('YYYY-MM-DD') or pandas Timestamp.</p> required <code>freq</code> <code>str</code> <p>Time frequency for intervals. Defaults to '8D'. Common values: '7D' (weekly), '10D' (dekadal), etc.</p> <code>'8D'</code> <p>Returns:</p> Type Description <p>pd.DatetimeIndex: Time range with specified frequency and annual reset.</p> Example <p>generate_start0101_time_range('2020-03-15', '2021-02-15', freq='10D') DatetimeIndex(['2020-03-15', '2020-03-25', '2020-04-04', ...,               '2021-01-01', '2021-01-11', '2021-02-11'],               dtype='datetime64[ns]', freq=None)</p> Note <ul> <li>If an interval would cross into a new year, it's truncated and the next   interval starts from January 1st of the new year.</li> <li>The frequency must be a valid pandas frequency string that represents   a fixed duration.</li> </ul> Source code in <code>hydroutils\\hydro_time.py</code> <pre><code>def generate_start0101_time_range(start_time, end_time, freq=\"8D\"):\n    \"\"\"Generate a time range with annual reset to January 1st.\n\n    This function creates a time range with a specified frequency, but with the special\n    behavior that each year starts from January 1st regardless of the frequency interval.\n    This is particularly useful for creating time series that need to align with\n    calendar years while maintaining a regular interval pattern within each year.\n\n    Args:\n        start_time (Union[str, pd.Timestamp]): Start date of the range.\n            Can be string ('YYYY-MM-DD') or pandas Timestamp.\n        end_time (Union[str, pd.Timestamp]): End date of the range.\n            Can be string ('YYYY-MM-DD') or pandas Timestamp.\n        freq (str, optional): Time frequency for intervals. Defaults to '8D'.\n            Common values: '7D' (weekly), '10D' (dekadal), etc.\n\n    Returns:\n        pd.DatetimeIndex: Time range with specified frequency and annual reset.\n\n    Example:\n        &gt;&gt;&gt; generate_start0101_time_range('2020-03-15', '2021-02-15', freq='10D')\n        DatetimeIndex(['2020-03-15', '2020-03-25', '2020-04-04', ...,\n                      '2021-01-01', '2021-01-11', '2021-02-11'],\n                      dtype='datetime64[ns]', freq=None)\n\n    Note:\n        - If an interval would cross into a new year, it's truncated and the next\n          interval starts from January 1st of the new year.\n        - The frequency must be a valid pandas frequency string that represents\n          a fixed duration.\n    \"\"\"\n    all_dates = []\n\n    # Ensure the start and end times are of type pd.Timestamp\n    current_time = pd.Timestamp(start_time)\n    end_time = pd.Timestamp(end_time)\n\n    # Parse the frequency interval correctly\n    interval_days = pd.Timedelta(freq)  # Ensure it's a Timedelta\n\n    while current_time &lt;= end_time:\n        all_dates.append(current_time)\n\n        # Calculate next date with the specified interval\n        next_time = current_time + interval_days\n\n        # If next_time crosses into a new year, reset to 01-01 of the new year\n        if next_time.year &gt; current_time.year:\n            next_time = pd.Timestamp(f\"{next_time.year}-01-01\")\n\n        current_time = next_time\n\n    return pd.to_datetime(all_dates)\n</code></pre>"},{"location":"api/hydro_time/#hydroutils.hydro_time.get_year","title":"<code>get_year(a_time)</code>","text":"<p>Extract year from various time formats.</p> <p>Parameters:</p> Name Type Description Default <code>a_time</code> <code>Union[date, datetime64, str]</code> <p>Time in various formats.</p> required <p>Returns:</p> Name Type Description <code>int</code> <p>Year value.</p> Note <p>Supports datetime.date, numpy.datetime64, and string formats. For strings, assumes YYYY is at the start.</p> Source code in <code>hydroutils\\hydro_time.py</code> <pre><code>def get_year(a_time):\n    \"\"\"Extract year from various time formats.\n\n    Args:\n        a_time (Union[datetime.date, np.datetime64, str]): Time in various formats.\n\n    Returns:\n        int: Year value.\n\n    Note:\n        Supports datetime.date, numpy.datetime64, and string formats.\n        For strings, assumes YYYY is at the start.\n    \"\"\"\n    if isinstance(a_time, datetime.date):\n        return a_time.year\n    elif isinstance(a_time, np.datetime64):\n        return a_time.astype(\"datetime64[Y]\").astype(int) + 1970\n    else:\n        return int(a_time[:4])\n</code></pre>"},{"location":"api/hydro_time/#hydroutils.hydro_time.intersect","title":"<code>intersect(t_lst1, t_lst2)</code>","text":"<p>Find indices of common elements between two time lists.</p> <p>Parameters:</p> Name Type Description Default <code>t_lst1</code> <code>array - like</code> <p>First time array.</p> required <code>t_lst2</code> <code>array - like</code> <p>Second time array.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>(ind1, ind2) where ind1 and ind2 are indices of common elements</p> <p>in t_lst1 and t_lst2 respectively.</p> Source code in <code>hydroutils\\hydro_time.py</code> <pre><code>def intersect(t_lst1, t_lst2):\n    \"\"\"Find indices of common elements between two time lists.\n\n    Args:\n        t_lst1 (array-like): First time array.\n        t_lst2 (array-like): Second time array.\n\n    Returns:\n        tuple: (ind1, ind2) where ind1 and ind2 are indices of common elements\n        in t_lst1 and t_lst2 respectively.\n    \"\"\"\n    C, ind1, ind2 = np.intersect1d(t_lst1, t_lst2, return_indices=True)\n    return ind1, ind2\n</code></pre>"},{"location":"api/hydro_time/#hydroutils.hydro_time.t2str","title":"<code>t2str(t_)</code>","text":"<p>Convert between datetime string and datetime object.</p> <p>Parameters:</p> Name Type Description Default <code>t_</code> <code>Union[str, datetime]</code> <p>Input time, either as string or datetime object.</p> required <p>Returns:</p> Type Description <p>Union[str, datetime.datetime]: If input is string, returns datetime object.                          If input is datetime, returns string.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If input type is not supported.</p> Note <p>String format is always \"%Y-%m-%d\".</p> Source code in <code>hydroutils\\hydro_time.py</code> <pre><code>def t2str(t_: Union[str, datetime.datetime]):\n    \"\"\"Convert between datetime string and datetime object.\n\n    Args:\n        t_ (Union[str, datetime.datetime]): Input time, either as string or datetime object.\n\n    Returns:\n        Union[str, datetime.datetime]: If input is string, returns datetime object.\n                                     If input is datetime, returns string.\n\n    Raises:\n        NotImplementedError: If input type is not supported.\n\n    Note:\n        String format is always \"%Y-%m-%d\".\n    \"\"\"\n    if type(t_) is str:\n        return datetime.datetime.strptime(t_, \"%Y-%m-%d\")\n    elif type(t_) is datetime.datetime:\n        return t_.strftime(\"%Y-%m-%d\")\n    else:\n        raise NotImplementedError(\"We don't support this data type yet\")\n</code></pre>"},{"location":"api/hydro_time/#hydroutils.hydro_time.t_days_lst2range","title":"<code>t_days_lst2range(t_array)</code>","text":"<p>Transform a list of dates into a start-end interval.</p> <p>Parameters:</p> Name Type Description Default <code>t_array</code> <code>list[Union[datetime64, str]]</code> <p>List of dates in chronological order.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>Two-element list containing first and last dates as strings.</p> Example <p>t_days_lst2range([\"2000-01-01\", \"2000-01-02\", \"2000-01-03\", \"2000-01-04\"]) [\"2000-01-01\", \"2000-01-04\"]</p> Source code in <code>hydroutils\\hydro_time.py</code> <pre><code>def t_days_lst2range(t_array: list) -&gt; list:\n    \"\"\"Transform a list of dates into a start-end interval.\n\n    Args:\n        t_array (list[Union[np.datetime64, str]]): List of dates in chronological order.\n\n    Returns:\n        list: Two-element list containing first and last dates as strings.\n\n    Example:\n        &gt;&gt;&gt; t_days_lst2range([\"2000-01-01\", \"2000-01-02\", \"2000-01-03\", \"2000-01-04\"])\n        [\"2000-01-01\", \"2000-01-04\"]\n    \"\"\"\n    if type(t_array[0]) == np.datetime64:\n        t0 = t_array[0].astype(datetime.datetime)\n        t1 = t_array[-1].astype(datetime.datetime)\n    else:\n        t0 = t_array[0]\n        t1 = t_array[-1]\n    sd = t0.strftime(\"%Y-%m-%d\")\n    ed = t1.strftime(\"%Y-%m-%d\")\n    return [sd, ed]\n</code></pre>"},{"location":"api/hydro_time/#hydroutils.hydro_time.t_range_days","title":"<code>t_range_days(t_range, *, step=np.timedelta64(1, 'D'))</code>","text":"<p>Transform a date range into a uniformly-spaced array of dates.</p> <p>Parameters:</p> Name Type Description Default <code>t_range</code> <code>list</code> <p>Two-element list containing start and end dates as strings.</p> required <code>step</code> <code>timedelta64</code> <p>Time interval between dates. Defaults to 1 day.</p> <code>timedelta64(1, 'D')</code> <p>Returns:</p> Type Description <code>array</code> <p>np.array: Array of datetime64 objects with uniform spacing.</p> Example <p>t_range_days([\"2000-01-01\", \"2000-01-05\"]) array(['2000-01-01', '2000-01-02', '2000-01-03', '2000-01-04'],       dtype='datetime64[D]')</p> Source code in <code>hydroutils\\hydro_time.py</code> <pre><code>def t_range_days(t_range, *, step=np.timedelta64(1, \"D\")) -&gt; np.array:\n    \"\"\"Transform a date range into a uniformly-spaced array of dates.\n\n    Args:\n        t_range (list): Two-element list containing start and end dates as strings.\n        step (np.timedelta64, optional): Time interval between dates. Defaults to 1 day.\n\n    Returns:\n        np.array: Array of datetime64 objects with uniform spacing.\n\n    Example:\n        &gt;&gt;&gt; t_range_days([\"2000-01-01\", \"2000-01-05\"])\n        array(['2000-01-01', '2000-01-02', '2000-01-03', '2000-01-04'],\n              dtype='datetime64[D]')\n    \"\"\"\n    sd = datetime.datetime.strptime(t_range[0], \"%Y-%m-%d\")\n    ed = datetime.datetime.strptime(t_range[1], \"%Y-%m-%d\")\n    return np.arange(sd, ed, step)\n</code></pre>"},{"location":"api/hydro_time/#hydroutils.hydro_time.t_range_days_timedelta","title":"<code>t_range_days_timedelta(t_array, td=12, td_type='h')</code>","text":"<p>Add a time delta to each date in an array.</p> <p>Parameters:</p> Name Type Description Default <code>t_array</code> <code>array</code> <p>Array of datetime64 objects (output of t_range_days).</p> required <code>td</code> <code>int</code> <p>Time period value. Defaults to 12.</p> <code>12</code> <code>td_type</code> <code>str</code> <p>Time period unit ('Y','M','D','h','m','s'). Defaults to \"h\".</p> <code>'h'</code> <p>Returns:</p> Type Description <p>np.array: New array with time delta added to each element.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If td_type is not one of 'Y','M','D','h','m','s'.</p> Source code in <code>hydroutils\\hydro_time.py</code> <pre><code>def t_range_days_timedelta(t_array, td=12, td_type=\"h\"):\n    \"\"\"Add a time delta to each date in an array.\n\n    Args:\n        t_array (np.array): Array of datetime64 objects (output of t_range_days).\n        td (int, optional): Time period value. Defaults to 12.\n        td_type (str, optional): Time period unit ('Y','M','D','h','m','s'). Defaults to \"h\".\n\n    Returns:\n        np.array: New array with time delta added to each element.\n\n    Raises:\n        AssertionError: If td_type is not one of 'Y','M','D','h','m','s'.\n    \"\"\"\n    assert td_type in [\"Y\", \"M\", \"D\", \"h\", \"m\", \"s\"]\n    t_array_final = [t + np.timedelta64(td, td_type) for t in t_array]\n    return np.array(t_array_final)\n</code></pre>"},{"location":"api/hydro_time/#hydroutils.hydro_time.t_range_to_julian","title":"<code>t_range_to_julian(t_range)</code>","text":"<p>Convert a date range to a list of Julian days.</p> <p>Parameters:</p> Name Type Description Default <code>t_range</code> <code>list</code> <p>Two-element list of dates as strings [\"YYYY-MM-DD\", \"YYYY-MM-DD\"].</p> required <p>Returns:</p> Type Description <p>list[int]: List of Julian days for each date in the range.</p> Source code in <code>hydroutils\\hydro_time.py</code> <pre><code>def t_range_to_julian(t_range):\n    \"\"\"Convert a date range to a list of Julian days.\n\n    Args:\n        t_range (list): Two-element list of dates as strings [\"YYYY-MM-DD\", \"YYYY-MM-DD\"].\n\n    Returns:\n        list[int]: List of Julian days for each date in the range.\n    \"\"\"\n    t_array = t_range_days(t_range)\n    t_array_str = np.datetime_as_string(t_array)\n    return [date_to_julian(a_time[:10]) for a_time in t_array_str]\n</code></pre>"},{"location":"api/hydro_time/#hydroutils.hydro_time.t_range_years","title":"<code>t_range_years(t_range)</code>","text":"<p>Get array of years covered by a date range.</p> <p>Parameters:</p> Name Type Description Default <code>t_range</code> <code>list</code> <p>Two-element list of dates as strings [\"YYYY-MM-DD\", \"YYYY-MM-DD\"].</p> required <p>Returns:</p> Type Description <p>np.array: Array of years covered by the date range.</p> Note <ul> <li>Range is left-closed and right-open interval.</li> <li>If end date is not January 1st, end year is included.</li> <li>Example: [\"2000-01-01\", \"2002-01-01\"] -&gt; [2000, 2001]</li> <li>Example: [\"2000-01-01\", \"2002-06-01\"] -&gt; [2000, 2001, 2002]</li> </ul> Source code in <code>hydroutils\\hydro_time.py</code> <pre><code>def t_range_years(t_range):\n    \"\"\"Get array of years covered by a date range.\n\n    Args:\n        t_range (list): Two-element list of dates as strings [\"YYYY-MM-DD\", \"YYYY-MM-DD\"].\n\n    Returns:\n        np.array: Array of years covered by the date range.\n\n    Note:\n        - Range is left-closed and right-open interval.\n        - If end date is not January 1st, end year is included.\n        - Example: [\"2000-01-01\", \"2002-01-01\"] -&gt; [2000, 2001]\n        - Example: [\"2000-01-01\", \"2002-06-01\"] -&gt; [2000, 2001, 2002]\n    \"\"\"\n    start_year = int(t_range[0].split(\"-\")[0])\n    end_year = int(t_range[1].split(\"-\")[0])\n    end_month = int(t_range[1].split(\"-\")[1])\n    end_day = int(t_range[1].split(\"-\")[2])\n    return (\n        np.arange(start_year, end_year)\n        if end_month == 1 and end_day == 1\n        else np.arange(start_year, end_year + 1)\n    )\n</code></pre>"},{"location":"api/hydro_units/","title":"hydro_units","text":"<p>The <code>hydro_units</code> module provides comprehensive unit conversion functionality for hydrological data.</p>"},{"location":"api/hydro_units/#core-functions","title":"Core Functions","text":""},{"location":"api/hydro_units/#streamflow_unit_conv","title":"streamflow_unit_conv","text":"<pre><code>def streamflow_unit_conv(\n    streamflow_data: Union[xr.Dataset, pint.Quantity, np.ndarray, pd.DataFrame, pd.Series],\n    source_unit: str = None,\n    target_unit: str = \"m\u00b3/s\",\n    basin_area: float = None,\n    time_interval: str = None\n) -&gt; Union[xr.Dataset, pint.Quantity, np.ndarray, pd.DataFrame, pd.Series]\n</code></pre> <p>Converts streamflow data between different units (depth-based to volume-based and vice versa).</p> <p>Example: <pre><code>import hydroutils as hu\nimport numpy as np\n\n# Convert from mm/h to m\u00b3/s\nflow_mmh = np.array([10, 20, 15])  # mm/h\nflow_m3s = hu.streamflow_unit_conv(\n    flow_mmh, \n    source_unit=\"mm/h\", \n    target_unit=\"m\u00b3/s\",\n    basin_area=1000  # km\u00b2\n)\n</code></pre></p>"},{"location":"api/hydro_units/#detect_time_interval","title":"detect_time_interval","text":"<pre><code>def detect_time_interval(time_series: Union[pd.Series, np.ndarray, list]) -&gt; str\n</code></pre> <p>Automatically detects the time interval from a time series.</p>"},{"location":"api/hydro_units/#get_time_interval_info","title":"get_time_interval_info","text":"<pre><code>def get_time_interval_info(time_interval: str) -&gt; dict\n</code></pre> <p>Returns detailed information about a time interval.</p>"},{"location":"api/hydro_units/#validate_unit_compatibility","title":"validate_unit_compatibility","text":"<pre><code>def validate_unit_compatibility(unit1: str, unit2: str) -&gt; bool\n</code></pre> <p>Validates if two units are compatible for conversion.</p>"},{"location":"api/hydro_units/#api-reference","title":"API Reference","text":"<p>Hydrological unit conversion utilities.</p> <p>This module provides comprehensive unit conversion functionality for hydrological data, including streamflow unit conversions between depth units (mm/time) and volume units (m\u00b3/s).</p>"},{"location":"api/hydro_units/#hydroutils.hydro_units.detect_time_interval","title":"<code>detect_time_interval(time_series)</code>","text":"<p>Detect the time interval between points in a time series.</p> <p>This function analyzes a time series to determine the most common time interval between consecutive points. It handles various input formats and converts the interval to a standardized string format.</p> <p>Parameters:</p> Name Type Description Default <code>time_series</code> <code>Union[DatetimeIndex, list, ndarray]</code> <p>Time series data containing datetime information. Can be: - pandas DatetimeIndex - List of datetime-like objects - NumPy array of datetime-like objects</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Detected time interval in format suitable for unit conversion: - For hourly data: \"Nh\" where N is number of hours (e.g., \"3h\") - For daily data: \"Nd\" where N is number of days (e.g., \"1d\")</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If time series has fewer than 2 points.</p> Note <ul> <li>Uses most common time difference (mode) for irregular intervals</li> <li>Rounds non-integer hours to nearest hour</li> <li>Prefers hours for intervals &lt; 24h, days for intervals \u2265 24h</li> <li>Automatically converts various datetime formats to pandas DatetimeIndex</li> </ul> Example <p>import pandas as pd</p> Source code in <code>hydroutils\\hydro_units.py</code> <pre><code>def detect_time_interval(\n    time_series: Union[pd.DatetimeIndex, list, np.ndarray],\n) -&gt; str:\n    \"\"\"Detect the time interval between points in a time series.\n\n    This function analyzes a time series to determine the most common time\n    interval between consecutive points. It handles various input formats and\n    converts the interval to a standardized string format.\n\n    Args:\n        time_series (Union[pd.DatetimeIndex, list, np.ndarray]): Time series\n            data containing datetime information. Can be:\n            - pandas DatetimeIndex\n            - List of datetime-like objects\n            - NumPy array of datetime-like objects\n\n    Returns:\n        str: Detected time interval in format suitable for unit conversion:\n            - For hourly data: \"Nh\" where N is number of hours (e.g., \"3h\")\n            - For daily data: \"Nd\" where N is number of days (e.g., \"1d\")\n\n    Raises:\n        ValueError: If time series has fewer than 2 points.\n\n    Note:\n        - Uses most common time difference (mode) for irregular intervals\n        - Rounds non-integer hours to nearest hour\n        - Prefers hours for intervals &lt; 24h, days for intervals \u2265 24h\n        - Automatically converts various datetime formats to pandas DatetimeIndex\n\n    Example:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; # Regular 3-hourly data\n        &gt;&gt;&gt; time_index = pd.date_range(\"2024-01-01\", periods=8, freq=\"3h\")\n        &gt;&gt;&gt; detect_time_interval(time_index)\n        '3h'\n\n        &gt;&gt;&gt; # Daily data\n        &gt;&gt;&gt; dates = [\"2024-01-01\", \"2024-01-02\", \"2024-01-03\"]\n        &gt;&gt;&gt; detect_time_interval(dates)\n        '1d'\n\n        &gt;&gt;&gt; # Mixed intervals (most common is 6h)\n        &gt;&gt;&gt; times = pd.to_datetime([\n        ...     \"2024-01-01 00:00\",\n        ...     \"2024-01-01 06:00\",\n        ...     \"2024-01-01 12:00\",\n        ...     \"2024-01-01 18:00\"\n        ... ])\n        &gt;&gt;&gt; detect_time_interval(times)\n        '6h'\n    \"\"\"\n    if isinstance(time_series, list):\n        time_series = pd.to_datetime(time_series)\n    elif isinstance(time_series, np.ndarray):\n        time_series = pd.to_datetime(time_series)\n    elif not isinstance(time_series, pd.DatetimeIndex):\n        time_series = pd.DatetimeIndex(time_series)\n\n    if len(time_series) &lt; 2:\n        raise ValueError(\n            \"Time series must have at least 2 time points to detect interval\"\n        )\n\n    # Calculate time differences\n    time_diffs = time_series[1:] - time_series[:-1]\n\n    # Get the most common time difference (mode)\n    most_common_diff = time_diffs.value_counts().index[0]\n\n    # Convert to hours and days\n    total_seconds = most_common_diff.total_seconds()\n    hours = total_seconds / 3600\n    days = total_seconds / (3600 * 24)\n\n    # Determine the appropriate unit\n    if hours == int(hours) and hours &lt; 24:\n        return f\"{int(hours)}h\"\n    elif days == int(days):\n        return f\"{int(days)}d\"\n    else:\n        # For non-integer hours, round to nearest hour\n        rounded_hours = round(hours)\n        return f\"{rounded_hours}h\"\n</code></pre>"},{"location":"api/hydro_units/#hydroutils.hydro_units.detect_time_interval--regular-3-hourly-data","title":"Regular 3-hourly data","text":"<p>time_index = pd.date_range(\"2024-01-01\", periods=8, freq=\"3h\") detect_time_interval(time_index) '3h'</p>"},{"location":"api/hydro_units/#hydroutils.hydro_units.detect_time_interval--daily-data","title":"Daily data","text":"<p>dates = [\"2024-01-01\", \"2024-01-02\", \"2024-01-03\"] detect_time_interval(dates) '1d'</p>"},{"location":"api/hydro_units/#hydroutils.hydro_units.detect_time_interval--mixed-intervals-most-common-is-6h","title":"Mixed intervals (most common is 6h)","text":"<p>times = pd.to_datetime([ ...     \"2024-01-01 00:00\", ...     \"2024-01-01 06:00\", ...     \"2024-01-01 12:00\", ...     \"2024-01-01 18:00\" ... ]) detect_time_interval(times) '6h'</p>"},{"location":"api/hydro_units/#hydroutils.hydro_units.get_time_interval_info","title":"<code>get_time_interval_info(time_interval)</code>","text":"<p>Parse a time interval string into its numeric value and unit.</p> <p>This function extracts the numeric value and unit from a time interval string using regular expressions. It supports hourly and daily intervals in a standardized format.</p> <p>Parameters:</p> Name Type Description Default <code>time_interval</code> <code>str</code> <p>Time interval string in format \"Nh\" or \"Nd\" where N is a positive integer. Examples: \"1h\", \"3h\", \"1d\", \"5d\".</p> required <p>Returns:</p> Type Description <code>Tuple[int, str]</code> <p>Tuple[int, str]: Two-element tuple containing: - number (int): The numeric value from the interval - unit (str): The unit, either 'h' for hours or 'd' for days</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If time_interval doesn't match expected format.</p> Note <ul> <li>Only supports hours ('h') and days ('d') units</li> <li>Number must be a positive integer</li> <li>Format is case-sensitive ('h' and 'd' must be lowercase)</li> <li>No spaces allowed in the interval string</li> </ul> Example Source code in <code>hydroutils\\hydro_units.py</code> <pre><code>def get_time_interval_info(time_interval: str) -&gt; Tuple[int, str]:\n    \"\"\"Parse a time interval string into its numeric value and unit.\n\n    This function extracts the numeric value and unit from a time interval\n    string using regular expressions. It supports hourly and daily intervals\n    in a standardized format.\n\n    Args:\n        time_interval (str): Time interval string in format \"Nh\" or \"Nd\" where\n            N is a positive integer. Examples: \"1h\", \"3h\", \"1d\", \"5d\".\n\n    Returns:\n        Tuple[int, str]: Two-element tuple containing:\n            - number (int): The numeric value from the interval\n            - unit (str): The unit, either 'h' for hours or 'd' for days\n\n    Raises:\n        ValueError: If time_interval doesn't match expected format.\n\n    Note:\n        - Only supports hours ('h') and days ('d') units\n        - Number must be a positive integer\n        - Format is case-sensitive ('h' and 'd' must be lowercase)\n        - No spaces allowed in the interval string\n\n    Example:\n        &gt;&gt;&gt; # Hourly intervals\n        &gt;&gt;&gt; get_time_interval_info(\"3h\")\n        (3, 'h')\n        &gt;&gt;&gt; get_time_interval_info(\"24h\")\n        (24, 'h')\n\n        &gt;&gt;&gt; # Daily intervals\n        &gt;&gt;&gt; get_time_interval_info(\"1d\")\n        (1, 'd')\n        &gt;&gt;&gt; get_time_interval_info(\"7d\")\n        (7, 'd')\n\n        &gt;&gt;&gt; # Invalid format raises error\n        &gt;&gt;&gt; get_time_interval_info(\"3hours\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n        Traceback (most recent call last):\n            ...\n        ValueError: Invalid time interval format: 3hours\n    \"\"\"\n    match = re.match(r\"^(\\d+)([hd])$\", time_interval)\n    if not match:\n        raise ValueError(f\"Invalid time interval format: {time_interval}\")\n\n    number, unit = match.groups()\n    return int(number), unit\n</code></pre>"},{"location":"api/hydro_units/#hydroutils.hydro_units.get_time_interval_info--hourly-intervals","title":"Hourly intervals","text":"<p>get_time_interval_info(\"3h\") (3, 'h') get_time_interval_info(\"24h\") (24, 'h')</p>"},{"location":"api/hydro_units/#hydroutils.hydro_units.get_time_interval_info--daily-intervals","title":"Daily intervals","text":"<p>get_time_interval_info(\"1d\") (1, 'd') get_time_interval_info(\"7d\") (7, 'd')</p>"},{"location":"api/hydro_units/#hydroutils.hydro_units.get_time_interval_info--invalid-format-raises-error","title":"Invalid format raises error","text":"<p>get_time_interval_info(\"3hours\")  # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last):     ... ValueError: Invalid time interval format: 3hours</p>"},{"location":"api/hydro_units/#hydroutils.hydro_units.streamflow_unit_conv","title":"<code>streamflow_unit_conv(data, area, target_unit, source_unit=None, area_unit='km^2')</code>","text":"<p>Convert streamflow data units between depth units (mm/time) and volume units (m\u00b3/s).</p> <p>This function automatically detects conversion direction based on source and target units, removing the need for an explicit inverse parameter.</p>"},{"location":"api/hydro_units/#hydroutils.hydro_units.streamflow_unit_conv--parameters","title":"Parameters","text":"<p>data : numpy.ndarray, pandas.Series, pandas.DataFrame, or xarray.Dataset     Streamflow data. Can include unit information in attributes (xarray) or     requires source_unit parameter for numpy/pandas data. area : numpy.ndarray, pandas.Series, pandas.DataFrame, xarray.Dataset, or pint.Quantity     Basin area data. Units will be detected from data attributes or pint units.     If no units detected, area_unit parameter will be used. target_unit : str     Target unit for conversion. Examples: \"mm/d\", \"mm/h\", \"mm/3h\", \"m^3/s\". source_unit : str, optional     Source unit of streamflow data. Required if data has no unit information.     If provided and data has units, they must match or ValueError is raised. area_unit : str, optional     Unit for area when area data has no unit information. Default is \"km^2\".</p>"},{"location":"api/hydro_units/#hydroutils.hydro_units.streamflow_unit_conv--returns","title":"Returns","text":"<p>Converted data in the same type as input data. Unit information is preserved in xarray attributes when applicable.</p>"},{"location":"api/hydro_units/#hydroutils.hydro_units.streamflow_unit_conv--raises","title":"Raises","text":"<p>ValueError     If no unit information can be determined for data or area.     If source_unit conflicts with detected data units.     If units are incompatible for conversion.</p>"},{"location":"api/hydro_units/#hydroutils.hydro_units.streamflow_unit_conv--examples","title":"Examples","text":"<p>import numpy as np import pandas as pd</p> Source code in <code>hydroutils\\hydro_units.py</code> <pre><code>def streamflow_unit_conv(\n    data,\n    area,\n    target_unit,\n    source_unit=None,\n    area_unit=\"km^2\",\n):\n    \"\"\"Convert streamflow data units between depth units (mm/time) and volume units (m\u00b3/s).\n\n    This function automatically detects conversion direction based on source and target units,\n    removing the need for an explicit inverse parameter.\n\n    Parameters\n    ----------\n    data : numpy.ndarray, pandas.Series, pandas.DataFrame, or xarray.Dataset\n        Streamflow data. Can include unit information in attributes (xarray) or\n        requires source_unit parameter for numpy/pandas data.\n    area : numpy.ndarray, pandas.Series, pandas.DataFrame, xarray.Dataset, or pint.Quantity\n        Basin area data. Units will be detected from data attributes or pint units.\n        If no units detected, area_unit parameter will be used.\n    target_unit : str\n        Target unit for conversion. Examples: \"mm/d\", \"mm/h\", \"mm/3h\", \"m^3/s\".\n    source_unit : str, optional\n        Source unit of streamflow data. Required if data has no unit information.\n        If provided and data has units, they must match or ValueError is raised.\n    area_unit : str, optional\n        Unit for area when area data has no unit information. Default is \"km^2\".\n\n    Returns\n    -------\n    Converted data in the same type as input data.\n    Unit information is preserved in xarray attributes when applicable.\n\n    Raises\n    ------\n    ValueError\n        If no unit information can be determined for data or area.\n        If source_unit conflicts with detected data units.\n        If units are incompatible for conversion.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; # Convert m\u00b3/s to mm/day\n    &gt;&gt;&gt; flow = np.array([10.5, 15.2, 8.1])\n    &gt;&gt;&gt; basin_area = np.array([1000])  # km\u00b2\n    &gt;&gt;&gt; result = streamflow_unit_conv(flow, basin_area, \"mm/d\", source_unit=\"m^3/s\")\n\n    &gt;&gt;&gt; # Convert mm/h to m\u00b3/s\n    &gt;&gt;&gt; flow_mm = np.array([2.1, 3.5, 1.8])\n    &gt;&gt;&gt; result = streamflow_unit_conv(flow_mm, basin_area, \"m^3/s\", source_unit=\"mm/h\")\n    \"\"\"\n    # Step 1: Detect and validate source unit\n    detected_source_unit = _detect_data_unit(data, source_unit)\n\n    # Step 2: Detect and validate area unit\n    detected_area_unit = _detect_area_unit(area, area_unit)\n\n    # Step 3: Determine conversion direction and validate compatibility\n    is_depth_to_volume = _determine_conversion_direction(\n        detected_source_unit, target_unit\n    )\n\n    # Step 4: Early return if no conversion needed\n    if _normalize_unit(detected_source_unit) == _normalize_unit(target_unit):\n        return data\n\n    # Step 5: Perform the actual conversion based on data type\n    return _perform_conversion(\n        data,\n        area,\n        detected_source_unit,\n        detected_area_unit,\n        target_unit,\n        is_depth_to_volume,\n    )\n</code></pre>"},{"location":"api/hydro_units/#hydroutils.hydro_units.streamflow_unit_conv--convert-m3s-to-mmday","title":"Convert m\u00b3/s to mm/day","text":"<p>flow = np.array([10.5, 15.2, 8.1]) basin_area = np.array([1000])  # km\u00b2 result = streamflow_unit_conv(flow, basin_area, \"mm/d\", source_unit=\"m^3/s\")</p>"},{"location":"api/hydro_units/#hydroutils.hydro_units.streamflow_unit_conv--convert-mmh-to-m3s","title":"Convert mm/h to m\u00b3/s","text":"<p>flow_mm = np.array([2.1, 3.5, 1.8]) result = streamflow_unit_conv(flow_mm, basin_area, \"m^3/s\", source_unit=\"mm/h\")</p>"},{"location":"api/hydro_units/#hydroutils.hydro_units.validate_unit_compatibility","title":"<code>validate_unit_compatibility(source_unit, target_unit)</code>","text":"<p>Check if two hydrological units can be converted between each other.</p> <p>This function determines whether two units are compatible for hydrological unit conversion. It supports depth units (mm/time) and volume units (m\u00b3/s), and checks if the conversion between them is possible.</p> <p>Parameters:</p> Name Type Description Default <code>source_unit</code> <code>str</code> <p>Source unit string. Examples: - Depth units: \"mm/h\", \"mm/3h\", \"mm/d\", \"in/d\" - Volume units: \"m^3/s\", \"ft^3/s\", \"l/s\"</p> required <code>target_unit</code> <code>str</code> <p>Target unit string (same format as source_unit).</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if units are compatible for conversion, False otherwise.</p> Note <ul> <li>Supports various time intervals for depth units</li> <li>Recognizes multiple formats for volume units</li> <li>Case-sensitive unit matching</li> <li>Compatible conversions:<ul> <li>depth -&gt; volume (e.g., mm/h -&gt; m\u00b3/s)</li> <li>volume -&gt; depth (e.g., m\u00b3/s -&gt; mm/d)</li> <li>depth -&gt; depth (e.g., mm/h -&gt; mm/d)</li> <li>volume -&gt; volume (e.g., m\u00b3/s -&gt; ft\u00b3/s)</li> </ul> </li> </ul> Example Source code in <code>hydroutils\\hydro_units.py</code> <pre><code>def validate_unit_compatibility(source_unit: str, target_unit: str) -&gt; bool:\n    \"\"\"Check if two hydrological units can be converted between each other.\n\n    This function determines whether two units are compatible for hydrological\n    unit conversion. It supports depth units (mm/time) and volume units (m\u00b3/s),\n    and checks if the conversion between them is possible.\n\n    Args:\n        source_unit (str): Source unit string. Examples:\n            - Depth units: \"mm/h\", \"mm/3h\", \"mm/d\", \"in/d\"\n            - Volume units: \"m^3/s\", \"ft^3/s\", \"l/s\"\n        target_unit (str): Target unit string (same format as source_unit).\n\n    Returns:\n        bool: True if units are compatible for conversion, False otherwise.\n\n    Note:\n        - Supports various time intervals for depth units\n        - Recognizes multiple formats for volume units\n        - Case-sensitive unit matching\n        - Compatible conversions:\n            - depth -&gt; volume (e.g., mm/h -&gt; m\u00b3/s)\n            - volume -&gt; depth (e.g., m\u00b3/s -&gt; mm/d)\n            - depth -&gt; depth (e.g., mm/h -&gt; mm/d)\n            - volume -&gt; volume (e.g., m\u00b3/s -&gt; ft\u00b3/s)\n\n    Example:\n        &gt;&gt;&gt; # Compatible conversions\n        &gt;&gt;&gt; validate_unit_compatibility(\"mm/3h\", \"m^3/s\")\n        True\n        &gt;&gt;&gt; validate_unit_compatibility(\"m^3/s\", \"mm/d\")\n        True\n        &gt;&gt;&gt; validate_unit_compatibility(\"mm/h\", \"mm/d\")\n        True\n\n        &gt;&gt;&gt; # Incompatible conversions\n        &gt;&gt;&gt; validate_unit_compatibility(\"mm/h\", \"celsius\")\n        False\n        &gt;&gt;&gt; validate_unit_compatibility(\"m^3/s\", \"kg/m^3\")\n        False\n    \"\"\"\n    # Define unit categories\n    depth_units = re.compile(r\"mm/\\d*[hd]\")\n    volume_units = re.compile(r\"m\\^?3/s\")\n\n    source_is_depth = bool(depth_units.match(source_unit))\n    source_is_volume = bool(volume_units.match(source_unit))\n    target_is_depth = bool(depth_units.match(target_unit))\n    target_is_volume = bool(volume_units.match(target_unit))\n\n    # Compatible if both are depth units, both are volume units, or one of each\n    return (source_is_depth or source_is_volume) and (\n        target_is_depth or target_is_volume\n    )\n</code></pre>"},{"location":"api/hydro_units/#hydroutils.hydro_units.validate_unit_compatibility--compatible-conversions","title":"Compatible conversions","text":"<p>validate_unit_compatibility(\"mm/3h\", \"m^3/s\") True validate_unit_compatibility(\"m^3/s\", \"mm/d\") True validate_unit_compatibility(\"mm/h\", \"mm/d\") True</p>"},{"location":"api/hydro_units/#hydroutils.hydro_units.validate_unit_compatibility--incompatible-conversions","title":"Incompatible conversions","text":"<p>validate_unit_compatibility(\"mm/h\", \"celsius\") False validate_unit_compatibility(\"m^3/s\", \"kg/m^3\") False</p>"},{"location":"api/hydroutils/","title":"API Reference","text":"<p>This section provides detailed documentation for all modules and functions in the <code>hydroutils</code> package.</p>"},{"location":"api/hydroutils/#core-module","title":"Core Module","text":"<p>Author: Wenyu Ouyang Date: 2022-12-02 10:42:19 LastEditTime: 2025-08-18 14:40:03 LastEditors: Wenyu Ouyang Description: Top-level package for hydroutils. FilePath: \\hydroutils\\hydroutils__init__.py Copyright (c) 2023-2024 Wenyu Ouyang. All rights reserved.</p>"},{"location":"api/hydroutils/#hydroutils.HydroWarning","title":"<code>HydroWarning</code>","text":"<p>A class for displaying formatted warning messages using Rich console.</p> <p>This class provides methods for displaying different types of warning messages with consistent formatting and color coding using the Rich library.</p> <p>Attributes:</p> Name Type Description <code>console</code> <code>Console</code> <p>Rich console instance for formatted output.</p> Source code in <code>hydroutils\\hydro_log.py</code> <pre><code>class HydroWarning:\n    \"\"\"A class for displaying formatted warning messages using Rich console.\n\n    This class provides methods for displaying different types of warning messages\n    with consistent formatting and color coding using the Rich library.\n\n    Attributes:\n        console (Console): Rich console instance for formatted output.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize HydroWarning with a Rich console instance.\"\"\"\n        self.console = Console()\n\n    def no_directory(self, directory_name, message=None):\n        \"\"\"Display a warning message for a missing directory.\n\n        Args:\n            directory_name (str): Name of the missing directory.\n            message (Text, optional): Custom warning message. If None, a default\n                message will be created. Defaults to None.\n        \"\"\"\n        if message is None:\n            message = Text(\n                f\"There is no such directory: {directory_name}\", style=\"bold red\"\n            )\n        self.console.print(message)\n\n    def file_not_found(self, file_name, message=None):\n        \"\"\"Display a warning message for a file that cannot be found.\n\n        Args:\n            file_name (str): Name of the file that could not be found.\n            message (Text, optional): Custom warning message. If None, a default\n                message will be created. Defaults to None.\n        \"\"\"\n        if message is None:\n            message = Text(\n                f\"We didn't find this file: {file_name}\", style=\"bold yellow\"\n            )\n        self.console.print(message)\n\n    def operation_successful(self, operation_detail, message=None):\n        \"\"\"Display a success message for a completed operation.\n\n        Args:\n            operation_detail (str): Description of the successful operation.\n            message (Text, optional): Custom success message. If None, a default\n                message will be created. Defaults to None.\n        \"\"\"\n        if message is None:\n            message = Text(f\"Operation Success: {operation_detail}\", style=\"bold green\")\n        self.console.print(message)\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.HydroWarning.__init__","title":"<code>__init__()</code>","text":"<p>Initialize HydroWarning with a Rich console instance.</p> Source code in <code>hydroutils\\hydro_log.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize HydroWarning with a Rich console instance.\"\"\"\n    self.console = Console()\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.HydroWarning.file_not_found","title":"<code>file_not_found(file_name, message=None)</code>","text":"<p>Display a warning message for a file that cannot be found.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>Name of the file that could not be found.</p> required <code>message</code> <code>Text</code> <p>Custom warning message. If None, a default message will be created. Defaults to None.</p> <code>None</code> Source code in <code>hydroutils\\hydro_log.py</code> <pre><code>def file_not_found(self, file_name, message=None):\n    \"\"\"Display a warning message for a file that cannot be found.\n\n    Args:\n        file_name (str): Name of the file that could not be found.\n        message (Text, optional): Custom warning message. If None, a default\n            message will be created. Defaults to None.\n    \"\"\"\n    if message is None:\n        message = Text(\n            f\"We didn't find this file: {file_name}\", style=\"bold yellow\"\n        )\n    self.console.print(message)\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.HydroWarning.no_directory","title":"<code>no_directory(directory_name, message=None)</code>","text":"<p>Display a warning message for a missing directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory_name</code> <code>str</code> <p>Name of the missing directory.</p> required <code>message</code> <code>Text</code> <p>Custom warning message. If None, a default message will be created. Defaults to None.</p> <code>None</code> Source code in <code>hydroutils\\hydro_log.py</code> <pre><code>def no_directory(self, directory_name, message=None):\n    \"\"\"Display a warning message for a missing directory.\n\n    Args:\n        directory_name (str): Name of the missing directory.\n        message (Text, optional): Custom warning message. If None, a default\n            message will be created. Defaults to None.\n    \"\"\"\n    if message is None:\n        message = Text(\n            f\"There is no such directory: {directory_name}\", style=\"bold red\"\n        )\n    self.console.print(message)\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.HydroWarning.operation_successful","title":"<code>operation_successful(operation_detail, message=None)</code>","text":"<p>Display a success message for a completed operation.</p> <p>Parameters:</p> Name Type Description Default <code>operation_detail</code> <code>str</code> <p>Description of the successful operation.</p> required <code>message</code> <code>Text</code> <p>Custom success message. If None, a default message will be created. Defaults to None.</p> <code>None</code> Source code in <code>hydroutils\\hydro_log.py</code> <pre><code>def operation_successful(self, operation_detail, message=None):\n    \"\"\"Display a success message for a completed operation.\n\n    Args:\n        operation_detail (str): Description of the successful operation.\n        message (Text, optional): Custom success message. If None, a default\n            message will be created. Defaults to None.\n    \"\"\"\n    if message is None:\n        message = Text(f\"Operation Success: {operation_detail}\", style=\"bold green\")\n    self.console.print(message)\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.NumpyArrayEncoder","title":"<code>NumpyArrayEncoder</code>","text":"<p>               Bases: <code>JSONEncoder</code></p> <p>JSON encoder that handles NumPy arrays and scalar types.</p> <p>This encoder converts NumPy arrays and scalar types to Python native types that can be serialized by the standard JSON encoder.</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>class NumpyArrayEncoder(json.JSONEncoder):\n    \"\"\"JSON encoder that handles NumPy arrays and scalar types.\n\n    This encoder converts NumPy arrays and scalar types to Python native types\n    that can be serialized by the standard JSON encoder.\n    \"\"\"\n\n    def default(self, obj):\n        \"\"\"Convert NumPy types to JSON serializable objects.\n\n        Args:\n            obj: Object to encode.\n\n        Returns:\n            JSON serializable object.\n        \"\"\"\n        if isinstance(obj, np.ndarray):\n            return self.convert_ndarray(obj)\n        elif isinstance(obj, (np.integer, np.floating)):\n            return obj.item()\n        return json.JSONEncoder.default(self, obj)\n\n    def convert_ndarray(self, array):\n        \"\"\"Convert a NumPy array to a nested list.\n\n        Args:\n            array (np.ndarray): NumPy array to convert.\n\n        Returns:\n            list or scalar: Python native type equivalent of the array.\n        \"\"\"\n        if array.ndim == 0:\n            return array.item()\n        return [\n            (\n                self.convert_ndarray(element)\n                if isinstance(element, np.ndarray)\n                else element\n            )\n            for element in array\n        ]\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.NumpyArrayEncoder.convert_ndarray","title":"<code>convert_ndarray(array)</code>","text":"<p>Convert a NumPy array to a nested list.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>NumPy array to convert.</p> required <p>Returns:</p> Type Description <p>list or scalar: Python native type equivalent of the array.</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def convert_ndarray(self, array):\n    \"\"\"Convert a NumPy array to a nested list.\n\n    Args:\n        array (np.ndarray): NumPy array to convert.\n\n    Returns:\n        list or scalar: Python native type equivalent of the array.\n    \"\"\"\n    if array.ndim == 0:\n        return array.item()\n    return [\n        (\n            self.convert_ndarray(element)\n            if isinstance(element, np.ndarray)\n            else element\n        )\n        for element in array\n    ]\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.NumpyArrayEncoder.default","title":"<code>default(obj)</code>","text":"<p>Convert NumPy types to JSON serializable objects.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <p>Object to encode.</p> required <p>Returns:</p> Type Description <p>JSON serializable object.</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def default(self, obj):\n    \"\"\"Convert NumPy types to JSON serializable objects.\n\n    Args:\n        obj: Object to encode.\n\n    Returns:\n        JSON serializable object.\n    \"\"\"\n    if isinstance(obj, np.ndarray):\n        return self.convert_ndarray(obj)\n    elif isinstance(obj, (np.integer, np.floating)):\n        return obj.item()\n    return json.JSONEncoder.default(self, obj)\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.KGE","title":"<code>KGE(xs, xo)</code>","text":"<p>Kling Gupta Efficiency (Gupta et al., 2009, http://dx.doi.org/10.1016/j.jhydrol.2009.08.003) input:     xs: simulated     xo: observed output:     KGE: Kling Gupta Efficiency</p> Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def KGE(xs: np.ndarray, xo: np.ndarray) -&gt; float:\n    \"\"\"\n    Kling Gupta Efficiency (Gupta et al., 2009, http://dx.doi.org/10.1016/j.jhydrol.2009.08.003)\n    input:\n        xs: simulated\n        xo: observed\n    output:\n        KGE: Kling Gupta Efficiency\n    \"\"\"\n    r = np.corrcoef(xo, xs)[0, 1]\n    alpha = np.std(xs) / np.std(xo)\n    beta = np.mean(xs) / np.mean(xo)\n    return 1 - np.sqrt((r - 1) ** 2 + (alpha - 1) ** 2 + (beta - 1) ** 2)\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.add_metric","title":"<code>add_metric(func_name, he_func_name, description)</code>","text":"<p>\u6dfb\u52a0\u65b0\u7684\u6307\u6807\u51fd\u6570</p>"},{"location":"api/hydroutils/#hydroutils.add_metric--parameters","title":"Parameters","text":"<p>func_name : str     \u65b0\u51fd\u6570\u7684\u540d\u79f0 he_func_name : str     HydroErr\u4e2d\u5bf9\u5e94\u51fd\u6570\u7684\u540d\u79f0 description : str     \u51fd\u6570\u63cf\u8ff0</p> Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def add_metric(func_name: str, he_func_name: str, description: str) -&gt; None:\n    \"\"\"\n    \u6dfb\u52a0\u65b0\u7684\u6307\u6807\u51fd\u6570\n\n    Parameters\n    ----------\n    func_name : str\n        \u65b0\u51fd\u6570\u7684\u540d\u79f0\n    he_func_name : str\n        HydroErr\u4e2d\u5bf9\u5e94\u51fd\u6570\u7684\u540d\u79f0\n    description : str\n        \u51fd\u6570\u63cf\u8ff0\n    \"\"\"\n    if hasattr(he, he_func_name):\n        metric_func = _create_metric_function(he_func_name, description)\n        setattr(current_module, func_name, metric_func)\n        HYDRO_METRICS[func_name] = (he_func_name, description)\n        print(f\"\u5df2\u6dfb\u52a0\u6307\u6807\u51fd\u6570: {func_name}\")\n    else:\n        print(f\"\u8b66\u544a: HydroErr\u4e2d\u4e0d\u5b58\u5728\u51fd\u6570 {he_func_name}\")\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.assign_time_start_end","title":"<code>assign_time_start_end(time_ranges, assign_way='intersection')</code>","text":"<p>Determine start and end times from multiple time ranges.</p> <p>Parameters:</p> Name Type Description Default <code>time_ranges</code> <code>list</code> <p>List of time range pairs [[start1, end1], [start2, end2], ...]. Each start/end can be any comparable type (datetime, string, etc.).</p> required <code>assign_way</code> <code>str</code> <p>Method to determine the final range. Defaults to \"intersection\". - \"intersection\": Use latest start time and earliest end time. - \"union\": Use earliest start time and latest end time.</p> <code>'intersection'</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>(time_start, time_end) The determined start and end times.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If assign_way is not \"intersection\" or \"union\".</p> Example <p>ranges = [[\"2020-01-01\", \"2020-12-31\"], [\"2020-03-01\", \"2021-02-28\"]] assign_time_start_end(ranges, \"intersection\") (\"2020-03-01\", \"2020-12-31\") assign_time_start_end(ranges, \"union\") (\"2020-01-01\", \"2021-02-28\")</p> Source code in <code>hydroutils\\hydro_time.py</code> <pre><code>def assign_time_start_end(time_ranges, assign_way=\"intersection\"):\n    \"\"\"Determine start and end times from multiple time ranges.\n\n    Args:\n        time_ranges (list): List of time range pairs [[start1, end1], [start2, end2], ...].\n            Each start/end can be any comparable type (datetime, string, etc.).\n        assign_way (str, optional): Method to determine the final range. Defaults to \"intersection\".\n            - \"intersection\": Use latest start time and earliest end time.\n            - \"union\": Use earliest start time and latest end time.\n\n    Returns:\n        tuple: (time_start, time_end) The determined start and end times.\n\n    Raises:\n        NotImplementedError: If assign_way is not \"intersection\" or \"union\".\n\n    Example:\n        &gt;&gt;&gt; ranges = [[\"2020-01-01\", \"2020-12-31\"], [\"2020-03-01\", \"2021-02-28\"]]\n        &gt;&gt;&gt; assign_time_start_end(ranges, \"intersection\")\n        (\"2020-03-01\", \"2020-12-31\")\n        &gt;&gt;&gt; assign_time_start_end(ranges, \"union\")\n        (\"2020-01-01\", \"2021-02-28\")\n    \"\"\"\n    if assign_way == \"intersection\":\n        time_start = max(t[0] for t in time_ranges)\n        time_end = min(t[1] for t in time_ranges)\n    elif assign_way == \"union\":\n        time_start = min(t[0] for t in time_ranges)\n        time_end = max(t[1] for t in time_ranges)\n    else:\n        raise NotImplementedError(\"We don't support this assign_way yet\")\n    return time_start, time_end\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.boto3_download_file","title":"<code>boto3_download_file(client, bucket_name, object_name, file_path)</code>","text":"<p>Download a file from S3 using boto3.</p> <p>This function downloads an object from S3 storage to a local file using the boto3 client. It provides a simple wrapper around boto3's download_file method.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>client</code> <p>Initialized boto3 S3 client instance.</p> required <code>bucket_name</code> <code>str</code> <p>Name of the bucket containing the object.</p> required <code>object_name</code> <code>str</code> <p>Name of the object to download.</p> required <code>file_path</code> <code>str</code> <p>Local path where the file should be saved.</p> required Example <p>import boto3 client = boto3.client('s3', ...                      endpoint_url='http://localhost:9000', ...                      aws_access_key_id='access_key', ...                      aws_secret_access_key='secret_key') boto3_download_file(client, ...                    'mybucket', ...                    'data/file.csv', ...                    '/local/path/file.csv')</p> Source code in <code>hydroutils\\hydro_s3.py</code> <pre><code>def boto3_download_file(client, bucket_name, object_name, file_path: str):\n    \"\"\"Download a file from S3 using boto3.\n\n    This function downloads an object from S3 storage to a local file using\n    the boto3 client. It provides a simple wrapper around boto3's download_file\n    method.\n\n    Args:\n        client (boto3.client): Initialized boto3 S3 client instance.\n        bucket_name (str): Name of the bucket containing the object.\n        object_name (str): Name of the object to download.\n        file_path (str): Local path where the file should be saved.\n\n    Example:\n        &gt;&gt;&gt; import boto3\n        &gt;&gt;&gt; client = boto3.client('s3',\n        ...                      endpoint_url='http://localhost:9000',\n        ...                      aws_access_key_id='access_key',\n        ...                      aws_secret_access_key='secret_key')\n        &gt;&gt;&gt; boto3_download_file(client,\n        ...                    'mybucket',\n        ...                    'data/file.csv',\n        ...                    '/local/path/file.csv')\n    \"\"\"\n    client.download_file(bucket_name, object_name, file_path)\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.boto3_upload_file","title":"<code>boto3_upload_file(client, bucket_name, object_name, file_path)</code>","text":"<p>Upload a file to S3 using boto3.</p> <p>This function uploads a local file to S3 storage using the boto3 client. If the specified bucket doesn't exist, it will be created automatically. After upload, it returns a list of all objects in the bucket.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>client</code> <p>Initialized boto3 S3 client instance.</p> required <code>bucket_name</code> <code>str</code> <p>Name of the bucket to upload to.</p> required <code>object_name</code> <code>str</code> <p>Name to give the object in S3 storage.</p> required <code>file_path</code> <code>str</code> <p>Path to the local file to upload.</p> required <p>Returns:</p> Type Description <p>list[str]: List of all object keys in the bucket after upload.</p> Note <ul> <li>Creates bucket if it doesn't exist</li> <li>Uses upload_file for efficient file upload</li> <li>Lists all objects in bucket after upload</li> <li>Handles bucket listing and creation using boto3's API</li> </ul> Example <p>import boto3 client = boto3.client('s3', ...                      endpoint_url='http://localhost:9000', ...                      aws_access_key_id='access_key', ...                      aws_secret_access_key='secret_key') objects = boto3_upload_file(client, ...                            'mybucket', ...                            'data/file.csv', ...                            '/local/path/file.csv') print(objects) ['data/file.csv', 'data/other.csv']</p> Source code in <code>hydroutils\\hydro_s3.py</code> <pre><code>def boto3_upload_file(client, bucket_name, object_name, file_path):\n    \"\"\"Upload a file to S3 using boto3.\n\n    This function uploads a local file to S3 storage using the boto3 client.\n    If the specified bucket doesn't exist, it will be created automatically.\n    After upload, it returns a list of all objects in the bucket.\n\n    Args:\n        client (boto3.client): Initialized boto3 S3 client instance.\n        bucket_name (str): Name of the bucket to upload to.\n        object_name (str): Name to give the object in S3 storage.\n        file_path (str): Path to the local file to upload.\n\n    Returns:\n        list[str]: List of all object keys in the bucket after upload.\n\n    Note:\n        - Creates bucket if it doesn't exist\n        - Uses upload_file for efficient file upload\n        - Lists all objects in bucket after upload\n        - Handles bucket listing and creation using boto3's API\n\n    Example:\n        &gt;&gt;&gt; import boto3\n        &gt;&gt;&gt; client = boto3.client('s3',\n        ...                      endpoint_url='http://localhost:9000',\n        ...                      aws_access_key_id='access_key',\n        ...                      aws_secret_access_key='secret_key')\n        &gt;&gt;&gt; objects = boto3_upload_file(client,\n        ...                            'mybucket',\n        ...                            'data/file.csv',\n        ...                            '/local/path/file.csv')\n        &gt;&gt;&gt; print(objects)\n        ['data/file.csv', 'data/other.csv']\n    \"\"\"\n    # Make a bucket\n    bucket_names = [dic[\"Name\"] for dic in client.list_buckets()[\"Buckets\"]]\n    if bucket_name not in bucket_names:\n        client.create_bucket(Bucket=bucket_name)\n    # Upload an object\n    client.upload_file(file_path, bucket_name, object_name)\n    return [dic[\"Key\"] for dic in client.list_objects(Bucket=bucket_name)[\"Contents\"]]\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.cal_4_stat_inds","title":"<code>cal_4_stat_inds(b)</code>","text":"<p>Calculate four basic statistical indices for an array.</p> <p>This function computes four common statistical measures: 10th and 90th percentiles, mean, and standard deviation. If the standard deviation is very small (&lt; 0.001), it is set to 1 to avoid numerical issues.</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>ndarray</code> <p>Input array of numerical values.</p> required <p>Returns:</p> Type Description <code>List[float]</code> <p>List[float]: Four statistical measures in order: - p10: 10th percentile - p90: 90th percentile - mean: Arithmetic mean - std: Standard deviation (minimum 0.001)</p> Note <ul> <li>NaN values should be removed before calling this function</li> <li>If std &lt; 0.001, it is set to 1 to avoid division issues</li> <li>All returned values are cast to float type</li> </ul> Example <p>data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) p10, p90, mean, std = cal_4_stat_inds(data) print(f\"P10: {p10}, P90: {p90}, Mean: {mean}, Std: {std}\") P10: 1.9, P90: 9.1, Mean: 5.5, Std: 2.87</p> Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def cal_4_stat_inds(b: np.ndarray) -&gt; List[float]:\n    \"\"\"Calculate four basic statistical indices for an array.\n\n    This function computes four common statistical measures: 10th and 90th\n    percentiles, mean, and standard deviation. If the standard deviation is\n    very small (&lt; 0.001), it is set to 1 to avoid numerical issues.\n\n    Args:\n        b (np.ndarray): Input array of numerical values.\n\n    Returns:\n        List[float]: Four statistical measures in order:\n            - p10: 10th percentile\n            - p90: 90th percentile\n            - mean: Arithmetic mean\n            - std: Standard deviation (minimum 0.001)\n\n    Note:\n        - NaN values should be removed before calling this function\n        - If std &lt; 0.001, it is set to 1 to avoid division issues\n        - All returned values are cast to float type\n\n    Example:\n        &gt;&gt;&gt; data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        &gt;&gt;&gt; p10, p90, mean, std = cal_4_stat_inds(data)\n        &gt;&gt;&gt; print(f\"P10: {p10}, P90: {p90}, Mean: {mean}, Std: {std}\")\n        P10: 1.9, P90: 9.1, Mean: 5.5, Std: 2.87\n    \"\"\"\n    p10: float = np.percentile(b, 10).astype(float)\n    p90: float = np.percentile(b, 90).astype(float)\n    mean: float = np.mean(b).astype(float)\n    std: float = np.std(b).astype(float)\n    if std &lt; 0.001:\n        std = 1\n    return [p10, p90, mean, std]\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.cal_fdc","title":"<code>cal_fdc(data, quantile_num=100)</code>","text":"<p>Calculate Flow Duration Curves (FDC) for multiple time series.</p> <p>This function computes flow duration curves for multiple time series data, typically used for analyzing streamflow characteristics. It handles NaN values and provides a specified number of quantile points.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array</code> <p>2D array of shape [n_grid, n_day] containing time series data for multiple locations/grids.</p> required <code>quantile_num</code> <code>int</code> <p>Number of quantile points to compute for each FDC. Defaults to 100.</p> <code>100</code> <p>Returns:</p> Type Description <p>np.ndarray: Array of shape [n_grid, quantile_num] containing FDC values for each location/grid.</p> Note <ul> <li>Data is sorted from high to low flow</li> <li>NaN values are removed before processing</li> <li>Empty series are filled with zeros</li> <li>Quantiles are evenly spaced from 0 to 1</li> <li>Output shape is always [n_grid, quantile_num]</li> </ul> <p>Raises:</p> Type Description <code>Exception</code> <p>If output flow array length doesn't match quantile_num.</p> Example <p>data = np.array([ ...     [10, 8, 6, 4, 2],  # First location ...     [20, 16, 12, 8, 4]  # Second location ... ]) fdc = cal_fdc(data, quantile_num=5) print(fdc) array([[10.,  8.,  6.,  4.,  2.],        [20., 16., 12.,  8.,  4.]])</p> Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def cal_fdc(data: np.array, quantile_num=100):\n    \"\"\"Calculate Flow Duration Curves (FDC) for multiple time series.\n\n    This function computes flow duration curves for multiple time series data,\n    typically used for analyzing streamflow characteristics. It handles NaN\n    values and provides a specified number of quantile points.\n\n    Args:\n        data (np.array): 2D array of shape [n_grid, n_day] containing time\n            series data for multiple locations/grids.\n        quantile_num (int, optional): Number of quantile points to compute\n            for each FDC. Defaults to 100.\n\n    Returns:\n        np.ndarray: Array of shape [n_grid, quantile_num] containing FDC\n            values for each location/grid.\n\n    Note:\n        - Data is sorted from high to low flow\n        - NaN values are removed before processing\n        - Empty series are filled with zeros\n        - Quantiles are evenly spaced from 0 to 1\n        - Output shape is always [n_grid, quantile_num]\n\n    Raises:\n        Exception: If output flow array length doesn't match quantile_num.\n\n    Example:\n        &gt;&gt;&gt; data = np.array([\n        ...     [10, 8, 6, 4, 2],  # First location\n        ...     [20, 16, 12, 8, 4]  # Second location\n        ... ])\n        &gt;&gt;&gt; fdc = cal_fdc(data, quantile_num=5)\n        &gt;&gt;&gt; print(fdc)\n        array([[10.,  8.,  6.,  4.,  2.],\n               [20., 16., 12.,  8.,  4.]])\n    \"\"\"\n    # data = n_grid * n_day\n    n_grid, n_day = data.shape\n    fdc = np.full([n_grid, quantile_num], np.nan)\n    for ii in range(n_grid):\n        temp_data0 = data[ii, :]\n        temp_data = temp_data0[~np.isnan(temp_data0)]\n        # deal with no data case for some gages\n        if len(temp_data) == 0:\n            temp_data = np.full(n_day, 0)\n        # sort from large to small\n        temp_sort = np.sort(temp_data)[::-1]\n        # select quantile_num quantile points\n        n_len = len(temp_data)\n        ind = (np.arange(quantile_num) / quantile_num * n_len).astype(int)\n        fdc_flow = temp_sort[ind]\n        if len(fdc_flow) != quantile_num:\n            raise Exception(\"unknown assimilation variable\")\n        else:\n            fdc[ii, :] = fdc_flow\n\n    return fdc\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.cal_stat","title":"<code>cal_stat(x)</code>","text":"<p>Calculate basic statistics for an array, handling NaN values.</p> <p>This function computes four basic statistical measures (10th and 90th percentiles, mean, and standard deviation) while properly handling NaN values. If the array is empty after removing NaN values, a zero value is used for calculations.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input array, may contain NaN values.</p> required <p>Returns:</p> Type Description <code>List[float]</code> <p>List[float]: Four statistical measures in order: - p10: 10th percentile - p90: 90th percentile - mean: Arithmetic mean - std: Standard deviation (minimum 0.001)</p> Note <ul> <li>NaN values are automatically removed before calculations</li> <li>If all values are NaN, returns statistics for [0]</li> <li>Uses cal_4_stat_inds for actual calculations</li> <li>If std &lt; 0.001, it is set to 1 to avoid division issues</li> </ul> Example <p>data = np.array([1.0, 2.0, np.nan, 4.0, 5.0]) p10, p90, mean, std = cal_stat(data) print(f\"P10: {p10}, P90: {p90}, Mean: {mean}, Std: {std}\") P10: 1.3, P90: 4.7, Mean: 3.0, Std: 1.58</p> Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def cal_stat(x: np.ndarray) -&gt; List[float]:\n    \"\"\"Calculate basic statistics for an array, handling NaN values.\n\n    This function computes four basic statistical measures (10th and 90th\n    percentiles, mean, and standard deviation) while properly handling NaN\n    values. If the array is empty after removing NaN values, a zero value\n    is used for calculations.\n\n    Args:\n        x (np.ndarray): Input array, may contain NaN values.\n\n    Returns:\n        List[float]: Four statistical measures in order:\n            - p10: 10th percentile\n            - p90: 90th percentile\n            - mean: Arithmetic mean\n            - std: Standard deviation (minimum 0.001)\n\n    Note:\n        - NaN values are automatically removed before calculations\n        - If all values are NaN, returns statistics for [0]\n        - Uses cal_4_stat_inds for actual calculations\n        - If std &lt; 0.001, it is set to 1 to avoid division issues\n\n    Example:\n        &gt;&gt;&gt; data = np.array([1.0, 2.0, np.nan, 4.0, 5.0])\n        &gt;&gt;&gt; p10, p90, mean, std = cal_stat(data)\n        &gt;&gt;&gt; print(f\"P10: {p10}, P90: {p90}, Mean: {mean}, Std: {std}\")\n        P10: 1.3, P90: 4.7, Mean: 3.0, Std: 1.58\n    \"\"\"\n    a = x.flatten()\n    b = a[~np.isnan(a)]\n    if b.size == 0:\n        # if b is [], then give it a 0 value\n        b = np.array([0])\n    return cal_4_stat_inds(b)\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.cal_stat_gamma","title":"<code>cal_stat_gamma(x)</code>","text":"<p>Transform time series data to approximate normal distribution.</p> <p>This function applies a transformation to hydrological time series data (streamflow, precipitation, evapotranspiration) to make it more normally distributed. The transformation is: log10(sqrt(x) + 0.1).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Time series data, typically daily values of: - Streamflow - Precipitation - Evapotranspiration</p> required <p>Returns:</p> Type Description <code>List[float]</code> <p>List[float]: Four statistical measures of transformed data: - p10: 10th percentile - p90: 90th percentile - mean: Arithmetic mean - std: Standard deviation (minimum 0.001)</p> Note <ul> <li>NaN values are automatically removed before transformation</li> <li>Transformation: log10(sqrt(x) + 0.1)</li> <li>This transformation helps handle gamma-distributed data</li> <li>If std &lt; 0.001, it is set to 1 to avoid division issues</li> </ul> Example <p>data = np.array([0.0, 0.1, 1.0, 10.0, np.nan, 100.0]) p10, p90, mean, std = cal_stat_gamma(data) print(f\"P10: {p10:.2f}, P90: {p90:.2f}\") P10: -0.52, P90: 1.01</p> Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def cal_stat_gamma(x: np.ndarray) -&gt; List[float]:\n    \"\"\"Transform time series data to approximate normal distribution.\n\n    This function applies a transformation to hydrological time series data\n    (streamflow, precipitation, evapotranspiration) to make it more normally\n    distributed. The transformation is: log10(sqrt(x) + 0.1).\n\n    Args:\n        x (np.ndarray): Time series data, typically daily values of:\n            - Streamflow\n            - Precipitation\n            - Evapotranspiration\n\n    Returns:\n        List[float]: Four statistical measures of transformed data:\n            - p10: 10th percentile\n            - p90: 90th percentile\n            - mean: Arithmetic mean\n            - std: Standard deviation (minimum 0.001)\n\n    Note:\n        - NaN values are automatically removed before transformation\n        - Transformation: log10(sqrt(x) + 0.1)\n        - This transformation helps handle gamma-distributed data\n        - If std &lt; 0.001, it is set to 1 to avoid division issues\n\n    Example:\n        &gt;&gt;&gt; data = np.array([0.0, 0.1, 1.0, 10.0, np.nan, 100.0])\n        &gt;&gt;&gt; p10, p90, mean, std = cal_stat_gamma(data)\n        &gt;&gt;&gt; print(f\"P10: {p10:.2f}, P90: {p90:.2f}\")\n        P10: -0.52, P90: 1.01\n    \"\"\"\n    a = x.flatten()\n    b = a[~np.isnan(a)]  # kick out Nan\n    b = np.log10(\n        np.sqrt(b) + 0.1\n    )  # do some tranformation to change gamma characteristics\n    return cal_4_stat_inds(b)\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.cal_stat_prcp_norm","title":"<code>cal_stat_prcp_norm(x, meanprep)</code>","text":"<p>Normalize variables by precipitation and calculate gamma statistics.</p> <p>This function normalizes a variable (e.g., streamflow) by mean precipitation to remove the influence of rainfall magnitude, making statistics comparable between dry and wet basins. After normalization, gamma transformation is applied.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Data to be normalized, typically streamflow or other hydrological variables.</p> required <code>meanprep</code> <code>ndarray</code> <p>Mean precipitation values for normalization. Usually obtained from basin attributes (e.g., p_mean).</p> required <p>Returns:</p> Type Description <p>List[float]: Four statistical measures of normalized data: - p10: 10th percentile - p90: 90th percentile - mean: Arithmetic mean - std: Standard deviation (minimum 0.001)</p> Note <ul> <li>Normalization: x / meanprep (unit: mm/day / mm/day)</li> <li>After normalization, gamma transformation is applied</li> <li>Helps compare basins with different precipitation regimes</li> <li>If std &lt; 0.001, it is set to 1 to avoid division issues</li> </ul> Example <p>data = np.array([[10.0, 20.0], [30.0, 40.0]])  # 2 basins, 2 timesteps mean_prep = np.array([100.0, 200.0])  # Mean prep for 2 basins p10, p90, mean, std = cal_stat_prcp_norm(data, mean_prep) print(f\"P10: {p10:.3f}, P90: {p90:.3f}\") P10: -0.523, P90: -0.398</p> Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def cal_stat_prcp_norm(x, meanprep):\n    \"\"\"Normalize variables by precipitation and calculate gamma statistics.\n\n    This function normalizes a variable (e.g., streamflow) by mean precipitation\n    to remove the influence of rainfall magnitude, making statistics comparable\n    between dry and wet basins. After normalization, gamma transformation is\n    applied.\n\n    Args:\n        x (np.ndarray): Data to be normalized, typically streamflow or other\n            hydrological variables.\n        meanprep (np.ndarray): Mean precipitation values for normalization.\n            Usually obtained from basin attributes (e.g., p_mean).\n\n    Returns:\n        List[float]: Four statistical measures of normalized data:\n            - p10: 10th percentile\n            - p90: 90th percentile\n            - mean: Arithmetic mean\n            - std: Standard deviation (minimum 0.001)\n\n    Note:\n        - Normalization: x / meanprep (unit: mm/day / mm/day)\n        - After normalization, gamma transformation is applied\n        - Helps compare basins with different precipitation regimes\n        - If std &lt; 0.001, it is set to 1 to avoid division issues\n\n    Example:\n        &gt;&gt;&gt; data = np.array([[10.0, 20.0], [30.0, 40.0]])  # 2 basins, 2 timesteps\n        &gt;&gt;&gt; mean_prep = np.array([100.0, 200.0])  # Mean prep for 2 basins\n        &gt;&gt;&gt; p10, p90, mean, std = cal_stat_prcp_norm(data, mean_prep)\n        &gt;&gt;&gt; print(f\"P10: {p10:.3f}, P90: {p90:.3f}\")\n        P10: -0.523, P90: -0.398\n    \"\"\"\n    # meanprep = readAttr(gageDict['id'], ['q_mean'])\n    tempprep = np.tile(meanprep, (1, x.shape[1]))\n    # unit (mm/day)/(mm/day)\n    flowua = x / tempprep\n    return cal_stat_gamma(flowua)\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.calculate_utc_offset","title":"<code>calculate_utc_offset(lat, lng, date=None)</code>","text":"<p>Calculate the UTC offset for a geographic location.</p> <p>This function determines the timezone and UTC offset for a given latitude and longitude coordinate pair using the tzfpy library, which provides accurate timezone data based on geographic location.</p> <p>Parameters:</p> Name Type Description Default <code>lat</code> <code>float</code> <p>Latitude in decimal degrees (-90 to 90).</p> required <code>lng</code> <code>float</code> <p>Longitude in decimal degrees (-180 to 180).</p> required <code>date</code> <code>datetime</code> <p>The date to calculate the offset for. Defaults to current UTC time. Important for handling daylight saving time.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>int</code> <p>UTC offset in hours, or None if timezone cannot be determined.</p> Example <p>calculate_utc_offset(35.6762, 139.6503)  # Tokyo, Japan 9 calculate_utc_offset(51.5074, -0.1278)   # London, UK 0  # or 1 during DST</p> Source code in <code>hydroutils\\hydro_time.py</code> <pre><code>def calculate_utc_offset(lat, lng, date=None):\n    \"\"\"Calculate the UTC offset for a geographic location.\n\n    This function determines the timezone and UTC offset for a given latitude and\n    longitude coordinate pair using the tzfpy library, which provides accurate\n    timezone data based on geographic location.\n\n    Args:\n        lat (float): Latitude in decimal degrees (-90 to 90).\n        lng (float): Longitude in decimal degrees (-180 to 180).\n        date (datetime.datetime, optional): The date to calculate the offset for.\n            Defaults to current UTC time. Important for handling daylight saving time.\n\n    Returns:\n        int: UTC offset in hours, or None if timezone cannot be determined.\n\n    Example:\n        &gt;&gt;&gt; calculate_utc_offset(35.6762, 139.6503)  # Tokyo, Japan\n        9\n        &gt;&gt;&gt; calculate_utc_offset(51.5074, -0.1278)   # London, UK\n        0  # or 1 during DST\n    \"\"\"\n    if date is None:\n        date = datetime.datetime.utcnow()\n\n    if timezone_str := tzfpy.get_tz(lng, lat):\n        # Get the timezone object using pytz\n        tz = pytz.timezone(timezone_str)\n        # Get the UTC offset for the specified date\n        offset = tz.utcoffset(date)\n        if offset is not None:\n            return int(offset.total_seconds() / 3600)\n    return None\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.create_median_labels","title":"<code>create_median_labels(ax, medians_value, percent25value=None, percent75value=None, size='small')</code>","text":"<p>\"create median labels for boxes in a boxplot Parameters</p> <p>ax : plt.AxesSubplot     an ax in a fig medians_value : np.array     description percent25value : type, optional     description, by default None percent75value : type, optional     description, by default None size : str, optional     the size of median-value labels, by default small</p> Source code in <code>hydroutils\\hydro_plot.py</code> <pre><code>def create_median_labels(\n    ax, medians_value, percent25value=None, percent75value=None, size=\"small\"\n):\n    \"\"\" \"create median labels for boxes in a boxplot\n    Parameters\n    ----------\n    ax : plt.AxesSubplot\n        an ax in a fig\n    medians_value : np.array\n        _description_\n    percent25value : _type_, optional\n        _description_, by default None\n    percent75value : _type_, optional\n        _description_, by default None\n    size : str, optional\n        the size of median-value labels, by default small\n    \"\"\"\n    decimal_places = \"2\"\n    if percent25value is None or percent75value is None:\n        vertical_offset = np.min(medians_value * 0.01)  # offset from median for display\n    else:\n        per25min = np.min(percent25value)\n        per75max = np.max(percent75value)\n        vertical_offset = (per75max - per25min) * 0.01\n    median_labels = [format(s, f\".{decimal_places}f\") for s in medians_value]\n    pos = range(len(medians_value))\n    for xtick in ax.get_xticks():\n        ax.text(\n            pos[xtick],\n            medians_value[xtick] + vertical_offset,\n            median_labels[xtick],\n            horizontalalignment=\"center\",\n            color=\"w\",\n            # https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.text.html\n            size=size,\n            weight=\"semibold\",\n        )\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.date_to_julian","title":"<code>date_to_julian(a_time)</code>","text":"<p>Convert a date to Julian day of the year.</p> <p>Parameters:</p> Name Type Description Default <code>a_time</code> <code>Union[str, datetime]</code> <p>Date to convert. If string, must be in format 'YYYY-MM-DD'.</p> required <p>Returns:</p> Name Type Description <code>int</code> <p>Day of the year (1-366).</p> Source code in <code>hydroutils\\hydro_time.py</code> <pre><code>def date_to_julian(a_time):\n    \"\"\"Convert a date to Julian day of the year.\n\n    Args:\n        a_time (Union[str, datetime.datetime]): Date to convert.\n            If string, must be in format 'YYYY-MM-DD'.\n\n    Returns:\n        int: Day of the year (1-366).\n    \"\"\"\n    if type(a_time) == str:\n        fmt = \"%Y-%m-%d\"\n        dt = datetime.datetime.strptime(a_time, fmt)\n    else:\n        dt = a_time\n    tt = dt.timetuple()\n    return tt.tm_yday\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.download_a_file_from_google_drive","title":"<code>download_a_file_from_google_drive(drive, dir_id, download_dir)</code>","text":"<p>Download files from Google Drive.</p> <p>Parameters:</p> Name Type Description Default <code>drive</code> <p>Google Drive API instance.</p> required <code>dir_id</code> <code>str</code> <p>ID of the Google Drive directory.</p> required <code>download_dir</code> <code>str</code> <p>Local directory to save downloaded files.</p> required <p>Returns:</p> Type Description <p>None</p> Note <p>Handles both files and folders recursively. Skips already downloaded files.</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def download_a_file_from_google_drive(drive, dir_id, download_dir):\n    \"\"\"Download files from Google Drive.\n\n    Args:\n        drive: Google Drive API instance.\n        dir_id (str): ID of the Google Drive directory.\n        download_dir (str): Local directory to save downloaded files.\n\n    Returns:\n        None\n\n    Note:\n        Handles both files and folders recursively.\n        Skips already downloaded files.\n    \"\"\"\n    file_list = drive.ListFile(\n        {\"q\": f\"'{dir_id}' in parents and trashed=false\"}\n    ).GetList()\n    for file in file_list:\n        print(f'title: {file[\"title\"]}, id: {file[\"id\"]}')\n        file_dl = drive.CreateFile({\"id\": file[\"id\"]})\n        print(f'mimetype is {file_dl[\"mimeType\"]}')\n        if file_dl[\"mimeType\"] == \"application/vnd.google-apps.folder\":\n            download_dir_sub = os.path.join(download_dir, file_dl[\"title\"])\n            if not os.path.isdir(download_dir_sub):\n                os.makedirs(download_dir_sub)\n            download_a_file_from_google_drive(drive, file_dl[\"id\"], download_dir_sub)\n        else:\n            # download\n            temp_file = os.path.join(download_dir, file_dl[\"title\"])\n            if os.path.isfile(temp_file):\n                print(\"file has been downloaded\")\n                continue\n            file_dl.GetContentFile(os.path.join(download_dir, file_dl[\"title\"]))\n            print(\"Downloading file finished\")\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.download_excel","title":"<code>download_excel(data_url, temp_file)</code>","text":"<p>Download an Excel file from URL.</p> <p>Parameters:</p> Name Type Description Default <code>data_url</code> <code>str</code> <p>URL of the Excel file to download.</p> required <code>temp_file</code> <code>str</code> <p>Path where the Excel file will be saved.</p> required <p>Returns:</p> Type Description <p>None</p> Note <p>Only downloads if the file doesn't already exist locally.</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def download_excel(data_url, temp_file):\n    \"\"\"Download an Excel file from URL.\n\n    Args:\n        data_url (str): URL of the Excel file to download.\n        temp_file (str): Path where the Excel file will be saved.\n\n    Returns:\n        None\n\n    Note:\n        Only downloads if the file doesn't already exist locally.\n    \"\"\"\n    if not os.path.isfile(temp_file):\n        urllib.request.urlretrieve(data_url, temp_file)\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.download_one_zip","title":"<code>download_one_zip(data_url, data_dir)</code>","text":"<p>Download one zip file from URL and extract it.</p> <p>Parameters:</p> Name Type Description Default <code>data_url</code> <code>str</code> <p>The URL of the file to download.</p> required <code>data_dir</code> <code>str</code> <p>Directory where the file will be downloaded and extracted.</p> required <p>Returns:</p> Type Description <p>None</p> Note <p>The function will create the target directory if it doesn't exist.</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def download_one_zip(data_url, data_dir):\n    \"\"\"Download one zip file from URL and extract it.\n\n    Args:\n        data_url (str): The URL of the file to download.\n        data_dir (str): Directory where the file will be downloaded and extracted.\n\n    Returns:\n        None\n\n    Note:\n        The function will create the target directory if it doesn't exist.\n    \"\"\"\n\n    zipfile_path, unzip_dir = zip_file_name_from_url(data_url, data_dir)\n    if not is_there_file(zipfile_path, unzip_dir):\n        if not os.path.isdir(unzip_dir):\n            os.makedirs(unzip_dir)\n        r = requests.get(data_url, stream=True)\n        with open(zipfile_path, \"wb\") as py_file:\n            for chunk in r.iter_content(chunk_size=1024):  # 1024 bytes\n                if chunk:\n                    py_file.write(chunk)\n        unzip_nested_zip(zipfile_path, unzip_dir), download_small_file\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.download_small_file","title":"<code>download_small_file(data_url, temp_file)</code>","text":"<p>Download a small file from URL.</p> <p>Parameters:</p> Name Type Description Default <code>data_url</code> <code>str</code> <p>URL of the file to download.</p> required <code>temp_file</code> <code>str</code> <p>Path where the downloaded file will be saved.</p> required <p>Returns:</p> Type Description <p>None</p> Note <p>Uses requests library for downloading.</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def download_small_file(data_url, temp_file):\n    \"\"\"Download a small file from URL.\n\n    Args:\n        data_url (str): URL of the file to download.\n        temp_file (str): Path where the downloaded file will be saved.\n\n    Returns:\n        None\n\n    Note:\n        Uses requests library for downloading.\n    \"\"\"\n    r = requests.get(data_url)\n    with open(temp_file, \"w\") as f:\n        f.write(r.text)\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.download_small_zip","title":"<code>download_small_zip(data_url, data_dir)</code>","text":"<p>Download a small zip file and extract it.</p> <p>Parameters:</p> Name Type Description Default <code>data_url</code> <code>str</code> <p>URL of the zip file to download.</p> required <code>data_dir</code> <code>str</code> <p>Directory where the file will be downloaded and extracted.</p> required <p>Returns:</p> Type Description <p>None</p> Note <p>Uses urllib.request for downloading small files.</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def download_small_zip(data_url, data_dir):\n    \"\"\"Download a small zip file and extract it.\n\n    Args:\n        data_url (str): URL of the zip file to download.\n        data_dir (str): Directory where the file will be downloaded and extracted.\n\n    Returns:\n        None\n\n    Note:\n        Uses urllib.request for downloading small files.\n    \"\"\"\n    zipfile_path, unzip_dir = zip_file_name_from_url(data_url, data_dir)\n    if not is_there_file(zipfile_path, unzip_dir):\n        if not os.path.isdir(unzip_dir):\n            os.mkdir(unzip_dir)\n        zipfile_path, _ = urllib.request.urlretrieve(data_url, zipfile_path)\n        unzip_nested_zip(zipfile_path, unzip_dir)\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.download_zip_files","title":"<code>download_zip_files(urls, the_dir)</code>","text":"<p>Download multiple files from multiple URLs.</p> <p>Parameters:</p> Name Type Description Default <code>urls</code> <code>list</code> <p>List of URLs to download files from.</p> required <code>the_dir</code> <code>Path</code> <p>Directory where downloaded files will be stored.</p> required <p>Returns:</p> Type Description <p>None</p> Note <p>Uses a temporary directory for caching during download.</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def download_zip_files(urls, the_dir: Path):\n    \"\"\"Download multiple files from multiple URLs.\n\n    Args:\n        urls (list): List of URLs to download files from.\n        the_dir (Path): Directory where downloaded files will be stored.\n\n    Returns:\n        None\n\n    Note:\n        Uses a temporary directory for caching during download.\n    \"\"\"\n    with tempfile.TemporaryDirectory() as tmpdir:\n        cache_names = tmpdir.joinpath(f\"{the_dir.stem}.sqlite\")\n        r = ar.retrieve(urls, \"binary\", cache_name=cache_names, ssl=False)\n        files = [the_dir.joinpath(url.split(\"/\")[-1]) for url in urls]\n        [files[i].write_bytes(io.BytesIO(r[i]).getbuffer()) for i in range(len(files))]\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.ecdf","title":"<code>ecdf(data)</code>","text":"<p>Compute Empirical Cumulative Distribution Function (ECDF).</p> <p>This function calculates the empirical CDF for a given dataset. The ECDF shows the fraction of observations less than or equal to each data point.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input data array.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray]: Two arrays: - x: Sorted input data - y: Cumulative probabilities (0 to 1)</p> Note <ul> <li>Data is sorted in ascending order</li> <li>Probabilities are calculated as (i)/(n) for i=1..n</li> <li>No special handling of NaN values - remove them before calling</li> </ul> Example <p>data = np.array([1, 2, 2, 3, 3, 3, 4, 4, 5]) x, y = ecdf(data) print(\"Values:\", x) Values: [1 2 2 3 3 3 4 4 5] print(\"Probabilities:\", y) Probabilities: [0.111 0.222 0.333 0.444 0.556 0.667 0.778 0.889 1.000]</p> Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def ecdf(data: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Compute Empirical Cumulative Distribution Function (ECDF).\n\n    This function calculates the empirical CDF for a given dataset. The ECDF\n    shows the fraction of observations less than or equal to each data point.\n\n    Args:\n        data (np.ndarray): Input data array.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: Two arrays:\n            - x: Sorted input data\n            - y: Cumulative probabilities (0 to 1)\n\n    Note:\n        - Data is sorted in ascending order\n        - Probabilities are calculated as (i)/(n) for i=1..n\n        - No special handling of NaN values - remove them before calling\n\n    Example:\n        &gt;&gt;&gt; data = np.array([1, 2, 2, 3, 3, 3, 4, 4, 5])\n        &gt;&gt;&gt; x, y = ecdf(data)\n        &gt;&gt;&gt; print(\"Values:\", x)\n        Values: [1 2 2 3 3 3 4 4 5]\n        &gt;&gt;&gt; print(\"Probabilities:\", y)\n        Probabilities: [0.111 0.222 0.333 0.444 0.556 0.667 0.778 0.889 1.000]\n    \"\"\"\n    x = np.sort(data)\n    n = x.size\n    y = np.arange(1, n + 1) / n\n    return (x, y)\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.extract_event_data_by_columns","title":"<code>extract_event_data_by_columns(df, event_indices, data_columns)</code>","text":"<p>Extract event data for specified columns using event indices.</p> <p>This function extracts data from specified columns for a flood event using the index information from get_event_indices or extract_flood_events.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Original DataFrame containing all data.</p> required <code>event_indices</code> <code>Dict</code> <p>Event index information dictionary containing: - warmup_start_idx (int): Start index including warmup period - end_idx (int): End index of event</p> required <code>data_columns</code> <code>List[str]</code> <p>List of column names to extract.</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict</code> <p>Dictionary mapping column names to numpy arrays containing the extracted data. If a column is not found, it will contain an array of NaN values.</p> Example <p>df = pd.DataFrame({ ...     'time': pd.date_range('2020-01-01', periods=5), ...     'flow': [100, 200, 300, 250, 150] ... }) indices = {'warmup_start_idx': 1, 'end_idx': 4} data = extract_event_data_by_columns(df, indices, ['flow']) data['flow'] array([200., 300., 250.])</p> Source code in <code>hydroutils\\hydro_event.py</code> <pre><code>def extract_event_data_by_columns(\n    df: pd.DataFrame, event_indices: Dict, data_columns: List[str]\n) -&gt; Dict:\n    \"\"\"Extract event data for specified columns using event indices.\n\n    This function extracts data from specified columns for a flood event using\n    the index information from get_event_indices or extract_flood_events.\n\n    Args:\n        df (pd.DataFrame): Original DataFrame containing all data.\n        event_indices (Dict): Event index information dictionary containing:\n            - warmup_start_idx (int): Start index including warmup period\n            - end_idx (int): End index of event\n        data_columns (List[str]): List of column names to extract.\n\n    Returns:\n        Dict: Dictionary mapping column names to numpy arrays containing the\n            extracted data. If a column is not found, it will contain an array\n            of NaN values.\n\n    Example:\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     'time': pd.date_range('2020-01-01', periods=5),\n        ...     'flow': [100, 200, 300, 250, 150]\n        ... })\n        &gt;&gt;&gt; indices = {'warmup_start_idx': 1, 'end_idx': 4}\n        &gt;&gt;&gt; data = extract_event_data_by_columns(df, indices, ['flow'])\n        &gt;&gt;&gt; data['flow']\n        array([200., 300., 250.])\n    \"\"\"\n    start_idx = event_indices[\"warmup_start_idx\"]\n    end_idx = event_indices[\"end_idx\"]\n\n    event_data = {}\n    for col in data_columns:\n        if col in df.columns:\n            event_data[col] = df.iloc[start_idx:end_idx][col].values\n        else:\n            # \u5982\u679c\u5217\u4e0d\u5b58\u5728\uff0c\u7528NaN\u6570\u7ec4\u586b\u5145\n            event_data[col] = np.full(end_idx - start_idx, np.nan)\n\n    return event_data\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.extract_flood_events","title":"<code>extract_flood_events(df, warmup_length=0, flood_event_col='flood_event', time_col='time')</code>","text":"<p>Extract flood events from a DataFrame based on a flood event indicator column.</p> <p>This function extracts flood events based on a binary indicator column (flood_event). The design philosophy is to be agnostic about other columns, letting the caller decide how to handle the data columns. The function only requires the flood_event column to mark events and a time column for event naming.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing site data. Must have flood_event and time columns.</p> required <code>warmup_length</code> <code>int</code> <p>Number of time steps to include as warmup period before each event. Defaults to 0.</p> <code>0</code> <code>flood_event_col</code> <code>str</code> <p>Name of the flood event indicator column. Defaults to \"flood_event\".</p> <code>'flood_event'</code> <code>time_col</code> <code>str</code> <p>Name of the time column. Defaults to \"time\".</p> <code>'time'</code> <p>Returns:</p> Type Description <code>List[Dict]</code> <p>List[Dict]: List of flood events. Each dictionary contains: - event_name (str): Event name based on start/end times - start_idx (int): Start index of actual event in original DataFrame - end_idx (int): End index of actual event in original DataFrame - warmup_start_idx (int): Start index including warmup period - data (pd.DataFrame): Event data including warmup period - is_warmup_mask (np.ndarray): Boolean array marking warmup rows - actual_start_time: Start time of actual event - actual_end_time: End time of actual event</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required columns are missing from DataFrame.</p> Example <p>df = pd.DataFrame({ ...     'time': pd.date_range('2020-01-01', periods=5), ...     'flood_event': [0, 1, 1, 1, 0], ...     'flow': [100, 200, 300, 250, 150] ... }) events = extract_flood_events(df, warmup_length=1) len(events) 1 events[0]['data']    time  flood_event  flow 0  2020-01-01    0  100  # warmup period 1  2020-01-02    1  200  # event start 2  2020-01-03    1  300 3  2020-01-04    1  250  # event end</p> Source code in <code>hydroutils\\hydro_event.py</code> <pre><code>def extract_flood_events(\n    df: pd.DataFrame,\n    warmup_length: int = 0,\n    flood_event_col: str = \"flood_event\",\n    time_col: str = \"time\",\n) -&gt; List[Dict]:\n    \"\"\"Extract flood events from a DataFrame based on a flood event indicator column.\n\n    This function extracts flood events based on a binary indicator column (flood_event).\n    The design philosophy is to be agnostic about other columns, letting the caller\n    decide how to handle the data columns. The function only requires the flood_event\n    column to mark events and a time column for event naming.\n\n    Args:\n        df (pd.DataFrame): DataFrame containing site data. Must have flood_event and\n            time columns.\n        warmup_length (int, optional): Number of time steps to include as warmup\n            period before each event. Defaults to 0.\n        flood_event_col (str, optional): Name of the flood event indicator column.\n            Defaults to \"flood_event\".\n        time_col (str, optional): Name of the time column. Defaults to \"time\".\n\n    Returns:\n        List[Dict]: List of flood events. Each dictionary contains:\n            - event_name (str): Event name based on start/end times\n            - start_idx (int): Start index of actual event in original DataFrame\n            - end_idx (int): End index of actual event in original DataFrame\n            - warmup_start_idx (int): Start index including warmup period\n            - data (pd.DataFrame): Event data including warmup period\n            - is_warmup_mask (np.ndarray): Boolean array marking warmup rows\n            - actual_start_time: Start time of actual event\n            - actual_end_time: End time of actual event\n\n    Raises:\n        ValueError: If required columns are missing from DataFrame.\n\n    Example:\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     'time': pd.date_range('2020-01-01', periods=5),\n        ...     'flood_event': [0, 1, 1, 1, 0],\n        ...     'flow': [100, 200, 300, 250, 150]\n        ... })\n        &gt;&gt;&gt; events = extract_flood_events(df, warmup_length=1)\n        &gt;&gt;&gt; len(events)\n        1\n        &gt;&gt;&gt; events[0]['data']\n           time  flood_event  flow\n        0  2020-01-01    0  100  # warmup period\n        1  2020-01-02    1  200  # event start\n        2  2020-01-03    1  300\n        3  2020-01-04    1  250  # event end\n    \"\"\"\n    events: List[Dict] = []\n\n    # \u68c0\u67e5\u5fc5\u8981\u7684\u5217\u662f\u5426\u5b58\u5728\n    required_cols = [flood_event_col, time_col]\n    missing_cols = [col for col in required_cols if col not in df.columns]\n    if missing_cols:\n        raise ValueError(f\"DataFrame\u7f3a\u5c11\u5fc5\u8981\u7684\u5217: {missing_cols}\")\n\n    # \u627e\u5230\u8fde\u7eed\u7684flood_event &gt; 0\u533a\u95f4\n    flood_mask = df[flood_event_col] &gt; 0\n    if not flood_mask.any():\n        return events\n\n    # \u627e\u8fde\u7eed\u533a\u95f4\n    in_event = False\n    start_idx = None\n\n    for idx, is_flood in enumerate(flood_mask):\n        if is_flood and not in_event:\n            start_idx = idx\n            in_event = True\n        elif not is_flood and in_event and start_idx is not None:\n            # \u4e8b\u4ef6\u7ed3\u675f\uff0c\u63d0\u53d6\u4e8b\u4ef6\u6570\u636e\n            event_dict = _extract_single_event(\n                df, start_idx, idx, warmup_length, flood_event_col, time_col\n            )\n            if event_dict is not None:\n                events.append(event_dict)\n            in_event = False\n\n    # \u5904\u7406\u6700\u540e\u4e00\u4e2a\u4e8b\u4ef6\uff08\u5982\u679c\u6570\u636e\u7ed3\u675f\u65f6\u4ecd\u5728\u4e8b\u4ef6\u4e2d\uff09\n    if in_event and start_idx is not None:\n        event_dict = _extract_single_event(\n            df, start_idx, len(df), warmup_length, flood_event_col, time_col\n        )\n        if event_dict is not None:\n            events.append(event_dict)\n\n    return events\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.find_flood_event_segments_as_tuples","title":"<code>find_flood_event_segments_as_tuples(flood_event_array, warmup_length=0)</code>","text":"<p>Find continuous flood event segments and return them as tuples.</p> <p>This is a convenience function that returns event segments as tuples instead of dictionaries, for compatibility with existing code.</p> <p>Parameters:</p> Name Type Description Default <code>flood_event_array</code> <code>ndarray</code> <p>Binary array where values &gt; 0 indicate flood events.</p> required <code>warmup_length</code> <code>int</code> <p>Number of time steps to include as warmup period before each event. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>List[Tuple[int, int, int, int]]</code> <p>List[Tuple[int, int, int, int]]: List of tuples, each containing: (extended_start, extended_end, original_start, original_end) where: - extended_start: Start index including warmup period - extended_end: End index of event - original_start: Start index of actual event - original_end: End index of actual event</p> Example <p>arr = np.array([0, 0, 1, 1, 1, 0]) segments = find_flood_event_segments_as_tuples(arr, warmup_length=1) segments[0]  # (warmup_start, event_end, event_start, event_end) (1, 4, 2, 4)</p> Source code in <code>hydroutils\\hydro_event.py</code> <pre><code>def find_flood_event_segments_as_tuples(\n    flood_event_array: np.ndarray,\n    warmup_length: int = 0,\n) -&gt; List[Tuple[int, int, int, int]]:\n    \"\"\"Find continuous flood event segments and return them as tuples.\n\n    This is a convenience function that returns event segments as tuples instead\n    of dictionaries, for compatibility with existing code.\n\n    Args:\n        flood_event_array (np.ndarray): Binary array where values &gt; 0 indicate\n            flood events.\n        warmup_length (int, optional): Number of time steps to include as warmup\n            period before each event. Defaults to 0.\n\n    Returns:\n        List[Tuple[int, int, int, int]]: List of tuples, each containing:\n            (extended_start, extended_end, original_start, original_end)\n            where:\n            - extended_start: Start index including warmup period\n            - extended_end: End index of event\n            - original_start: Start index of actual event\n            - original_end: End index of actual event\n\n    Example:\n        &gt;&gt;&gt; arr = np.array([0, 0, 1, 1, 1, 0])\n        &gt;&gt;&gt; segments = find_flood_event_segments_as_tuples(arr, warmup_length=1)\n        &gt;&gt;&gt; segments[0]  # (warmup_start, event_end, event_start, event_end)\n        (1, 4, 2, 4)\n    \"\"\"\n    segments = find_flood_event_segments_from_array(flood_event_array, warmup_length)\n\n    return [\n        (\n            seg[\"extended_start\"],\n            seg[\"extended_end\"],\n            seg[\"original_start\"],\n            seg[\"original_end\"],\n        )\n        for seg in segments\n    ]\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.find_flood_event_segments_from_array","title":"<code>find_flood_event_segments_from_array(flood_event_array, warmup_length=0)</code>","text":"<p>Find continuous flood event segments in a binary indicator array.</p> <p>This is a low-level function that handles the core logic of segmenting a flood event indicator array into continuous events. It can be reused by different higher-level functions.</p> <p>Parameters:</p> Name Type Description Default <code>flood_event_array</code> <code>ndarray</code> <p>Binary array where values &gt; 0 indicate flood events.</p> required <code>warmup_length</code> <code>int</code> <p>Number of time steps to include as warmup period before each event. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>List[Dict]</code> <p>List[Dict]: List of event segment information. Each dictionary contains: - extended_start (int): Start index including warmup period - extended_end (int): End index of event - original_start (int): Start index of actual event - original_end (int): End index of actual event - duration (int): Duration of actual event in time steps - total_length (int): Total length including warmup period</p> Example <p>arr = np.array([0, 0, 1, 1, 1, 0, 0, 1, 1, 0]) segments = find_flood_event_segments_from_array(arr, warmup_length=1) len(segments)  # Two events found 2 segments[0]  # First event with one timestep warmup {     'extended_start': 1,  # Warmup start     'extended_end': 4,    # Event end     'original_start': 2,  # Actual event start     'original_end': 4,    # Actual event end     'duration': 3,        # Event duration     'total_length': 4     # Total length with warmup }</p> Source code in <code>hydroutils\\hydro_event.py</code> <pre><code>def find_flood_event_segments_from_array(\n    flood_event_array: np.ndarray,\n    warmup_length: int = 0,\n) -&gt; List[Dict]:\n    \"\"\"Find continuous flood event segments in a binary indicator array.\n\n    This is a low-level function that handles the core logic of segmenting a\n    flood event indicator array into continuous events. It can be reused by\n    different higher-level functions.\n\n    Args:\n        flood_event_array (np.ndarray): Binary array where values &gt; 0 indicate\n            flood events.\n        warmup_length (int, optional): Number of time steps to include as warmup\n            period before each event. Defaults to 0.\n\n    Returns:\n        List[Dict]: List of event segment information. Each dictionary contains:\n            - extended_start (int): Start index including warmup period\n            - extended_end (int): End index of event\n            - original_start (int): Start index of actual event\n            - original_end (int): End index of actual event\n            - duration (int): Duration of actual event in time steps\n            - total_length (int): Total length including warmup period\n\n    Example:\n        &gt;&gt;&gt; arr = np.array([0, 0, 1, 1, 1, 0, 0, 1, 1, 0])\n        &gt;&gt;&gt; segments = find_flood_event_segments_from_array(arr, warmup_length=1)\n        &gt;&gt;&gt; len(segments)  # Two events found\n        2\n        &gt;&gt;&gt; segments[0]  # First event with one timestep warmup\n        {\n            'extended_start': 1,  # Warmup start\n            'extended_end': 4,    # Event end\n            'original_start': 2,  # Actual event start\n            'original_end': 4,    # Actual event end\n            'duration': 3,        # Event duration\n            'total_length': 4     # Total length with warmup\n        }\n    \"\"\"\n    segments = []\n\n    # \u627e\u5230\u6240\u6709 flood_event &gt; 0 \u7684\u7d22\u5f15\n    event_indices = np.where(flood_event_array &gt; 0)[0]\n\n    if len(event_indices) == 0:\n        return segments\n\n    # \u627e\u5230\u8fde\u7eed\u6bb5\u7684\u5206\u5272\u70b9\n    gaps = np.diff(event_indices) &gt; 1\n    split_points = np.where(gaps)[0] + 1\n    split_indices = np.split(event_indices, split_points)\n\n    # \u4e3a\u6bcf\u4e2a\u8fde\u7eed\u6bb5\u751f\u6210\u4fe1\u606f\n    for indices in split_indices:\n        if len(indices) &gt; 0:\n            original_start = indices[0]\n            original_end = indices[-1]\n\n            # \u6dfb\u52a0\u9884\u70ed\u671f\n            extended_start = max(0, original_start - warmup_length)\n\n            segments.append(\n                {\n                    \"extended_start\": extended_start,\n                    \"extended_end\": original_end,\n                    \"original_start\": original_start,\n                    \"original_end\": original_end,\n                    \"duration\": original_end - original_start + 1,\n                    \"total_length\": original_end - extended_start + 1,\n                }\n            )\n\n    return segments\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.flood_peak_error","title":"<code>flood_peak_error(Q_obs, Q_sim)</code>","text":"<p>Calculate relative flood peak error.</p>"},{"location":"api/hydroutils/#hydroutils.flood_peak_error--parameters","title":"Parameters","text":"<p>Q_obs : array-like     Observed streamflow. Q_sim : array-like     Simulated streamflow.</p>"},{"location":"api/hydroutils/#hydroutils.flood_peak_error--returns","title":"Returns","text":"<p>float     Relative flood peak error (%).</p> Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def flood_peak_error(Q_obs, Q_sim):\n    \"\"\"\n    Calculate relative flood peak error.\n\n    Parameters\n    ----------\n    Q_obs : array-like\n        Observed streamflow.\n    Q_sim : array-like\n        Simulated streamflow.\n\n    Returns\n    -------\n    float\n        Relative flood peak error (%).\n    \"\"\"\n    peak_obs = np.max(Q_obs)\n    peak_sim = np.max(Q_sim)\n\n    if peak_obs &gt; 1e-6:\n        return ((peak_sim - peak_obs) / peak_obs) * 100.0\n    else:\n        return np.nan\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.flood_peak_timing","title":"<code>flood_peak_timing(obs, sim, window=None, resolution='1D', datetime_coord=None)</code>","text":"<p>Calculate mean difference in peak flow timing (simplified version for numpy arrays).</p> <p>Uses scipy.find_peaks to find peaks in the observed time series. Starting with all observed peaks, those with a prominence of less than the standard deviation of the observed time series are discarded. Next, the lowest peaks are subsequently discarded until all remaining peaks have a distance of at least 100 steps. Finally, the corresponding peaks in the simulated time series are searched in a window of size <code>window</code> on either side of the observed peaks and the absolute time differences between observed and simulated peaks is calculated. The final metric is the mean absolute time difference across all peaks (in time steps).</p>"},{"location":"api/hydroutils/#hydroutils.flood_peak_timing--parameters","title":"Parameters","text":"<p>obs : np.ndarray     Observed time series. sim : np.ndarray     Simulated time series. window : int, optional     Size of window to consider on each side of the observed peak for finding the simulated peak. That is, the total     window length to find the peak in the simulations is 2 * window + 1 centered at the observed     peak. The default depends on the temporal resolution, e.g. for a resolution of '1D', a window of 3 is used and     for a resolution of '1H' the window size is 12. resolution : str, optional     Temporal resolution of the time series in pandas format, e.g. '1D' for daily and '1H' for hourly.     Currently used only for determining default window size. datetime_coord : str, optional     Name of datetime coordinate. Currently unused in this simplified implementation.</p>"},{"location":"api/hydroutils/#hydroutils.flood_peak_timing--returns","title":"Returns","text":"<p>float     Mean peak time difference in time steps. Returns NaN if no peaks are found.</p>"},{"location":"api/hydroutils/#hydroutils.flood_peak_timing--references","title":"References","text":"<p>.. [#] Kratzert, F., Klotz, D., Hochreiter, S., and Nearing, G. S.: A note on leveraging synergy in multiple     meteorological datasets with deep learning for rainfall-runoff modeling, Hydrol. Earth Syst. Sci.,     https://doi.org/10.5194/hess-2020-221</p> Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def flood_peak_timing(\n    obs: np.ndarray,\n    sim: np.ndarray,\n    window: Optional[int] = None,\n    resolution: str = \"1D\",\n    datetime_coord: Optional[str] = None,\n) -&gt; float:\n    \"\"\"\n    Calculate mean difference in peak flow timing (simplified version for numpy arrays).\n\n    Uses scipy.find_peaks to find peaks in the observed time series. Starting with all observed peaks, those with a\n    prominence of less than the standard deviation of the observed time series are discarded. Next, the lowest peaks\n    are subsequently discarded until all remaining peaks have a distance of at least 100 steps. Finally, the\n    corresponding peaks in the simulated time series are searched in a window of size `window` on either side of the\n    observed peaks and the absolute time differences between observed and simulated peaks is calculated.\n    The final metric is the mean absolute time difference across all peaks (in time steps).\n\n    Parameters\n    ----------\n    obs : np.ndarray\n        Observed time series.\n    sim : np.ndarray\n        Simulated time series.\n    window : int, optional\n        Size of window to consider on each side of the observed peak for finding the simulated peak. That is, the total\n        window length to find the peak in the simulations is 2 * window + 1 centered at the observed\n        peak. The default depends on the temporal resolution, e.g. for a resolution of '1D', a window of 3 is used and\n        for a resolution of '1H' the window size is 12.\n    resolution : str, optional\n        Temporal resolution of the time series in pandas format, e.g. '1D' for daily and '1H' for hourly.\n        Currently used only for determining default window size.\n    datetime_coord : str, optional\n        Name of datetime coordinate. Currently unused in this simplified implementation.\n\n    Returns\n    -------\n    float\n        Mean peak time difference in time steps. Returns NaN if no peaks are found.\n\n    References\n    ----------\n    .. [#] Kratzert, F., Klotz, D., Hochreiter, S., and Nearing, G. S.: A note on leveraging synergy in multiple\n        meteorological datasets with deep learning for rainfall-runoff modeling, Hydrol. Earth Syst. Sci.,\n        https://doi.org/10.5194/hess-2020-221\n    \"\"\"\n    # verify inputs\n    _validate_inputs(obs, sim)\n\n    # get time series with only valid observations (scipy's find_peaks doesn't guarantee correctness with NaNs)\n    obs_clean, sim_clean = _mask_valid(obs, sim)\n\n    if len(obs_clean) &lt; 3:\n        return np.nan\n\n    # determine default window size based on resolution\n    if window is None:\n        # infer a reasonable window size based on resolution\n        window = max(int(_get_frequency_factor(\"12H\", resolution)), 3)\n\n    # heuristic to get indices of peaks and their corresponding height.\n    # Use prominence based on standard deviation to filter significant peaks\n    prominence_threshold = np.std(obs_clean)\n    if prominence_threshold == 0:  # Handle constant time series\n        prominence_threshold = (\n            0.01 * np.mean(obs_clean) if np.mean(obs_clean) != 0 else 0.01\n        )\n\n    peaks, _ = signal.find_peaks(\n        obs_clean, distance=100, prominence=prominence_threshold\n    )\n\n    if len(peaks) == 0:\n        return np.nan\n\n    # evaluate timing\n    timing_errors = []\n    for idx in peaks:\n        # skip peaks at the start and end of the sequence\n        if (idx - window &lt; 0) or (idx + window &gt;= len(obs_clean)):\n            continue\n\n        # find the corresponding peak in simulated data within the window\n        window_start = max(0, idx - window)\n        window_end = min(len(sim_clean), idx + window + 1)\n        sim_window = sim_clean[window_start:window_end]\n\n        # find the index of maximum value in the window\n        local_peak_idx = np.argmax(sim_window)\n        global_peak_idx = window_start + local_peak_idx\n\n        # calculate the time difference between the peaks (in time steps)\n        timing_error = abs(idx - global_peak_idx)\n        timing_errors.append(timing_error)\n\n    return np.mean(timing_errors) if timing_errors else np.nan\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.flood_volume_error","title":"<code>flood_volume_error(Q_obs, Q_sim, delta_t_seconds=10800)</code>","text":"<p>Calculate relative flood volume error.</p>"},{"location":"api/hydroutils/#hydroutils.flood_volume_error--parameters","title":"Parameters","text":"<p>Q_obs : array-like     Observed streamflow. Q_sim : array-like     Simulated streamflow. delta_t_seconds : int, optional     Time step in seconds, by default 10800 (3 hours).</p>"},{"location":"api/hydroutils/#hydroutils.flood_volume_error--returns","title":"Returns","text":"<p>float     Relative flood volume error (%).</p> Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def flood_volume_error(Q_obs, Q_sim, delta_t_seconds=10800):\n    \"\"\"\n    Calculate relative flood volume error.\n\n    Parameters\n    ----------\n    Q_obs : array-like\n        Observed streamflow.\n    Q_sim : array-like\n        Simulated streamflow.\n    delta_t_seconds : int, optional\n        Time step in seconds, by default 10800 (3 hours).\n\n    Returns\n    -------\n    float\n        Relative flood volume error (%).\n    \"\"\"\n    vol_obs = np.sum(Q_obs) * delta_t_seconds\n    vol_sim = np.sum(Q_sim) * delta_t_seconds\n\n    if vol_obs &gt; 1e-6:\n        return ((vol_sim - vol_obs) / vol_obs) * 100.0\n    else:\n        return np.nan\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.fms","title":"<code>fms(obs, sim, lower=0.2, upper=0.7)</code>","text":"<p>TODO: not fully tested Calculate the slope of the middle section of the flow duration curve [#]_</p> <p>.. math::     \\%\\text{BiasFMS} = \\frac{\\left | \\log(Q_{s,\\text{lower}}) - \\log(Q_{s,\\text{upper}}) \\right | -         \\left | \\log(Q_{o,\\text{lower}}) - \\log(Q_{o,\\text{upper}}) \\right |}{\\left |         \\log(Q_{s,\\text{lower}}) - \\log(Q_{s,\\text{upper}}) \\right |} \\times 100,</p> <p>where :math:<code>Q_{s,\\text{lower/upper}}</code> corresponds to the FDC of the simulations (here, <code>sim</code>) at the <code>lower</code> and <code>upper</code> bound of the middle section and :math:<code>Q_{o,\\text{lower/upper}}</code> similarly for the observations (here, <code>obs</code>).</p>"},{"location":"api/hydroutils/#hydroutils.fms--parameters","title":"Parameters","text":"<p>obs : DataArray     Observed time series. sim : DataArray     Simulated time series. lower : float, optional     Lower bound of the middle section in range ]0,1[, by default 0.2 upper : float, optional     Upper bound of the middle section in range ]0,1[, by default 0.7</p>"},{"location":"api/hydroutils/#hydroutils.fms--returns","title":"Returns","text":"<p>float     Slope of the middle section of the flow duration curve.</p>"},{"location":"api/hydroutils/#hydroutils.fms--references","title":"References","text":"<p>.. [#] Yilmaz, K. K., Gupta, H. V., and Wagener, T. ( 2008), A process-based diagnostic approach to model     evaluation: Application to the NWS distributed hydrologic model, Water Resour. Res., 44, W09417,     doi:10.1029/2007WR006716.</p> Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def fms(\n    obs: np.ndarray, sim: np.ndarray, lower: float = 0.2, upper: float = 0.7\n) -&gt; float:\n    r\"\"\"\n    TODO: not fully tested\n    Calculate the slope of the middle section of the flow duration curve [#]_\n\n    .. math::\n        \\%\\text{BiasFMS} = \\frac{\\left | \\log(Q_{s,\\text{lower}}) - \\log(Q_{s,\\text{upper}}) \\right | -\n            \\left | \\log(Q_{o,\\text{lower}}) - \\log(Q_{o,\\text{upper}}) \\right |}{\\left |\n            \\log(Q_{s,\\text{lower}}) - \\log(Q_{s,\\text{upper}}) \\right |} \\times 100,\n\n    where :math:`Q_{s,\\text{lower/upper}}` corresponds to the FDC of the simulations (here, `sim`) at the `lower` and\n    `upper` bound of the middle section and :math:`Q_{o,\\text{lower/upper}}` similarly for the observations (here,\n    `obs`).\n\n    Parameters\n    ----------\n    obs : DataArray\n        Observed time series.\n    sim : DataArray\n        Simulated time series.\n    lower : float, optional\n        Lower bound of the middle section in range ]0,1[, by default 0.2\n    upper : float, optional\n        Upper bound of the middle section in range ]0,1[, by default 0.7\n\n    Returns\n    -------\n    float\n        Slope of the middle section of the flow duration curve.\n\n    References\n    ----------\n    .. [#] Yilmaz, K. K., Gupta, H. V., and Wagener, T. ( 2008), A process-based diagnostic approach to model\n        evaluation: Application to the NWS distributed hydrologic model, Water Resour. Res., 44, W09417,\n        doi:10.1029/2007WR006716.\n    \"\"\"\n    if len(obs) &lt; 1:\n        return np.nan\n\n    if any((x &lt;= 0) or (x &gt;= 1) for x in [upper, lower]):\n        raise ValueError(\"upper and lower have to be in range ]0,1[\")\n\n    if lower &gt;= upper:\n        raise ValueError(\"The lower threshold has to be smaller than the upper.\")\n\n    # get arrays of sorted (descending) discharges\n    obs = np.sort(obs)\n    sim = np.sort(sim)\n\n    # for numerical reasons change 0s to 1e-6. Simulations can still contain negatives, so also reset those.\n    sim[sim &lt;= 0] = 1e-6\n    obs[obs == 0] = 1e-6\n\n    # calculate fms part by part\n    qsm_lower = np.log(sim[np.round(lower * len(sim)).astype(int)])\n    qsm_upper = np.log(sim[np.round(upper * len(sim)).astype(int)])\n    qom_lower = np.log(obs[np.round(lower * len(obs)).astype(int)])\n    qom_upper = np.log(obs[np.round(upper * len(obs)).astype(int)])\n\n    fms = ((qsm_lower - qsm_upper) - (qom_lower - qom_upper)) / (\n        qom_lower - qom_upper + 1e-6\n    )\n\n    return fms * 100\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.generate_start0101_time_range","title":"<code>generate_start0101_time_range(start_time, end_time, freq='8D')</code>","text":"<p>Generate a time range with annual reset to January 1st.</p> <p>This function creates a time range with a specified frequency, but with the special behavior that each year starts from January 1st regardless of the frequency interval. This is particularly useful for creating time series that need to align with calendar years while maintaining a regular interval pattern within each year.</p> <p>Parameters:</p> Name Type Description Default <code>start_time</code> <code>Union[str, Timestamp]</code> <p>Start date of the range. Can be string ('YYYY-MM-DD') or pandas Timestamp.</p> required <code>end_time</code> <code>Union[str, Timestamp]</code> <p>End date of the range. Can be string ('YYYY-MM-DD') or pandas Timestamp.</p> required <code>freq</code> <code>str</code> <p>Time frequency for intervals. Defaults to '8D'. Common values: '7D' (weekly), '10D' (dekadal), etc.</p> <code>'8D'</code> <p>Returns:</p> Type Description <p>pd.DatetimeIndex: Time range with specified frequency and annual reset.</p> Example <p>generate_start0101_time_range('2020-03-15', '2021-02-15', freq='10D') DatetimeIndex(['2020-03-15', '2020-03-25', '2020-04-04', ...,               '2021-01-01', '2021-01-11', '2021-02-11'],               dtype='datetime64[ns]', freq=None)</p> Note <ul> <li>If an interval would cross into a new year, it's truncated and the next   interval starts from January 1st of the new year.</li> <li>The frequency must be a valid pandas frequency string that represents   a fixed duration.</li> </ul> Source code in <code>hydroutils\\hydro_time.py</code> <pre><code>def generate_start0101_time_range(start_time, end_time, freq=\"8D\"):\n    \"\"\"Generate a time range with annual reset to January 1st.\n\n    This function creates a time range with a specified frequency, but with the special\n    behavior that each year starts from January 1st regardless of the frequency interval.\n    This is particularly useful for creating time series that need to align with\n    calendar years while maintaining a regular interval pattern within each year.\n\n    Args:\n        start_time (Union[str, pd.Timestamp]): Start date of the range.\n            Can be string ('YYYY-MM-DD') or pandas Timestamp.\n        end_time (Union[str, pd.Timestamp]): End date of the range.\n            Can be string ('YYYY-MM-DD') or pandas Timestamp.\n        freq (str, optional): Time frequency for intervals. Defaults to '8D'.\n            Common values: '7D' (weekly), '10D' (dekadal), etc.\n\n    Returns:\n        pd.DatetimeIndex: Time range with specified frequency and annual reset.\n\n    Example:\n        &gt;&gt;&gt; generate_start0101_time_range('2020-03-15', '2021-02-15', freq='10D')\n        DatetimeIndex(['2020-03-15', '2020-03-25', '2020-04-04', ...,\n                      '2021-01-01', '2021-01-11', '2021-02-11'],\n                      dtype='datetime64[ns]', freq=None)\n\n    Note:\n        - If an interval would cross into a new year, it's truncated and the next\n          interval starts from January 1st of the new year.\n        - The frequency must be a valid pandas frequency string that represents\n          a fixed duration.\n    \"\"\"\n    all_dates = []\n\n    # Ensure the start and end times are of type pd.Timestamp\n    current_time = pd.Timestamp(start_time)\n    end_time = pd.Timestamp(end_time)\n\n    # Parse the frequency interval correctly\n    interval_days = pd.Timedelta(freq)  # Ensure it's a Timedelta\n\n    while current_time &lt;= end_time:\n        all_dates.append(current_time)\n\n        # Calculate next date with the specified interval\n        next_time = current_time + interval_days\n\n        # If next_time crosses into a new year, reset to 01-01 of the new year\n        if next_time.year &gt; current_time.year:\n            next_time = pd.Timestamp(f\"{next_time.year}-01-01\")\n\n        current_time = next_time\n\n    return pd.to_datetime(all_dates)\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.get_cache_dir","title":"<code>get_cache_dir(app_name='hydro')</code>","text":"<p>Get the appropriate cache directory for the current operating system.</p> <p>Parameters:</p> Name Type Description Default <code>app_name</code> <code>str</code> <p>Name of the application. Defaults to \"hydro\".</p> <code>'hydro'</code> <p>Returns:</p> Name Type Description <code>str</code> <p>Path to the cache directory.</p> Note <p>Creates the directory if it doesn't exist. Follows OS-specific conventions: - Windows: %LOCALAPPDATA%/app_name/Cache - macOS: ~/Library/Caches/app_name - Linux: ~/.cache/app_name</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def get_cache_dir(app_name=\"hydro\"):\n    \"\"\"Get the appropriate cache directory for the current operating system.\n\n    Args:\n        app_name (str, optional): Name of the application. Defaults to \"hydro\".\n\n    Returns:\n        str: Path to the cache directory.\n\n    Note:\n        Creates the directory if it doesn't exist.\n        Follows OS-specific conventions:\n        - Windows: %LOCALAPPDATA%/app_name/Cache\n        - macOS: ~/Library/Caches/app_name\n        - Linux: ~/.cache/app_name\n    \"\"\"\n    home = os.path.expanduser(\"~\")\n    system = platform.system()\n\n    if system == \"Windows\":\n        cache_dir = os.path.join(home, \"AppData\", \"Local\", app_name, \"Cache\")\n    elif system == \"Darwin\":\n        cache_dir = os.path.join(home, \"Library\", \"Caches\", app_name)\n    else:\n        cache_dir = os.path.join(home, \".cache\", app_name)\n\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    return cache_dir\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.get_event_indices","title":"<code>get_event_indices(df, warmup_length=0, flood_event_col='flood_event')</code>","text":"<p>Get index information for flood events without extracting data.</p> <p>This function identifies flood events in the DataFrame and returns their index information, but does not extract the actual data. This is useful when you only need to know the locations and durations of events.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing site data.</p> required <code>warmup_length</code> <code>int</code> <p>Number of time steps to include as warmup period before each event. Defaults to 0.</p> <code>0</code> <code>flood_event_col</code> <code>str</code> <p>Name of flood event indicator column. Defaults to \"flood_event\".</p> <code>'flood_event'</code> <p>Returns:</p> Type Description <code>List[Dict]</code> <p>List[Dict]: List of event index information. Each dictionary contains: - start_idx (int): Start index of actual event - end_idx (int): End index of actual event - warmup_start_idx (int): Start index including warmup period - duration (int): Duration of actual event in time steps - total_length (int): Total length including warmup period</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If flood_event_col is not found in DataFrame.</p> Example <p>df = pd.DataFrame({'flood_event': [0, 1, 1, 1, 0]}) indices = get_event_indices(df, warmup_length=1) indices[0] {     'start_idx': 1,     'end_idx': 4,     'warmup_start_idx': 0,     'duration': 3,     'total_length': 4 }</p> Source code in <code>hydroutils\\hydro_event.py</code> <pre><code>def get_event_indices(\n    df: pd.DataFrame, warmup_length: int = 0, flood_event_col: str = \"flood_event\"\n) -&gt; List[Dict]:\n    \"\"\"Get index information for flood events without extracting data.\n\n    This function identifies flood events in the DataFrame and returns their index\n    information, but does not extract the actual data. This is useful when you only\n    need to know the locations and durations of events.\n\n    Args:\n        df (pd.DataFrame): DataFrame containing site data.\n        warmup_length (int, optional): Number of time steps to include as warmup\n            period before each event. Defaults to 0.\n        flood_event_col (str, optional): Name of flood event indicator column.\n            Defaults to \"flood_event\".\n\n    Returns:\n        List[Dict]: List of event index information. Each dictionary contains:\n            - start_idx (int): Start index of actual event\n            - end_idx (int): End index of actual event\n            - warmup_start_idx (int): Start index including warmup period\n            - duration (int): Duration of actual event in time steps\n            - total_length (int): Total length including warmup period\n\n    Raises:\n        ValueError: If flood_event_col is not found in DataFrame.\n\n    Example:\n        &gt;&gt;&gt; df = pd.DataFrame({'flood_event': [0, 1, 1, 1, 0]})\n        &gt;&gt;&gt; indices = get_event_indices(df, warmup_length=1)\n        &gt;&gt;&gt; indices[0]\n        {\n            'start_idx': 1,\n            'end_idx': 4,\n            'warmup_start_idx': 0,\n            'duration': 3,\n            'total_length': 4\n        }\n    \"\"\"\n    # \u68c0\u67e5\u5fc5\u8981\u7684\u5217\u662f\u5426\u5b58\u5728\n    if flood_event_col not in df.columns:\n        raise ValueError(f\"DataFrame\u7f3a\u5c11\u6d2a\u6c34\u4e8b\u4ef6\u6807\u8bb0\u5217: {flood_event_col}\")\n\n    # \u4f7f\u7528\u5e95\u5c42\u51fd\u6570\u5904\u7406\u5206\u5272\u903b\u8f91\n    flood_event_array = df[flood_event_col].values\n    segments = find_flood_event_segments_from_array(flood_event_array, warmup_length)\n\n    # \u8f6c\u6362\u4e3a\u4e0e\u539f\u63a5\u53e3\u517c\u5bb9\u7684\u683c\u5f0f\n    events = []\n    for seg in segments:\n        events.append(\n            {\n                \"start_idx\": seg[\"original_start\"],\n                \"end_idx\": seg[\"original_end\"] + 1,  # +1 \u56e0\u4e3a\u539f\u6765\u662f\u4e0d\u5305\u542b\u7ed3\u675f\u7d22\u5f15\u7684\n                \"warmup_start_idx\": seg[\"extended_start\"],\n                \"duration\": seg[\"duration\"],\n                \"total_length\": seg[\"total_length\"],\n            }\n        )\n\n    return events\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.get_lastest_file_in_a_dir","title":"<code>get_lastest_file_in_a_dir(dir_path)</code>","text":"<p>Get the last file in a directory</p>"},{"location":"api/hydroutils/#hydroutils.get_lastest_file_in_a_dir--parameters","title":"Parameters","text":"<p>dir_path : str     the directory</p>"},{"location":"api/hydroutils/#hydroutils.get_lastest_file_in_a_dir--returns","title":"Returns","text":"<p>str     the path of the weight file</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def get_lastest_file_in_a_dir(dir_path):\n    \"\"\"Get the last file in a directory\n\n    Parameters\n    ----------\n    dir_path : str\n        the directory\n\n    Returns\n    -------\n    str\n        the path of the weight file\n    \"\"\"\n    pth_files_lst = [\n        os.path.join(dir_path, file)\n        for file in os.listdir(dir_path)\n        if fnmatch.fnmatch(file, \"*.pth\")\n    ]\n    return get_latest_file_in_a_lst(pth_files_lst)\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.get_latest_file_in_a_lst","title":"<code>get_latest_file_in_a_lst(lst)</code>","text":"<p>Get the most recently modified file from a list of files.</p> <p>Parameters:</p> Name Type Description Default <code>lst</code> <code>list</code> <p>List of file paths.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>Path of the most recently modified file.</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def get_latest_file_in_a_lst(lst):\n    \"\"\"Get the most recently modified file from a list of files.\n\n    Args:\n        lst (list): List of file paths.\n\n    Returns:\n        str: Path of the most recently modified file.\n    \"\"\"\n    lst_ctime = [os.path.getctime(file) for file in lst]\n    sort_idx = np.argsort(lst_ctime)\n    return lst[sort_idx[-1]]\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.get_year","title":"<code>get_year(a_time)</code>","text":"<p>Extract year from various time formats.</p> <p>Parameters:</p> Name Type Description Default <code>a_time</code> <code>Union[date, datetime64, str]</code> <p>Time in various formats.</p> required <p>Returns:</p> Name Type Description <code>int</code> <p>Year value.</p> Note <p>Supports datetime.date, numpy.datetime64, and string formats. For strings, assumes YYYY is at the start.</p> Source code in <code>hydroutils\\hydro_time.py</code> <pre><code>def get_year(a_time):\n    \"\"\"Extract year from various time formats.\n\n    Args:\n        a_time (Union[datetime.date, np.datetime64, str]): Time in various formats.\n\n    Returns:\n        int: Year value.\n\n    Note:\n        Supports datetime.date, numpy.datetime64, and string formats.\n        For strings, assumes YYYY is at the start.\n    \"\"\"\n    if isinstance(a_time, datetime.date):\n        return a_time.year\n    elif isinstance(a_time, np.datetime64):\n        return a_time.astype(\"datetime64[Y]\").astype(int) + 1970\n    else:\n        return int(a_time[:4])\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.hydro_logger","title":"<code>hydro_logger(cls)</code>","text":"<p>Class decorator that adds a configured logger to the decorated class.</p> <p>This decorator sets up a logger with both file and console handlers. The file handler writes all logs (DEBUG and above) to a timestamped file in the cache directory, while the console handler shows INFO and above messages.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <p>The class to be decorated.</p> required <p>Returns:</p> Type Description <p>The decorated class with an added logger attribute.</p> Example <p>@hydro_logger class MyClass:     def my_method(self):         self.logger.info(\"This will be logged\")</p> Source code in <code>hydroutils\\hydro_log.py</code> <pre><code>def hydro_logger(cls):\n    \"\"\"Class decorator that adds a configured logger to the decorated class.\n\n    This decorator sets up a logger with both file and console handlers. The file\n    handler writes all logs (DEBUG and above) to a timestamped file in the cache\n    directory, while the console handler shows INFO and above messages.\n\n    Args:\n        cls: The class to be decorated.\n\n    Returns:\n        The decorated class with an added logger attribute.\n\n    Example:\n        @hydro_logger\n        class MyClass:\n            def my_method(self):\n                self.logger.info(\"This will be logged\")\n    \"\"\"\n    # Use the class name as the logger name\n    logger_name = f\"{cls.__module__}.{cls.__name__}\"\n    logger = logging.getLogger(logger_name)\n    logger.setLevel(logging.DEBUG)\n    cache_dir = get_cache_dir()\n    log_dir = os.path.join(cache_dir, \"logs\")\n    if not os.path.exists(log_dir):\n        os.makedirs(log_dir)\n    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    log_file = os.path.join(log_dir, f\"{logger_name}_{current_time}.log\")\n    # Check if handlers have already been added to avoid duplication\n    if not logger.handlers:\n        # Create a file handler to write logs to the specified file\n        file_handler = logging.FileHandler(log_file)\n        file_handler.setLevel(logging.DEBUG)\n\n        # Create a console handler to output logs to the console (optional)\n        console_handler = logging.StreamHandler()\n        console_handler.setLevel(logging.INFO)\n\n        # set the format of the log\n        formatter = logging.Formatter(\n            \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n        )\n        file_handler.setFormatter(formatter)\n        console_handler.setFormatter(formatter)\n\n        # Add handlers to the logger\n        logger.addHandler(file_handler)\n        logger.addHandler(console_handler)\n\n    # Bind the logger to the class attribute\n    cls.logger = logger\n    return cls\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.intersect","title":"<code>intersect(t_lst1, t_lst2)</code>","text":"<p>Find indices of common elements between two time lists.</p> <p>Parameters:</p> Name Type Description Default <code>t_lst1</code> <code>array - like</code> <p>First time array.</p> required <code>t_lst2</code> <code>array - like</code> <p>Second time array.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>(ind1, ind2) where ind1 and ind2 are indices of common elements</p> <p>in t_lst1 and t_lst2 respectively.</p> Source code in <code>hydroutils\\hydro_time.py</code> <pre><code>def intersect(t_lst1, t_lst2):\n    \"\"\"Find indices of common elements between two time lists.\n\n    Args:\n        t_lst1 (array-like): First time array.\n        t_lst2 (array-like): Second time array.\n\n    Returns:\n        tuple: (ind1, ind2) where ind1 and ind2 are indices of common elements\n        in t_lst1 and t_lst2 respectively.\n    \"\"\"\n    C, ind1, ind2 = np.intersect1d(t_lst1, t_lst2, return_indices=True)\n    return ind1, ind2\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.is_there_file","title":"<code>is_there_file(zipfile_path, unzip_dir)</code>","text":"<p>if a file has existed</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def is_there_file(zipfile_path, unzip_dir):\n    \"\"\"if a file has existed\"\"\"\n    if os.path.isfile(zipfile_path):\n        if os.path.isdir(unzip_dir):\n            return True\n        unzip_nested_zip(zipfile_path, unzip_dir)\n        return True\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.minio_download_file","title":"<code>minio_download_file(client, bucket_name, object_name, file_path, version_id=None)</code>","text":"<p>Download a file from MinIO S3-compatible storage.</p> <p>This function downloads an object from MinIO storage to a local file. It supports versioned objects and handles UTF-8 encoded text files. The function ensures proper cleanup of resources after download.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Minio</code> <p>Initialized MinIO client instance.</p> required <code>bucket_name</code> <code>str</code> <p>Name of the bucket containing the object.</p> required <code>object_name</code> <code>str</code> <p>Name of the object to download.</p> required <code>file_path</code> <code>str</code> <p>Local path where the file should be saved.</p> required <code>version_id</code> <code>str</code> <p>Version ID for versioned objects. Defaults to None.</p> <code>None</code> Note <ul> <li>Assumes UTF-8 encoding for text files</li> <li>Properly closes and releases connection after download</li> <li>Uses context managers for file handling</li> <li>Handles cleanup in finally block for robustness</li> </ul> Example <p>client = Minio('play.min.io', ...               access_key='access_key', ...               secret_key='secret_key') minio_download_file(client, ...                    'mybucket', ...                    'data/file.csv', ...                    '/local/path/file.csv')</p> Source code in <code>hydroutils\\hydro_s3.py</code> <pre><code>def minio_download_file(\n    client: Minio, bucket_name, object_name, file_path: str, version_id=None\n):\n    \"\"\"Download a file from MinIO S3-compatible storage.\n\n    This function downloads an object from MinIO storage to a local file. It\n    supports versioned objects and handles UTF-8 encoded text files. The function\n    ensures proper cleanup of resources after download.\n\n    Args:\n        client (Minio): Initialized MinIO client instance.\n        bucket_name (str): Name of the bucket containing the object.\n        object_name (str): Name of the object to download.\n        file_path (str): Local path where the file should be saved.\n        version_id (str, optional): Version ID for versioned objects.\n            Defaults to None.\n\n    Note:\n        - Assumes UTF-8 encoding for text files\n        - Properly closes and releases connection after download\n        - Uses context managers for file handling\n        - Handles cleanup in finally block for robustness\n\n    Example:\n        &gt;&gt;&gt; client = Minio('play.min.io',\n        ...               access_key='access_key',\n        ...               secret_key='secret_key')\n        &gt;&gt;&gt; minio_download_file(client,\n        ...                    'mybucket',\n        ...                    'data/file.csv',\n        ...                    '/local/path/file.csv')\n    \"\"\"\n    try:\n        response = client.get_object(bucket_name, object_name, version_id)\n        res_csv: str = response.data.decode(\"utf8\")\n        with open(file_path, \"w+\") as fp:\n            fp.write(res_csv)\n    finally:\n        response.close()\n        response.release_conn()\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.minio_upload_file","title":"<code>minio_upload_file(client, bucket_name, object_name, file_path)</code>","text":"<p>Upload a file to MinIO S3-compatible storage.</p> <p>This function uploads a local file to MinIO storage. If the specified bucket doesn't exist, it will be created automatically. After upload, it returns a list of all objects in the bucket.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Minio</code> <p>Initialized MinIO client instance.</p> required <code>bucket_name</code> <code>str</code> <p>Name of the bucket to upload to.</p> required <code>object_name</code> <code>str</code> <p>Name to give the object in MinIO storage.</p> required <code>file_path</code> <code>str</code> <p>Path to the local file to upload.</p> required <p>Returns:</p> Type Description <p>list[str]: List of all object names in the bucket after upload.</p> Note <ul> <li>Creates bucket if it doesn't exist</li> <li>Uses fput_object for efficient file upload</li> <li>Lists all objects recursively after upload</li> </ul> Example <p>client = Minio('play.min.io', ...               access_key='access_key', ...               secret_key='secret_key') objects = minio_upload_file(client, ...                            'mybucket', ...                            'data/file.csv', ...                            '/local/path/file.csv') print(objects) ['data/file.csv', 'data/other.csv']</p> Source code in <code>hydroutils\\hydro_s3.py</code> <pre><code>def minio_upload_file(client, bucket_name, object_name, file_path):\n    \"\"\"Upload a file to MinIO S3-compatible storage.\n\n    This function uploads a local file to MinIO storage. If the specified bucket\n    doesn't exist, it will be created automatically. After upload, it returns a\n    list of all objects in the bucket.\n\n    Args:\n        client (minio.Minio): Initialized MinIO client instance.\n        bucket_name (str): Name of the bucket to upload to.\n        object_name (str): Name to give the object in MinIO storage.\n        file_path (str): Path to the local file to upload.\n\n    Returns:\n        list[str]: List of all object names in the bucket after upload.\n\n    Note:\n        - Creates bucket if it doesn't exist\n        - Uses fput_object for efficient file upload\n        - Lists all objects recursively after upload\n\n    Example:\n        &gt;&gt;&gt; client = Minio('play.min.io',\n        ...               access_key='access_key',\n        ...               secret_key='secret_key')\n        &gt;&gt;&gt; objects = minio_upload_file(client,\n        ...                            'mybucket',\n        ...                            'data/file.csv',\n        ...                            '/local/path/file.csv')\n        &gt;&gt;&gt; print(objects)\n        ['data/file.csv', 'data/other.csv']\n    \"\"\"\n    # Make a bucket\n    bucket_names = [bucket.name for bucket in client.list_buckets()]\n    if bucket_name not in bucket_names:\n        client.make_bucket(bucket_name)\n    # Upload an object\n    client.fput_object(bucket_name, object_name, file_path)\n    # List objects\n    objects = client.list_objects(bucket_name, recursive=True)\n    return [obj.object_name for obj in objects]\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.month_stat_for_daily_df","title":"<code>month_stat_for_daily_df(df)</code>","text":"<p>Calculate monthly statistics from daily data.</p> <p>This function resamples daily data to monthly frequency by computing the mean value for each month. It ensures the input DataFrame has a datetime index before resampling.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing daily data with datetime index or index that can be converted to datetime.</p> required <p>Returns:</p> Type Description <p>pd.DataFrame: DataFrame containing monthly statistics (means). Index is the start of each month.</p> Note <ul> <li>Uses pandas resample with 'MS' (month start) frequency</li> <li>Automatically converts index to datetime if needed</li> <li>Computes mean value for each month</li> <li>Handles missing values according to pandas defaults</li> </ul> Example <p>dates = pd.date_range('2020-01-01', '2020-12-31', freq='D') data = pd.DataFrame({'value': range(366)}, index=dates) monthly = month_stat_for_daily_df(data) print(monthly.head())                value 2020-01-01  15.0 2020-02-01  45.5 2020-03-01  74.0 2020-04-01  105.0 2020-05-01  135.5</p> Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def month_stat_for_daily_df(df):\n    \"\"\"Calculate monthly statistics from daily data.\n\n    This function resamples daily data to monthly frequency by computing the\n    mean value for each month. It ensures the input DataFrame has a datetime\n    index before resampling.\n\n    Args:\n        df (pd.DataFrame): DataFrame containing daily data with datetime index\n            or index that can be converted to datetime.\n\n    Returns:\n        pd.DataFrame: DataFrame containing monthly statistics (means).\n            Index is the start of each month.\n\n    Note:\n        - Uses pandas resample with 'MS' (month start) frequency\n        - Automatically converts index to datetime if needed\n        - Computes mean value for each month\n        - Handles missing values according to pandas defaults\n\n    Example:\n        &gt;&gt;&gt; dates = pd.date_range('2020-01-01', '2020-12-31', freq='D')\n        &gt;&gt;&gt; data = pd.DataFrame({'value': range(366)}, index=dates)\n        &gt;&gt;&gt; monthly = month_stat_for_daily_df(data)\n        &gt;&gt;&gt; print(monthly.head())\n                       value\n        2020-01-01  15.0\n        2020-02-01  45.5\n        2020-03-01  74.0\n        2020-04-01  105.0\n        2020-05-01  135.5\n    \"\"\"\n    # guarantee the index is datetime\n    df.index = pd.to_datetime(df.index)\n    return df.resample(\"MS\").mean()\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.pbias","title":"<code>pbias(observed, simulated)</code>","text":"<p>Calculate Percent Bias (PBIAS)</p>"},{"location":"api/hydroutils/#hydroutils.pbias--parameters","title":"Parameters","text":"<p>observed : array-like     Observed values simulated : array-like     Simulated values</p>"},{"location":"api/hydroutils/#hydroutils.pbias--returns","title":"Returns","text":"<p>float     Percent bias value</p> Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def pbias(\n    observed: Union[np.ndarray, List[float]], simulated: Union[np.ndarray, List[float]]\n) -&gt; float:\n    \"\"\"\n    Calculate Percent Bias (PBIAS)\n\n    Parameters\n    ----------\n    observed : array-like\n        Observed values\n    simulated : array-like\n        Simulated values\n\n    Returns\n    -------\n    float\n        Percent bias value\n    \"\"\"\n    observed = np.asarray(observed)\n    simulated = np.asarray(simulated)\n\n    # Remove NaN values\n    mask = ~(np.isnan(observed) | np.isnan(simulated))\n    observed = observed[mask]\n    simulated = simulated[mask]\n\n    if len(observed) == 0:\n        return np.nan\n\n    return np.sum(simulated - observed) / np.sum(observed) * 100\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.plot_boxes_matplotlib","title":"<code>plot_boxes_matplotlib(data, label1=None, label2=None, leg_col=None, colorlst='rbgcmywrbgcmyw', title=None, figsize=(8, 6), sharey=False, xticklabel=None, axin=None, ylim=None, ylabel=None, notch=False, widths=0.5, subplots_adjust_wspace=0.2, show_median=True, median_line_color='black', median_font_size='small')</code>","text":"<p>Create multiple boxplots for comparing multiple indicators.</p> <p>This function creates a figure with multiple boxplots, each representing a different indicator. It supports customization of appearance, labels, and layout, and can display median values and other statistics.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>list</code> <p>List of data arrays, each element represents one indicator and can contain multiple numpy arrays for box comparison.</p> required <code>label1</code> <code>list</code> <p>Names for each subplot. Defaults to None.</p> <code>None</code> <code>label2</code> <code>list</code> <p>Legend names for boxes within each subplot. Same across all subplots. Defaults to None.</p> <code>None</code> <code>leg_col</code> <code>int</code> <p>Number of columns in the legend. Defaults to None.</p> <code>None</code> <code>colorlst</code> <code>str</code> <p>String of color characters for boxes. Defaults to \"rbgcmywrbgcmyw\".</p> <code>'rbgcmywrbgcmyw'</code> <code>title</code> <code>str</code> <p>Figure title. Defaults to None.</p> <code>None</code> <code>figsize</code> <code>tuple</code> <p>Figure size as (width, height). Defaults to (8, 6).</p> <code>(8, 6)</code> <code>sharey</code> <code>bool</code> <p>If True, all subplots share y-axis scale. Defaults to False.</p> <code>False</code> <code>xticklabel</code> <code>list</code> <p>Labels for x-axis ticks. Defaults to None.</p> <code>None</code> <code>axin</code> <code>Axes</code> <p>Existing axes to plot on. Defaults to None.</p> <code>None</code> <code>ylim</code> <code>list</code> <p>Y-axis limits [min, max]. Defaults to None.</p> <code>None</code> <code>ylabel</code> <code>list</code> <p>Y-axis labels for each subplot. Defaults to None.</p> <code>None</code> <code>notch</code> <code>bool</code> <p>If True, boxes will have notches. Defaults to False.</p> <code>False</code> <code>widths</code> <code>float</code> <p>Width of the boxes. Defaults to 0.5.</p> <code>0.5</code> <code>subplots_adjust_wspace</code> <code>float</code> <p>Width space between subplots. Defaults to 0.2.</p> <code>0.2</code> <code>show_median</code> <code>bool</code> <p>If True, show median values above boxes. Defaults to True.</p> <code>True</code> <code>median_line_color</code> <code>str</code> <p>Color of median lines. Defaults to \"black\".</p> <code>'black'</code> <code>median_font_size</code> <code>str</code> <p>Font size for median values. Defaults to \"small\".</p> <code>'small'</code> <p>Returns:</p> Type Description <p>Union[plt.Figure, Tuple[plt.Axes, dict]]: If axin is None, returns the</p> <p>figure object. Otherwise, returns a tuple of (axes, boxplot_dict).</p> Example <p>data = [np.random.normal(0, 1, 100), np.random.normal(2, 1, 100)] fig = plot_boxes_matplotlib(data, ...                           label1=['Group A'], ...                           label2=['Sample 1', 'Sample 2'], ...                           show_median=True)</p> Source code in <code>hydroutils\\hydro_plot.py</code> <pre><code>def plot_boxes_matplotlib(\n    data: list,\n    label1: list = None,\n    label2: list = None,\n    leg_col: int = None,\n    colorlst=\"rbgcmywrbgcmyw\",\n    title=None,\n    figsize=(8, 6),\n    sharey=False,\n    xticklabel=None,\n    axin=None,\n    ylim=None,\n    ylabel=None,\n    notch=False,\n    widths=0.5,\n    subplots_adjust_wspace=0.2,\n    show_median=True,\n    median_line_color=\"black\",\n    median_font_size=\"small\",\n):\n    \"\"\"Create multiple boxplots for comparing multiple indicators.\n\n    This function creates a figure with multiple boxplots, each representing a\n    different indicator. It supports customization of appearance, labels, and\n    layout, and can display median values and other statistics.\n\n    Args:\n        data (list): List of data arrays, each element represents one indicator\n            and can contain multiple numpy arrays for box comparison.\n        label1 (list, optional): Names for each subplot. Defaults to None.\n        label2 (list, optional): Legend names for boxes within each subplot.\n            Same across all subplots. Defaults to None.\n        leg_col (int, optional): Number of columns in the legend. Defaults to None.\n        colorlst (str, optional): String of color characters for boxes.\n            Defaults to \"rbgcmywrbgcmyw\".\n        title (str, optional): Figure title. Defaults to None.\n        figsize (tuple, optional): Figure size as (width, height).\n            Defaults to (8, 6).\n        sharey (bool, optional): If True, all subplots share y-axis scale.\n            Defaults to False.\n        xticklabel (list, optional): Labels for x-axis ticks. Defaults to None.\n        axin (matplotlib.axes.Axes, optional): Existing axes to plot on.\n            Defaults to None.\n        ylim (list, optional): Y-axis limits [min, max]. Defaults to None.\n        ylabel (list, optional): Y-axis labels for each subplot. Defaults to None.\n        notch (bool, optional): If True, boxes will have notches.\n            Defaults to False.\n        widths (float, optional): Width of the boxes. Defaults to 0.5.\n        subplots_adjust_wspace (float, optional): Width space between subplots.\n            Defaults to 0.2.\n        show_median (bool, optional): If True, show median values above boxes.\n            Defaults to True.\n        median_line_color (str, optional): Color of median lines.\n            Defaults to \"black\".\n        median_font_size (str, optional): Font size for median values.\n            Defaults to \"small\".\n\n    Returns:\n        Union[plt.Figure, Tuple[plt.Axes, dict]]: If axin is None, returns the\n        figure object. Otherwise, returns a tuple of (axes, boxplot_dict).\n\n    Example:\n        &gt;&gt;&gt; data = [np.random.normal(0, 1, 100), np.random.normal(2, 1, 100)]\n        &gt;&gt;&gt; fig = plot_boxes_matplotlib(data,\n        ...                           label1=['Group A'],\n        ...                           label2=['Sample 1', 'Sample 2'],\n        ...                           show_median=True)\n    \"\"\"\n    nc = len(data)\n    if axin is None:\n        fig, axes = plt.subplots(\n            ncols=nc, sharey=sharey, figsize=figsize, constrained_layout=False\n        )\n    else:\n        axes = axin\n\n    # the next few lines are for showing median values\n    decimal_places = \"2\"\n    for k in range(nc):\n        ax = axes[k] if nc &gt; 1 else axes\n        temp = data[k]\n        if type(temp) is list:\n            for kk in range(len(temp)):\n                tt = temp[kk]\n                if tt is not None and len(tt) &gt; 0:\n                    tt = tt[~np.isnan(tt)]\n                    temp[kk] = tt\n                else:\n                    temp[kk] = []\n        else:\n            temp = temp[~np.isnan(temp)]\n        bp = ax.boxplot(\n            temp, patch_artist=True, notch=notch, showfliers=False, widths=widths\n        )\n        for median in bp[\"medians\"]:\n            median.set_color(median_line_color)\n        medians_value = [np.median(tmp) for tmp in temp]\n        percent25value = [np.percentile(tmp, 25) for tmp in temp]\n        percent75value = [np.percentile(tmp, 75) for tmp in temp]\n        per25min = np.min(percent25value)\n        per75max = np.max(percent75value)\n        median_labels = [format(s, f\".{decimal_places}f\") for s in medians_value]\n        pos = range(len(medians_value))\n        if show_median:\n            for tick, label in zip(pos, ax.get_xticklabels()):\n                # params of ax.text could be seen here: https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.text.html\n                ax.text(\n                    pos[tick] + 1,\n                    medians_value[tick] + (per75max - per25min) * 0.01,\n                    median_labels[tick],\n                    horizontalalignment=\"center\",\n                    # https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.text.html\n                    size=median_font_size,\n                    weight=\"semibold\",\n                    color=median_line_color,\n                )\n        for kk in range(len(bp[\"boxes\"])):\n            plt.setp(bp[\"boxes\"][kk], facecolor=colorlst[kk])\n\n        if label1 is not None:\n            ax.set_xlabel(label1[k])\n        else:\n            ax.set_xlabel(str(k))\n        if xticklabel is None:\n            ax.set_xticks([])\n        else:\n            ax.set_xticks([y + 1 for y in range(0, len(data[k]), 2)])\n            ax.set_xticklabels(xticklabel)\n        if ylabel is not None:\n            ax.set_ylabel(ylabel[k])\n        if ylim is not None:\n            ax.set_ylim(ylim[k])\n    if label2 is not None:\n        plt.legend(\n            bp[\"boxes\"],\n            label2,\n            # explanation for bbox_to_anchor: https://zhuanlan.zhihu.com/p/101059179\n            bbox_to_anchor=(1.0, 1.02, 0.25, 0.05),\n            loc=\"upper right\",\n            borderaxespad=0,\n            ncol=len(label2) if leg_col is None else leg_col,\n            frameon=False,\n            fontsize=12,\n        )\n    if title is not None:\n        # fig.suptitle(title)\n        ax.set_title(title)\n    plt.tight_layout()\n    plt.subplots_adjust(wspace=subplots_adjust_wspace)\n    return fig if axin is None else (ax, bp)\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.plot_boxs","title":"<code>plot_boxs(data, x_name, y_name, uniform_color=None, swarm_plot=False, hue=None, colormap=False, xlim=None, ylim=None, order=None, font='serif', rotation=45, show_median=False)</code>","text":"<p>plot multiple boxes in one ax with seaborn Parameters</p> <p>data : pd.DataFrame     a tidy pandas dataframe;     if you don't know what is \"tidy data\", please read: https://github.com/jizhang/pandas-tidy-data x_name : str     the names of each box y_name : str     what is shown uniform_color : str, optional     unified color for all boxes, by default None swarm_plot : bool, optional     description, by default False hue : type, optional     description, by default None colormap : bool, optional     description, by default False xlim : type, optional     description, by default None ylim : type, optional     description, by default None order : type, optional     description, by default None font : str, optional     description, by default \"serif\" rotation : int, optional     rotation for labels in x-axis, by default 45 show_median: bool, optional     if True, show median value for each box, by default False Returns</p> <p>type description</p> Source code in <code>hydroutils\\hydro_plot.py</code> <pre><code>def plot_boxs(\n    data: pd.DataFrame,\n    x_name: str,\n    y_name: str,\n    uniform_color=None,\n    swarm_plot=False,\n    hue=None,\n    colormap=False,\n    xlim=None,\n    ylim=None,\n    order=None,\n    font=\"serif\",\n    rotation=45,\n    show_median=False,\n):\n    \"\"\"plot multiple boxes in one ax with seaborn\n    Parameters\n    ----------\n    data : pd.DataFrame\n        a tidy pandas dataframe;\n        if you don't know what is \"tidy data\", please read: https://github.com/jizhang/pandas-tidy-data\n    x_name : str\n        the names of each box\n    y_name : str\n        what is shown\n    uniform_color : str, optional\n        unified color for all boxes, by default None\n    swarm_plot : bool, optional\n        _description_, by default False\n    hue : _type_, optional\n        _description_, by default None\n    colormap : bool, optional\n        _description_, by default False\n    xlim : _type_, optional\n        _description_, by default None\n    ylim : _type_, optional\n        _description_, by default None\n    order : _type_, optional\n        _description_, by default None\n    font : str, optional\n        _description_, by default \"serif\"\n    rotation : int, optional\n        rotation for labels in x-axis, by default 45\n    show_median: bool, optional\n        if True, show median value for each box, by default False\n    Returns\n    -------\n    _type_\n        _description_\n    \"\"\"\n    fig = plt.figure()\n    sns.set(style=\"ticks\", palette=\"pastel\", font=font, font_scale=1.5)\n    # Draw a nested boxplot to show bills by day and time\n    if uniform_color is not None:\n        sns_box = sns.boxplot(\n            x=x_name,\n            y=y_name,\n            data=data,\n            color=uniform_color,\n            showfliers=False,\n            order=order,\n        )\n    else:\n        sns_box = sns.boxplot(\n            x=x_name, y=y_name, data=data, showfliers=False, order=order\n        )\n    if swarm_plot:\n        if hue is None:\n            sns_box = sns.swarmplot(\n                x=x_name, y=y_name, data=data, color=\".2\", order=order\n            )\n        elif colormap:\n            # Create a matplotlib colormap from the sns seagreen color palette\n            cmap = sns.light_palette(\"seagreen\", reverse=False, as_cmap=True)\n            # Normalize to the range of possible values from df[\"c\"]\n            norm = matplotlib.colors.Normalize(\n                vmin=data[hue].min(), vmax=data[hue].max()\n            )\n            colors = {cval: cmap(norm(cval)) for cval in data[hue]}\n            # plot the swarmplot with the colors dictionary as palette, s=2 means size is 2\n            sns_box = sns.swarmplot(\n                x=x_name,\n                y=y_name,\n                hue=hue,\n                s=2,\n                data=data,\n                palette=colors,\n                order=order,\n            )\n            # remove the legend, because we want to set a colorbar instead\n            plt.gca().legend_.remove()\n            # create colorbar\n            divider = make_axes_locatable(plt.gca())\n            ax_cb = divider.new_horizontal(size=\"5%\", pad=0.05)\n            fig = sns_box.get_figure()\n            fig.add_axes(ax_cb)\n            cb1 = matplotlib.colorbar.ColorbarBase(\n                ax_cb, cmap=cmap, norm=norm, orientation=\"vertical\"\n            )\n            cb1.set_label(\"Some Units\")\n        else:\n            palette = sns.light_palette(\"seagreen\", reverse=False, n_colors=10)\n            sns_box = sns.swarmplot(\n                x=x_name,\n                y=y_name,\n                hue=hue,\n                s=2,\n                data=data,\n                palette=palette,\n                order=order,\n            )\n    if xlim is not None:\n        plt.xlim(xlim[0], xlim[1])\n    if ylim is not None:\n        plt.ylim(ylim[0], ylim[1])\n    if show_median:\n        medians = data.groupby([x_name], sort=False)[y_name].median().values\n        create_median_labels(sns_box, medians_value=medians, size=\"x-small\")\n    sns.despine()\n    locs, labels = plt.xticks()\n    plt.setp(labels, rotation=rotation)\n    # plt.show()\n    return sns_box.get_figure()\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.plot_diff_boxes","title":"<code>plot_diff_boxes(data, row_and_col=None, y_col=None, x_col=None, hspace=0.3, wspace=1, title_str=None, title_font_size=14)</code>","text":"<p>plot boxplots in rows and cols</p> Source code in <code>hydroutils\\hydro_plot.py</code> <pre><code>def plot_diff_boxes(\n    data,\n    row_and_col=None,\n    y_col=None,\n    x_col=None,\n    hspace=0.3,\n    wspace=1,\n    title_str=None,\n    title_font_size=14,\n):\n    \"\"\"plot boxplots in rows and cols\"\"\"\n    # matplotlib.use('TkAgg')\n    if type(data) is not pd.DataFrame:\n        data = pd.DataFrame(data)\n    subplot_num = data.shape[1] if y_col is None else len(y_col)\n    if row_and_col is None:\n        row_num = 1\n        col_num = subplot_num\n        f, axes = plt.subplots(row_num, col_num)\n        plt.subplots_adjust(hspace=hspace, wspace=wspace)\n    else:\n        assert subplot_num &lt;= row_and_col[0] * row_and_col[1]\n        row_num = row_and_col[0]\n        col_num = row_and_col[1]\n        f, axes = plt.subplots(row_num, col_num)\n        f.tight_layout()\n    for i in range(subplot_num):\n        if y_col is None:\n            if row_num == 1 or col_num == 1:\n                sns.boxplot(\n                    y=data.columns.values[i],\n                    data=data,\n                    width=0.5,\n                    orient=\"v\",\n                    ax=axes[i],\n                    showfliers=False,\n                ).set(xlabel=data.columns.values[i], ylabel=\"\")\n            else:\n                row_idx = int(i / col_num)\n                col_idx = i % col_num\n                sns.boxplot(\n                    y=data.columns.values[i],\n                    data=data,\n                    orient=\"v\",\n                    ax=axes[row_idx, col_idx],\n                    showfliers=False,\n                )\n        else:\n            assert x_col is not None\n            if row_num == 1 or col_num == 1:\n                sns.boxplot(\n                    x=data.columns.values[x_col],\n                    y=data.columns.values[y_col[i]],\n                    data=data,\n                    orient=\"v\",\n                    ax=axes[i],\n                    showfliers=False,\n                )\n            else:\n                row_idx = int(i / col_num)\n                col_idx = i % col_num\n                sns.boxplot(\n                    x=data.columns.values[x_col],\n                    y=data.columns.values[y_col[i]],\n                    data=data,\n                    orient=\"v\",\n                    ax=axes[row_idx, col_idx],\n                    showfliers=False,\n                )\n    if title_str is not None:\n        f.suptitle(title_str, fontsize=title_font_size)\n    return f\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.plot_ecdf","title":"<code>plot_ecdf(mydataframe, mycolumn, save_file=None)</code>","text":"<p>Empirical cumulative distribution function</p> Source code in <code>hydroutils\\hydro_plot.py</code> <pre><code>def plot_ecdf(mydataframe, mycolumn, save_file=None):\n    \"\"\"Empirical cumulative distribution function\"\"\"\n    x, y = ecdf(mydataframe[mycolumn])\n    df = pd.DataFrame({\"x\": x, \"y\": y})\n    sns.set_style(\"ticks\", {\"axes.grid\": True})\n    sns.lineplot(x=\"x\", y=\"y\", data=df, estimator=None).set(\n        xlim=(0, 1), xticks=np.arange(0, 1, 0.05), yticks=np.arange(0, 1, 0.05)\n    )\n    plt.show()\n    if save_file is not None:\n        plt.savefig(save_file)\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.plot_ecdfs","title":"<code>plot_ecdfs(xs, ys, legends=None, style=None, case_str='case', event_str='event', x_str='x', y_str='y', ax_as_subplot=None, interval=0.1)</code>","text":"<p>Empirical cumulative distribution function</p> Source code in <code>hydroutils\\hydro_plot.py</code> <pre><code>def plot_ecdfs(\n    xs,\n    ys,\n    legends=None,\n    style=None,\n    case_str=\"case\",\n    event_str=\"event\",\n    x_str=\"x\",\n    y_str=\"y\",\n    ax_as_subplot=None,\n    interval=0.1,\n):\n    \"\"\"Empirical cumulative distribution function\"\"\"\n    assert isinstance(xs, list) and isinstance(ys, list)\n    assert len(xs) == len(ys)\n    if legends is not None:\n        assert isinstance(legends, list)\n        assert len(ys) == len(legends)\n    if style is not None:\n        assert isinstance(style, list)\n        assert len(ys) == len(style)\n    for y in ys:\n        assert all(xi &lt; yi for xi, yi in zip(y, y[1:]))\n    frames = []\n    for i in range(len(xs)):\n        str_i = x_str + str(i) if legends is None else legends[i]\n        assert all(xi &lt; yi for xi, yi in zip(xs[i], xs[i][1:]))\n        df_dict_i = {\n            x_str: xs[i],\n            y_str: ys[i],\n            case_str: np.full([xs[i].size], str_i),\n        }\n        if style is not None:\n            df_dict_i[event_str] = np.full([xs[i].size], style[i])\n        df_i = pd.DataFrame(df_dict_i)\n        frames.append(df_i)\n    df = pd.concat(frames)\n    sns.set_style(\"ticks\", {\"axes.grid\": True})\n    if style is None:\n        return (\n            sns.lineplot(x=x_str, y=y_str, hue=case_str, data=df, estimator=None).set(\n                xlim=(0, 1),\n                xticks=np.arange(0, 1, interval),\n                yticks=np.arange(0, 1, interval),\n            )\n            if ax_as_subplot is None\n            else sns.lineplot(\n                ax=ax_as_subplot,\n                x=x_str,\n                y=y_str,\n                hue=case_str,\n                data=df,\n                estimator=None,\n            ).set(\n                xlim=(0, 1),\n                xticks=np.arange(0, 1, interval),\n                yticks=np.arange(0, 1, interval),\n            )\n        )\n    elif ax_as_subplot is None:\n        return sns.lineplot(\n            x=x_str,\n            y=y_str,\n            hue=case_str,\n            style=event_str,\n            data=df,\n            estimator=None,\n        ).set(\n            xlim=(0, 1),\n            xticks=np.arange(0, 1, interval),\n            yticks=np.arange(0, 1, interval),\n        )\n    else:\n        return sns.lineplot(\n            ax=ax_as_subplot,\n            x=x_str,\n            y=y_str,\n            hue=case_str,\n            style=event_str,\n            data=df,\n            estimator=None,\n        ).set(\n            xlim=(0, 1),\n            xticks=np.arange(0, 1, interval),\n            yticks=np.arange(0, 1, interval),\n        )\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.plot_ecdfs_matplot","title":"<code>plot_ecdfs_matplot(xs, ys, legends=None, colors='rbkgcmy', dash_lines=None, x_str='x', y_str='y', x_interval=0.1, y_interval=0.1, x_lim=(0, 1), y_lim=(0, 1), show_legend=True, legend_font_size=16, fig_size=(8, 6))</code>","text":"<p>Empirical cumulative distribution function with matplotlib Parameters</p> <p>xs : type description ys : type description legends : type, optional     description, by default None colors : str, optional     description, by default \"rbkgcmy\" dash_lines : type, optional     description, by default None x_str : str, optional     description, by default \"x\" y_str : str, optional     description, by default \"y\" x_interval : float, optional     description, by default 0.1 y_interval : float, optional     description, by default 0.1 x_lim : tuple, optional     description, by default (0, 1) y_lim : tuple, optional     description, by default (0, 1) show_legend : bool, optional     description, by default True legend_font_size : int, optional     description, by default 16 fig_size : tuple, optional     size of the figure, by default (8, 6) Returns</p> <p>type description</p> Source code in <code>hydroutils\\hydro_plot.py</code> <pre><code>def plot_ecdfs_matplot(\n    xs,\n    ys,\n    legends=None,\n    colors=\"rbkgcmy\",\n    dash_lines=None,\n    x_str=\"x\",\n    y_str=\"y\",\n    x_interval=0.1,\n    y_interval=0.1,\n    x_lim=(0, 1),\n    y_lim=(0, 1),\n    show_legend=True,\n    legend_font_size=16,\n    fig_size=(8, 6),\n):\n    \"\"\"Empirical cumulative distribution function with matplotlib\n    Parameters\n    ----------\n    xs : _type_\n        _description_\n    ys : _type_\n        _description_\n    legends : _type_, optional\n        _description_, by default None\n    colors : str, optional\n        _description_, by default \"rbkgcmy\"\n    dash_lines : _type_, optional\n        _description_, by default None\n    x_str : str, optional\n        _description_, by default \"x\"\n    y_str : str, optional\n        _description_, by default \"y\"\n    x_interval : float, optional\n        _description_, by default 0.1\n    y_interval : float, optional\n        _description_, by default 0.1\n    x_lim : tuple, optional\n        _description_, by default (0, 1)\n    y_lim : tuple, optional\n        _description_, by default (0, 1)\n    show_legend : bool, optional\n        _description_, by default True\n    legend_font_size : int, optional\n        _description_, by default 16\n    fig_size : tuple, optional\n        size of the figure, by default (8, 6)\n    Returns\n    -------\n    _type_\n        _description_\n    \"\"\"\n    assert isinstance(xs, list) and isinstance(ys, list)\n    assert len(xs) == len(ys)\n    if legends is not None:\n        assert isinstance(legends, list) and len(ys) == len(legends)\n    if dash_lines is not None:\n        assert isinstance(dash_lines, list)\n    else:\n        dash_lines = np.full(len(xs), False).tolist()\n    for y in ys:\n        assert all(xi &lt; yi for xi, yi in zip(y, y[1:]))\n    fig = plt.figure(figsize=fig_size)\n    ax = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n    for i in range(len(xs)):\n        if (\n            np.nanmax(np.array(xs[i])) == np.inf\n            or np.nanmin(np.array(xs[i])) == -np.inf\n        ):\n            assert all(xi &lt;= yi for xi, yi in zip(xs[i], xs[i][1:]))\n        else:\n            assert all(xi &lt;= yi for xi, yi in zip(xs[i], xs[i][1:]))\n        (line_i,) = ax.plot(xs[i], ys[i], color=colors[i], label=legends[i])\n        if dash_lines[i]:\n            line_i.set_dashes([2, 2, 10, 2])\n\n    plt.xlabel(x_str, fontsize=18)\n    plt.ylabel(y_str, fontsize=18)\n    ax.set_xlim(x_lim[0], x_lim[1])\n    ax.set_ylim(y_lim[0], y_lim[1])\n    # set x y number font size\n    plt.xticks(np.arange(x_lim[0], x_lim[1] + x_lim[1] / 100, x_interval), fontsize=16)\n    plt.yticks(np.arange(y_lim[0], y_lim[1] + y_lim[1] / 100, y_interval), fontsize=16)\n    if show_legend:\n        ax.legend()\n        plt.legend(prop={\"size\": legend_font_size})\n    plt.grid()\n    # Hide the right and top spines\n    ax.spines[\"right\"].set_visible(False)\n    ax.spines[\"top\"].set_visible(False)\n    return fig, ax\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.plot_event_characteristics","title":"<code>plot_event_characteristics(event_analysis, output_folder, delta_t_hours=3.0, net_rain_key='P_eff', obs_flow_key='Q_obs_eff')</code>","text":"<p>Create and save a detailed flood event characteristics plot.</p> <p>This function creates a comprehensive visualization of a flood event, showing both the net rainfall and direct runoff, along with key event characteristics in a text box. The runoff curve is plotted on top of the rainfall bars for better visibility.</p> <p>Parameters:</p> Name Type Description Default <code>event_analysis</code> <code>Dict</code> <p>Dictionary containing flood event data and analysis: - filepath (str): Path to source data file - peak_obs (float): Peak observed flow - runoff_volume_m3 (float): Total runoff volume in m\u00b3 - runoff_duration_hours (float): Event duration in hours - total_net_rain (float): Total net rainfall in mm - lag_time_hours (float): Time lag between peak rain and peak flow - P_eff (np.ndarray): Net rainfall time series - Q_obs_eff (np.ndarray): Observed flow time series</p> required <code>output_folder</code> <code>str</code> <p>Directory where the plot will be saved.</p> required <code>delta_t_hours</code> <code>float</code> <p>Time step in hours. Defaults to 3.0.</p> <code>3.0</code> <code>net_rain_key</code> <code>str</code> <p>Key for net rainfall in event_analysis. Defaults to \"P_eff\".</p> <code>'P_eff'</code> <code>obs_flow_key</code> <code>str</code> <p>Key for observed flow in event_analysis. Defaults to \"Q_obs_eff\".</p> <code>'Q_obs_eff'</code> Note <ul> <li>Uses dual axes: left for runoff (m\u00b3/s), right for rainfall (mm)</li> <li>Rainfall is plotted as blue bars from top</li> <li>Runoff is plotted as orange line with higher z-order</li> <li>Includes grid for better readability</li> <li>Text box shows key event characteristics</li> <li>Saves plot as PNG with 150 DPI</li> <li>Uses SimSun font for Chinese characters</li> </ul> Example <p>event = { ...     'filepath': 'event_001.csv', ...     'peak_obs': 150.5, ...     'runoff_volume_m3': 2.5e6, ...     'runoff_duration_hours': 48.0, ...     'total_net_rain': 85.5, ...     'lag_time_hours': 6.0, ...     'P_eff': np.array([...]),  # rainfall data ...     'Q_obs_eff': np.array([...])  # flow data ... } plot_event_characteristics(event, 'output/plots/')</p> Credit <p>Original implementation by Zheng Zhang</p> Source code in <code>hydroutils\\hydro_plot.py</code> <pre><code>def plot_event_characteristics(\n    event_analysis: Dict,\n    output_folder: str,\n    delta_t_hours: float = 3.0,\n    net_rain_key: str = \"P_eff\",\n    obs_flow_key: str = \"Q_obs_eff\",\n):\n    \"\"\"Create and save a detailed flood event characteristics plot.\n\n    This function creates a comprehensive visualization of a flood event, showing\n    both the net rainfall and direct runoff, along with key event characteristics\n    in a text box. The runoff curve is plotted on top of the rainfall bars for\n    better visibility.\n\n    Args:\n        event_analysis (Dict): Dictionary containing flood event data and analysis:\n            - filepath (str): Path to source data file\n            - peak_obs (float): Peak observed flow\n            - runoff_volume_m3 (float): Total runoff volume in m\u00b3\n            - runoff_duration_hours (float): Event duration in hours\n            - total_net_rain (float): Total net rainfall in mm\n            - lag_time_hours (float): Time lag between peak rain and peak flow\n            - P_eff (np.ndarray): Net rainfall time series\n            - Q_obs_eff (np.ndarray): Observed flow time series\n        output_folder (str): Directory where the plot will be saved.\n        delta_t_hours (float, optional): Time step in hours. Defaults to 3.0.\n        net_rain_key (str, optional): Key for net rainfall in event_analysis.\n            Defaults to \"P_eff\".\n        obs_flow_key (str, optional): Key for observed flow in event_analysis.\n            Defaults to \"Q_obs_eff\".\n\n    Note:\n        - Uses dual axes: left for runoff (m\u00b3/s), right for rainfall (mm)\n        - Rainfall is plotted as blue bars from top\n        - Runoff is plotted as orange line with higher z-order\n        - Includes grid for better readability\n        - Text box shows key event characteristics\n        - Saves plot as PNG with 150 DPI\n        - Uses SimSun font for Chinese characters\n\n    Example:\n        &gt;&gt;&gt; event = {\n        ...     'filepath': 'event_001.csv',\n        ...     'peak_obs': 150.5,\n        ...     'runoff_volume_m3': 2.5e6,\n        ...     'runoff_duration_hours': 48.0,\n        ...     'total_net_rain': 85.5,\n        ...     'lag_time_hours': 6.0,\n        ...     'P_eff': np.array([...]),  # rainfall data\n        ...     'Q_obs_eff': np.array([...])  # flow data\n        ... }\n        &gt;&gt;&gt; plot_event_characteristics(event, 'output/plots/')\n\n    Credit:\n        Original implementation by Zheng Zhang\n    \"\"\"\n    net_rain = event_analysis[net_rain_key]\n    direct_runoff = event_analysis[obs_flow_key]\n    event_filename = os.path.basename(event_analysis[\"filepath\"])\n\n    fig, ax1 = plt.subplots(figsize=(15, 7))\n    fig.suptitle(f\"\u6d2a\u6c34\u4e8b\u4ef6\u7279\u5f81\u5206\u6790 - {event_filename}\", fontsize=16)\n\n    # \u7ed8\u5236\u5f84\u6d41\u66f2\u7ebf (\u5de6Y\u8f74)\n    x_axis = np.arange(len(direct_runoff))\n    # --- \u6838\u5fc3\u4fee\u6539\uff1a\u4e3a\u66f2\u7ebf\u8bbe\u7f6e\u4e00\u4e2a\u8f83\u9ad8\u7684 zorder ---\n    ax1.plot(\n        x_axis,\n        direct_runoff,\n        color=\"orangered\",\n        label=r\"\u5f84\u6d41 ($m^3/s$)\",\n        zorder=10,\n        linewidth=2,\n    )  # zorder=10\n\n    ax1.set_xlabel(f\"\u65f6\u6bb5 (\u0394t = {delta_t_hours}h)\", fontsize=12)\n    ax1.set_ylabel(r\"\u5f84\u6d41\u6d41\u91cf ($m^3/s$)\", color=\"orangered\", fontsize=12)\n    ax1.tick_params(axis=\"y\", labelcolor=\"orangered\")\n    ax1.set_ylim(bottom=0)\n    ax1.grid(True, which=\"major\", linestyle=\"--\", linewidth=\"0.5\", color=\"gray\")\n\n    # \u521b\u5efa\u5171\u4eabX\u8f74\u7684\u7b2c\u4e8c\u4e2aY\u8f74\n    ax2 = ax1.twinx()\n    # \u7ed8\u5236\u51c0\u96e8\u67f1\u72b6\u56fe (\u53f3Y\u8f74\uff0c\u5411\u4e0b)\n    # --- \u6838\u5fc3\u4fee\u6539\uff1a\u4e3a\u67f1\u72b6\u56fe\u8bbe\u7f6e\u4e00\u4e2a\u8f83\u4f4e\u7684 zorder (\u53ef\u9009\uff0c\u4f46\u597d\u4e60\u60ef) ---\n    ax2.bar(\n        x_axis,\n        net_rain,\n        color=\"steelblue\",\n        label=\"\u51c0\u96e8 (mm)\",\n        width=0.8,\n        zorder=5,\n    )  # zorder=5\n\n    ax2.set_ylabel(\"\u51c0\u96e8\u91cf (mm)\", color=\"steelblue\", fontsize=12)\n    ax2.tick_params(axis=\"y\", labelcolor=\"steelblue\")\n    ax2.invert_yaxis()\n    ax2.set_ylim(top=0)\n\n    # \u51c6\u5907\u5e76\u6807\u6ce8\u6587\u672c\u6846 (\u4e0e\u4e4b\u524d\u5bf9\u9f50\u7248\u672c\u76f8\u540c)\n    labels = [\n        \"\u6d2a\u5cf0\u6d41\u91cf:\",\n        \"\u6d2a   \u91cf:\",\n        \"\u6d2a\u6c34\u5386\u65f6:\",\n        \"\u603b\u51c0\u96e8\u91cf:\",\n        \"\u6d2a\u5cf0\u96e8\u5cf0\u5ef6\u8fdf:\",\n    ]\n    values = [\n        f\"{event_analysis['peak_obs']:.2f} \" + r\"$m^3/s$\",\n        f\"{event_analysis['runoff_volume_m3'] / 1e6:.2f} \" + r\"$\\times 10^6\\ m^3$\",\n        f\"{event_analysis['runoff_duration_hours']:.1f} \u5c0f\u65f6\",\n        f\"{event_analysis['total_net_rain']:.2f} mm\",\n        f\"{event_analysis['lag_time_hours']:.1f} \u5c0f\u65f6\",\n    ]\n    label_text = \"\\n\".join(labels)\n    value_text = \"\\n\".join(values)\n    bbox_props = dict(boxstyle=\"round,pad=0.5\", facecolor=\"wheat\", alpha=0.8)\n    ax1.text(\n        0.85,\n        0.95,\n        \"--- \u6c34\u6587\u7279\u5f81 ---\",\n        transform=ax1.transAxes,\n        fontsize=12,\n        verticalalignment=\"top\",\n        horizontalalignment=\"center\",\n        bbox=bbox_props,\n    )\n    ax1.text(\n        0.80,\n        0.85,\n        label_text,\n        transform=ax1.transAxes,\n        fontsize=12,\n        verticalalignment=\"top\",\n        horizontalalignment=\"right\",\n        family=\"SimSun\",\n    )\n    ax1.text(\n        0.82,\n        0.85,\n        value_text,\n        transform=ax1.transAxes,\n        fontsize=12,\n        verticalalignment=\"top\",\n        horizontalalignment=\"left\",\n    )\n\n    # \u5408\u5e76\u56fe\u4f8b\n    lines1, labels1 = ax1.get_legend_handles_labels()\n    lines2, labels2 = ax2.get_legend_handles_labels()\n    ax2.legend(\n        lines1 + lines2,\n        labels1 + labels2,\n        loc=\"upper left\",\n        bbox_to_anchor=(0.01, 0.95),\n    )\n\n    # \u4fdd\u5b58\u56fe\u5f62\n    output_filename = os.path.join(\n        output_folder, f\"{os.path.splitext(event_filename)[0]}.png\"\n    )\n    try:\n        plt.savefig(output_filename, dpi=150, bbox_inches=\"tight\")\n        print(f\"  \u5df2\u751f\u6210\u56fe\u8868: {output_filename}\")\n    except Exception as e:\n        print(f\"  \u4fdd\u5b58\u56fe\u8868\u5931\u8d25: {output_filename}, \u9519\u8bef: {e}\")\n\n    plt.close(fig)\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.plot_heat_map","title":"<code>plot_heat_map(data, mask=None, fig_size=None, fmt='d', square=True, annot=True, xticklabels=True, yticklabels=True)</code>","text":"<p>Create a heatmap visualization using seaborn.</p> <p>This function creates a customizable heatmap for visualizing 2D data arrays. It uses seaborn's heatmap function with additional formatting options and supports masking specific data points.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>2D data array to visualize.</p> required <code>mask</code> <code>ndarray</code> <p>Boolean array of same shape as data. True values will be masked (not shown). Defaults to None.</p> <code>None</code> <code>fig_size</code> <code>tuple</code> <p>Figure size as (width, height). Defaults to None.</p> <code>None</code> <code>fmt</code> <code>str</code> <p>String formatting code for cell annotations. Defaults to \"d\" (integer).</p> <code>'d'</code> <code>square</code> <code>bool</code> <p>If True, set the Axes aspect to \"equal\". Defaults to True.</p> <code>True</code> <code>annot</code> <code>bool</code> <p>If True, write the data value in each cell. Defaults to True.</p> <code>True</code> <code>xticklabels</code> <code>bool</code> <p>If True, show x-axis tick labels. Defaults to True.</p> <code>True</code> <code>yticklabels</code> <code>bool</code> <p>If True, show y-axis tick labels. Defaults to True.</p> <code>True</code> Note <ul> <li>Uses \"RdBu_r\" colormap (red-blue diverging)</li> <li>Annotations are shown by default</li> <li>Cells are square by default for better visualization</li> <li>Based on seaborn's heatmap: https://seaborn.pydata.org/generated/seaborn.heatmap.html</li> </ul> Example <p>data = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) plot_heat_map(data, fig_size=(8, 6))</p> Source code in <code>hydroutils\\hydro_plot.py</code> <pre><code>def plot_heat_map(\n    data: pd.DataFrame,\n    mask=None,\n    fig_size=None,\n    fmt=\"d\",\n    square=True,\n    annot=True,\n    xticklabels=True,\n    yticklabels=True,\n):\n    \"\"\"Create a heatmap visualization using seaborn.\n\n    This function creates a customizable heatmap for visualizing 2D data arrays.\n    It uses seaborn's heatmap function with additional formatting options and\n    supports masking specific data points.\n\n    Args:\n        data (pd.DataFrame): 2D data array to visualize.\n        mask (np.ndarray, optional): Boolean array of same shape as data. True\n            values will be masked (not shown). Defaults to None.\n        fig_size (tuple, optional): Figure size as (width, height). Defaults to None.\n        fmt (str, optional): String formatting code for cell annotations.\n            Defaults to \"d\" (integer).\n        square (bool, optional): If True, set the Axes aspect to \"equal\".\n            Defaults to True.\n        annot (bool, optional): If True, write the data value in each cell.\n            Defaults to True.\n        xticklabels (bool, optional): If True, show x-axis tick labels.\n            Defaults to True.\n        yticklabels (bool, optional): If True, show y-axis tick labels.\n            Defaults to True.\n\n    Note:\n        - Uses \"RdBu_r\" colormap (red-blue diverging)\n        - Annotations are shown by default\n        - Cells are square by default for better visualization\n        - Based on seaborn's heatmap: https://seaborn.pydata.org/generated/seaborn.heatmap.html\n\n    Example:\n        &gt;&gt;&gt; data = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n        &gt;&gt;&gt; plot_heat_map(data, fig_size=(8, 6))\n    \"\"\"\n    if fig_size is not None:\n        fig = plt.figure(figsize=fig_size)\n    ax = sns.heatmap(\n        data=data,\n        square=square,\n        annot=annot,\n        fmt=fmt,\n        cmap=\"RdBu_r\",\n        mask=mask,\n        xticklabels=xticklabels,\n        yticklabels=yticklabels,\n    )\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.plot_map_carto","title":"<code>plot_map_carto(data, lat, lon, fig=None, ax=None, pertile_range=None, value_range=None, fig_size=(8, 8), need_colorbar=True, colorbar_size=[0.91, 0.318, 0.02, 0.354], cmap_str='jet', idx_lst=None, markers=None, marker_size=20, is_discrete=False, colors='rbkgcmywrbkgcmyw', category_names=None, legend_font_size=None, colorbar_font_size=None)</code>","text":"<p>Create a map visualization using Cartopy with data points or categories.</p> <p>This function creates a map using Cartopy and plots data points on it. It supports both continuous and discrete data visualization, with options for customizing markers, colors, and legends/colorbars.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>1-D array of values to plot, one per point.</p> required <code>lat</code> <code>ndarray</code> <p>1-D array of latitude values.</p> required <code>lon</code> <code>ndarray</code> <p>1-D array of longitude values.</p> required <code>fig</code> <code>Figure</code> <p>Existing figure to plot on. Defaults to None.</p> <code>None</code> <code>ax</code> <code>Axes</code> <p>Existing axes to plot on. Defaults to None.</p> <code>None</code> <code>pertile_range</code> <code>list</code> <p>Percentile range for color scaling as [min_percentile, max_percentile]. Example: [25, 75] for interquartile range. Defaults to None.</p> <code>None</code> <code>value_range</code> <code>list</code> <p>Explicit value range for color scaling as [min_value, max_value]. Overrides pertile_range. Defaults to None.</p> <code>None</code> <code>fig_size</code> <code>tuple</code> <p>Figure size as (width, height). Defaults to (8, 8).</p> <code>(8, 8)</code> <code>need_colorbar</code> <code>bool</code> <p>Whether to show colorbar. Defaults to True.</p> <code>True</code> <code>colorbar_size</code> <code>list</code> <p>Colorbar position and size as [left, bottom, width, height]. Defaults to [0.91, 0.318, 0.02, 0.354].</p> <code>[0.91, 0.318, 0.02, 0.354]</code> <code>cmap_str</code> <code>Union[str, list]</code> <p>Colormap name(s). Can be single string or list for multiple point types. Defaults to \"jet\".</p> <code>'jet'</code> <code>idx_lst</code> <code>list</code> <p>List of index arrays for plotting multiple point types separately. Defaults to None.</p> <code>None</code> <code>markers</code> <code>Union[str, list]</code> <p>Marker style(s) for points. Can be single style or list. Defaults to None.</p> <code>None</code> <code>marker_size</code> <code>Union[int, list]</code> <p>Marker size(s). Can be single value or list. Defaults to 20.</p> <code>20</code> <code>is_discrete</code> <code>bool</code> <p>If True, treat data as discrete categories. Defaults to False.</p> <code>False</code> <code>colors</code> <code>str</code> <p>String of color characters for discrete categories. Defaults to \"rbkgcmywrbkgcmyw\".</p> <code>'rbkgcmywrbkgcmyw'</code> <code>category_names</code> <code>list</code> <p>Names for discrete categories. Defaults to None.</p> <code>None</code> <code>legend_font_size</code> <code>float</code> <p>Font size for legend. Defaults to None.</p> <code>None</code> <code>colorbar_font_size</code> <code>float</code> <p>Font size for colorbar. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>plt.Axes: The map axes object.</p> Note <ul> <li>Uses Cartopy's PlateCarree projection</li> <li>Automatically determines map extent from data points</li> <li>Includes state boundaries and coastlines</li> <li>Supports both continuous (colorbar) and discrete (legend) data</li> <li>Can plot multiple point types with different markers/colors</li> <li>Handles NaN values appropriately</li> </ul> Example Source code in <code>hydroutils\\hydro_plot.py</code> <pre><code>def plot_map_carto(\n    data,\n    lat,\n    lon,\n    fig=None,\n    ax=None,\n    pertile_range=None,\n    value_range=None,\n    fig_size=(8, 8),\n    need_colorbar=True,\n    colorbar_size=[0.91, 0.318, 0.02, 0.354],\n    cmap_str=\"jet\",\n    idx_lst=None,\n    markers=None,\n    marker_size=20,\n    is_discrete=False,\n    colors=\"rbkgcmywrbkgcmyw\",\n    category_names=None,\n    legend_font_size=None,\n    colorbar_font_size=None,\n):\n    \"\"\"Create a map visualization using Cartopy with data points or categories.\n\n    This function creates a map using Cartopy and plots data points on it. It supports\n    both continuous and discrete data visualization, with options for customizing\n    markers, colors, and legends/colorbars.\n\n    Args:\n        data (np.ndarray): 1-D array of values to plot, one per point.\n        lat (np.ndarray): 1-D array of latitude values.\n        lon (np.ndarray): 1-D array of longitude values.\n        fig (plt.Figure, optional): Existing figure to plot on. Defaults to None.\n        ax (plt.Axes, optional): Existing axes to plot on. Defaults to None.\n        pertile_range (list, optional): Percentile range for color scaling as\n            [min_percentile, max_percentile]. Example: [25, 75] for interquartile\n            range. Defaults to None.\n        value_range (list, optional): Explicit value range for color scaling as\n            [min_value, max_value]. Overrides pertile_range. Defaults to None.\n        fig_size (tuple, optional): Figure size as (width, height).\n            Defaults to (8, 8).\n        need_colorbar (bool, optional): Whether to show colorbar.\n            Defaults to True.\n        colorbar_size (list, optional): Colorbar position and size as\n            [left, bottom, width, height]. Defaults to [0.91, 0.318, 0.02, 0.354].\n        cmap_str (Union[str, list], optional): Colormap name(s). Can be single\n            string or list for multiple point types. Defaults to \"jet\".\n        idx_lst (list, optional): List of index arrays for plotting multiple\n            point types separately. Defaults to None.\n        markers (Union[str, list], optional): Marker style(s) for points. Can be\n            single style or list. Defaults to None.\n        marker_size (Union[int, list], optional): Marker size(s). Can be single\n            value or list. Defaults to 20.\n        is_discrete (bool, optional): If True, treat data as discrete categories.\n            Defaults to False.\n        colors (str, optional): String of color characters for discrete\n            categories. Defaults to \"rbkgcmywrbkgcmyw\".\n        category_names (list, optional): Names for discrete categories.\n            Defaults to None.\n        legend_font_size (float, optional): Font size for legend.\n            Defaults to None.\n        colorbar_font_size (float, optional): Font size for colorbar.\n            Defaults to None.\n\n    Returns:\n        plt.Axes: The map axes object.\n\n    Note:\n        - Uses Cartopy's PlateCarree projection\n        - Automatically determines map extent from data points\n        - Includes state boundaries and coastlines\n        - Supports both continuous (colorbar) and discrete (legend) data\n        - Can plot multiple point types with different markers/colors\n        - Handles NaN values appropriately\n\n    Example:\n        &gt;&gt;&gt; # Continuous data example\n        &gt;&gt;&gt; data = np.random.rand(100)\n        &gt;&gt;&gt; lat = np.random.uniform(30, 45, 100)\n        &gt;&gt;&gt; lon = np.random.uniform(-120, -100, 100)\n        &gt;&gt;&gt; ax = plot_map_carto(data, lat, lon,\n        ...                    value_range=[0, 1],\n        ...                    cmap_str='viridis')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Discrete categories example\n        &gt;&gt;&gt; categories = np.random.randint(0, 3, 100)\n        &gt;&gt;&gt; ax = plot_map_carto(categories, lat, lon,\n        ...                    is_discrete=True,\n        ...                    category_names=['Low', 'Medium', 'High'])\n    \"\"\"\n    if value_range is not None:\n        vmin = value_range[0]\n        vmax = value_range[1]\n    elif pertile_range is None:\n        # https://blog.csdn.net/chenirene510/article/details/111318539\n        mask_data = np.ma.masked_invalid(data)\n        vmin = np.min(mask_data)\n        vmax = np.max(mask_data)\n    else:\n        assert 0 &lt;= pertile_range[0] &lt; pertile_range[1] &lt;= 100\n        vmin = np.nanpercentile(data, pertile_range[0])\n        vmax = np.nanpercentile(data, pertile_range[1])\n    llcrnrlat = (np.min(lat),)\n    urcrnrlat = (np.max(lat),)\n    llcrnrlon = (np.min(lon),)\n    urcrnrlon = (np.max(lon),)\n    extent = [llcrnrlon[0], urcrnrlon[0], llcrnrlat[0], urcrnrlat[0]]\n    # Figure\n    if fig is None or ax is None:\n        fig, ax = plt.subplots(\n            1, 1, figsize=fig_size, subplot_kw={\"projection\": ccrs.PlateCarree()}\n        )\n    ax.set_extent(extent)\n    states = NaturalEarthFeature(\n        category=\"cultural\",\n        scale=\"50m\",\n        facecolor=\"none\",\n        name=\"admin_1_states_provinces_shp\",\n    )\n    ax.add_feature(states, linewidth=0.5, edgecolor=\"black\")\n    ax.coastlines(\"50m\", linewidth=0.8)\n    if idx_lst is not None:\n        if isinstance(marker_size, list):\n            assert len(marker_size) == len(idx_lst)\n        else:\n            marker_size = np.full(len(idx_lst), marker_size).tolist()\n        if not isinstance(marker_size, list):\n            markers = np.full(len(idx_lst), markers).tolist()\n        else:\n            assert len(markers) == len(idx_lst)\n        if not isinstance(cmap_str, list):\n            cmap_str = np.full(len(idx_lst), cmap_str).tolist()\n        else:\n            assert len(cmap_str) == len(idx_lst)\n        if is_discrete:\n            for i in range(len(idx_lst)):\n                ax.plot(\n                    lon[idx_lst[i]],\n                    lat[idx_lst[i]],\n                    marker=markers[i],\n                    ms=marker_size[i],\n                    label=category_names[i],\n                    c=colors[i],\n                    linestyle=\"\",\n                )\n                ax.legend(prop=dict(size=legend_font_size))\n        else:\n            scatter = []\n            for i in range(len(idx_lst)):\n                scat = ax.scatter(\n                    lon[idx_lst[i]],\n                    lat[idx_lst[i]],\n                    c=data[idx_lst[i]],\n                    marker=markers[i],\n                    s=marker_size[i],\n                    cmap=cmap_str[i],\n                    vmin=vmin,\n                    vmax=vmax,\n                )\n                scatter.append(scat)\n            if need_colorbar:\n                if colorbar_size is not None:\n                    cbar_ax = fig.add_axes(colorbar_size)\n                    cbar = fig.colorbar(scat, cax=cbar_ax, orientation=\"vertical\")\n                else:\n                    cbar = fig.colorbar(scat, ax=ax, pad=0.01)\n                if colorbar_font_size is not None:\n                    cbar.ax.tick_params(labelsize=colorbar_font_size)\n            if category_names is not None:\n                ax.legend(\n                    scatter, category_names, prop=dict(size=legend_font_size), ncol=2\n                )\n    elif is_discrete:\n        scatter = ax.scatter(lon, lat, c=data, s=marker_size)\n        # produce a legend with the unique colors from the scatter\n        legend1 = ax.legend(\n            *scatter.legend_elements(), loc=\"lower left\", title=\"Classes\"\n        )\n        ax.add_artist(legend1)\n    else:\n        scat = plt.scatter(\n            lon, lat, c=data, s=marker_size, cmap=cmap_str, vmin=vmin, vmax=vmax\n        )\n        if need_colorbar:\n            if colorbar_size is not None:\n                cbar_ax = fig.add_axes(colorbar_size)\n                cbar = fig.colorbar(scat, cax=cbar_ax, orientation=\"vertical\")\n            else:\n                cbar = fig.colorbar(scat, ax=ax, pad=0.01)\n            if colorbar_font_size is not None:\n                cbar.ax.tick_params(labelsize=colorbar_font_size)\n    return ax\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.plot_map_carto--continuous-data-example","title":"Continuous data example","text":"<p>data = np.random.rand(100) lat = np.random.uniform(30, 45, 100) lon = np.random.uniform(-120, -100, 100) ax = plot_map_carto(data, lat, lon, ...                    value_range=[0, 1], ...                    cmap_str='viridis')</p>"},{"location":"api/hydroutils/#hydroutils.plot_map_carto--discrete-categories-example","title":"Discrete categories example","text":"<p>categories = np.random.randint(0, 3, 100) ax = plot_map_carto(categories, lat, lon, ...                    is_discrete=True, ...                    category_names=['Low', 'Medium', 'High'])</p>"},{"location":"api/hydroutils/#hydroutils.plot_rainfall_runoff","title":"<code>plot_rainfall_runoff(t, p, qs, fig_size=(8, 6), c_lst='rbkgcmy', leg_lst=None, dash_lines=None, title=None, xlabel=None, ylabel=None, prcp_ylabel='prcp(mm/day)', linewidth=1, prcp_interval=20)</code>","text":"<p>Create a combined rainfall-runoff plot with dual axes.</p> <p>This function creates a figure with two synchronized axes: one for streamflow (primary) and one for precipitation (secondary, inverted). The precipitation is plotted as filled areas from the top, while streamflow lines are plotted normally.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Union[array, list]</code> <p>Time values. If list, must match length of qs.</p> required <code>p</code> <code>array</code> <p>Precipitation time series.</p> required <code>qs</code> <code>Union[array, list]</code> <p>Streamflow time series. Can be single array or list of arrays for multiple series.</p> required <code>fig_size</code> <code>tuple</code> <p>Figure size as (width, height). Defaults to (8, 6).</p> <code>(8, 6)</code> <code>c_lst</code> <code>str</code> <p>String of color characters for lines. Defaults to \"rbkgcmy\".</p> <code>'rbkgcmy'</code> <code>leg_lst</code> <code>list</code> <p>Legend labels for streamflow series. Defaults to None.</p> <code>None</code> <code>dash_lines</code> <code>list[bool]</code> <p>Which streamflow lines should be dashed. Defaults to None.</p> <code>None</code> <code>title</code> <code>str</code> <p>Plot title. Defaults to None.</p> <code>None</code> <code>xlabel</code> <code>str</code> <p>X-axis label. Defaults to None.</p> <code>None</code> <code>ylabel</code> <code>str</code> <p>Primary Y-axis label (streamflow). Defaults to None.</p> <code>None</code> <code>prcp_ylabel</code> <code>str</code> <p>Secondary Y-axis label (precipitation). Defaults to \"prcp(mm/day)\".</p> <code>'prcp(mm/day)'</code> <code>linewidth</code> <code>int</code> <p>Width of streamflow lines. Defaults to 1.</p> <code>1</code> <code>prcp_interval</code> <code>int</code> <p>Interval for precipitation Y-axis ticks. Defaults to 20.</p> <code>20</code> <p>Returns:</p> Type Description <p>Tuple[plt.Figure, plt.Axes]: The figure and primary axes objects.</p> Note <ul> <li>Precipitation is plotted from top with blue fill and 0.5 alpha</li> <li>Streamflow axis range is extended by 20% at top</li> <li>Legend is placed at upper left with fontsize 16</li> <li>Grid is enabled on primary (streamflow) axis</li> <li>All tick labels use fontsize 16</li> <li>Right and top spines are hidden</li> </ul> Example <p>t = np.arange(100) p = np.random.uniform(0, 10, 100)  # precipitation q1 = np.random.uniform(0, 100, 100)  # streamflow 1 q2 = np.random.uniform(0, 80, 100)   # streamflow 2 fig, ax = plot_rainfall_runoff(t, p, [q1, q2], ...                               leg_lst=['Obs', 'Sim'], ...                               ylabel='Streamflow (m\u00b3/s)')</p> Source code in <code>hydroutils\\hydro_plot.py</code> <pre><code>def plot_rainfall_runoff(\n    t,\n    p,\n    qs,\n    fig_size=(8, 6),\n    c_lst=\"rbkgcmy\",\n    leg_lst=None,\n    dash_lines=None,\n    title=None,\n    xlabel=None,\n    ylabel=None,\n    prcp_ylabel=\"prcp(mm/day)\",\n    linewidth=1,\n    prcp_interval=20,\n):\n    \"\"\"Create a combined rainfall-runoff plot with dual axes.\n\n    This function creates a figure with two synchronized axes: one for streamflow\n    (primary) and one for precipitation (secondary, inverted). The precipitation\n    is plotted as filled areas from the top, while streamflow lines are plotted\n    normally.\n\n    Args:\n        t (Union[np.array, list]): Time values. If list, must match length of qs.\n        p (np.array): Precipitation time series.\n        qs (Union[np.array, list]): Streamflow time series. Can be single array\n            or list of arrays for multiple series.\n        fig_size (tuple, optional): Figure size as (width, height).\n            Defaults to (8, 6).\n        c_lst (str, optional): String of color characters for lines.\n            Defaults to \"rbkgcmy\".\n        leg_lst (list, optional): Legend labels for streamflow series.\n            Defaults to None.\n        dash_lines (list[bool], optional): Which streamflow lines should be\n            dashed. Defaults to None.\n        title (str, optional): Plot title. Defaults to None.\n        xlabel (str, optional): X-axis label. Defaults to None.\n        ylabel (str, optional): Primary Y-axis label (streamflow).\n            Defaults to None.\n        prcp_ylabel (str, optional): Secondary Y-axis label (precipitation).\n            Defaults to \"prcp(mm/day)\".\n        linewidth (int, optional): Width of streamflow lines. Defaults to 1.\n        prcp_interval (int, optional): Interval for precipitation Y-axis ticks.\n            Defaults to 20.\n\n    Returns:\n        Tuple[plt.Figure, plt.Axes]: The figure and primary axes objects.\n\n    Note:\n        - Precipitation is plotted from top with blue fill and 0.5 alpha\n        - Streamflow axis range is extended by 20% at top\n        - Legend is placed at upper left with fontsize 16\n        - Grid is enabled on primary (streamflow) axis\n        - All tick labels use fontsize 16\n        - Right and top spines are hidden\n\n    Example:\n        &gt;&gt;&gt; t = np.arange(100)\n        &gt;&gt;&gt; p = np.random.uniform(0, 10, 100)  # precipitation\n        &gt;&gt;&gt; q1 = np.random.uniform(0, 100, 100)  # streamflow 1\n        &gt;&gt;&gt; q2 = np.random.uniform(0, 80, 100)   # streamflow 2\n        &gt;&gt;&gt; fig, ax = plot_rainfall_runoff(t, p, [q1, q2],\n        ...                               leg_lst=['Obs', 'Sim'],\n        ...                               ylabel='Streamflow (m\u00b3/s)')\n    \"\"\"\n    fig, ax = plt.subplots(figsize=fig_size)\n    if dash_lines is not None:\n        assert isinstance(dash_lines, list)\n    else:\n        dash_lines = np.full(len(qs), False).tolist()\n    for k in range(len(qs)):\n        tt = t[k] if type(t) is list else t\n        q = qs[k]\n        leg_str = None\n        if leg_lst is not None:\n            leg_str = leg_lst[k]\n        (line_i,) = ax.plot(tt, q, color=c_lst[k], label=leg_str, linewidth=linewidth)\n        if dash_lines[k]:\n            line_i.set_dashes([2, 2, 10, 2])\n\n    ax.set_ylim(ax.get_ylim()[0], ax.get_ylim()[1] * 1.2)\n    # Create second axes, in order to get the bars from the top you can multiply by -1\n    ax2 = ax.twinx()\n    # ax2.bar(tt, -p, color=\"b\")\n    ax2.fill_between(tt, 0, -p, step=\"mid\", color=\"b\", alpha=0.5)\n    # ax2.plot(tt, -p, color=\"b\", alpha=0.7, linewidth=1.5)\n\n    # Now need to fix the axis labels\n    # max_pre = max(p)\n    max_pre = p.max().item()\n    ax2.set_ylim(-max_pre * 5, 0)\n    y2_ticks = np.arange(0, max_pre, prcp_interval)\n    y2_ticklabels = [str(i) for i in y2_ticks]\n    ax2.set_yticks(-1 * y2_ticks)\n    ax2.set_yticklabels(y2_ticklabels, fontsize=16)\n    # ax2.set_yticklabels([lab.get_text()[1:] for lab in ax2.get_yticklabels()])\n    if title is not None:\n        ax.set_title(title, loc=\"center\", fontdict={\"fontsize\": 17})\n    if ylabel is not None:\n        ax.set_ylabel(ylabel, fontsize=18)\n    if xlabel is not None:\n        ax.set_xlabel(xlabel, fontsize=18)\n    ax2.set_ylabel(prcp_ylabel, fontsize=8, loc=\"top\")\n    # ax2.set_ylabel(\"precipitation (mm/day)\", fontsize=12, loc='top')\n    # https://github.com/matplotlib/matplotlib/issues/12318\n    ax.tick_params(axis=\"x\", labelsize=16)\n    ax.tick_params(axis=\"y\", labelsize=16)\n    ax.legend(bbox_to_anchor=(0.01, 0.85), loc=\"upper left\", fontsize=16)\n    ax.grid()\n    return fig, ax\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.plot_scatter_with_11line","title":"<code>plot_scatter_with_11line(x, y, point_color='blue', line_color='black', xlim=[0.0, 1.0], ylim=[0.0, 1.0], xlabel=None, ylabel=None)</code>","text":"<p>Create a scatter plot with a 1:1 line for comparing two variables.</p> <p>This function creates a scatter plot comparing two variables and adds a 1:1 line to show the perfect correlation line. The plot includes customizable colors, axis limits, and labels.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array</code> <p>First variable to plot (x-axis).</p> required <code>y</code> <code>array</code> <p>Second variable to plot (y-axis).</p> required <code>point_color</code> <code>str</code> <p>Color of scatter points. Defaults to \"blue\".</p> <code>'blue'</code> <code>line_color</code> <code>str</code> <p>Color of 1:1 line. Defaults to \"black\".</p> <code>'black'</code> <code>xlim</code> <code>list</code> <p>X-axis limits [min, max]. Defaults to [0.0, 1.0].</p> <code>[0.0, 1.0]</code> <code>ylim</code> <code>list</code> <p>Y-axis limits [min, max]. Defaults to [0.0, 1.0].</p> <code>[0.0, 1.0]</code> <code>xlabel</code> <code>str</code> <p>X-axis label. Defaults to None.</p> <code>None</code> <code>ylabel</code> <code>str</code> <p>Y-axis label. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>tuple[plt.Figure, plt.Axes]: Matplotlib figure and axes objects.</p> Note <ul> <li>The plot uses a whitesmoke background</li> <li>Right and top spines are hidden</li> <li>Tick labels use font size 16</li> <li>The 1:1 line is dashed</li> </ul> Example <p>x = np.array([0.1, 0.2, 0.3, 0.4]) y = np.array([0.15, 0.25, 0.35, 0.45]) fig, ax = plot_scatter_with_11line(x, y, ...                                    xlabel='Predicted', ...                                    ylabel='Observed')</p> Source code in <code>hydroutils\\hydro_plot.py</code> <pre><code>def plot_scatter_with_11line(\n    x: np.array,\n    y: np.array,\n    point_color=\"blue\",\n    line_color=\"black\",\n    xlim=[0.0, 1.0],\n    ylim=[0.0, 1.0],\n    xlabel=None,\n    ylabel=None,\n):\n    \"\"\"Create a scatter plot with a 1:1 line for comparing two variables.\n\n    This function creates a scatter plot comparing two variables and adds a 1:1\n    line to show the perfect correlation line. The plot includes customizable\n    colors, axis limits, and labels.\n\n    Args:\n        x (np.array): First variable to plot (x-axis).\n        y (np.array): Second variable to plot (y-axis).\n        point_color (str, optional): Color of scatter points. Defaults to \"blue\".\n        line_color (str, optional): Color of 1:1 line. Defaults to \"black\".\n        xlim (list, optional): X-axis limits [min, max]. Defaults to [0.0, 1.0].\n        ylim (list, optional): Y-axis limits [min, max]. Defaults to [0.0, 1.0].\n        xlabel (str, optional): X-axis label. Defaults to None.\n        ylabel (str, optional): Y-axis label. Defaults to None.\n\n    Returns:\n        tuple[plt.Figure, plt.Axes]: Matplotlib figure and axes objects.\n\n    Note:\n        - The plot uses a whitesmoke background\n        - Right and top spines are hidden\n        - Tick labels use font size 16\n        - The 1:1 line is dashed\n\n    Example:\n        &gt;&gt;&gt; x = np.array([0.1, 0.2, 0.3, 0.4])\n        &gt;&gt;&gt; y = np.array([0.15, 0.25, 0.35, 0.45])\n        &gt;&gt;&gt; fig, ax = plot_scatter_with_11line(x, y,\n        ...                                    xlabel='Predicted',\n        ...                                    ylabel='Observed')\n    \"\"\"\n    fig, ax = plt.subplots()\n    # set background color for ax\n    ax.set_facecolor(\"whitesmoke\")\n    # plot the grid of the figure\n    # plt.grid(color=\"whitesmoke\")\n    ax.scatter(x, y, c=point_color, s=10)\n    line = mlines.Line2D([0, 1], [0, 1], color=line_color, linestyle=\"--\")\n    transform = ax.transAxes\n    line.set_transform(transform)\n    ax.add_line(line)\n    ax.set_xlim(xlim)\n    ax.set_ylim(ylim)\n    plt.xticks(np.arange(xlim[0], xlim[1], 0.1), fontsize=16)\n    plt.yticks(np.arange(ylim[0], ylim[1], 0.1), fontsize=16)\n    # set xlable and ylabel\n    if xlabel is not None:\n        plt.xlabel(xlabel, fontsize=16)\n    if ylabel is not None:\n        plt.ylabel(ylabel, fontsize=16)\n    ax.spines[\"right\"].set_visible(False)\n    ax.spines[\"top\"].set_visible(False)\n    ax.spines[\"left\"].set_visible(False)\n    ax.spines[\"bottom\"].set_visible(False)\n    return fig, ax\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.plot_scatter_xyc","title":"<code>plot_scatter_xyc(x_label, x, y_label, y, c_label=None, c=None, size=20, is_reg=False, xlim=None, ylim=None, quadrant=None)</code>","text":"<p>scatter plot: x-y relationship with c as colorbar Parameters</p> <p>x_label : type description x : type description y_label : type description y : type description c_label : type, optional     description, by default None c : type, optional     description, by default None size : int, optional     size of points, by default 20 is_reg : bool, optional     description, by default False xlim : type, optional     description, by default None ylim : type, optional     description, by default None quadrant: list, optional     if it is not None, it should be a list like [0.0,0.0],     the first means we put a new axis in x=0.0, second for y=0.0,     so that we can build a 4-quadrant plot</p> Source code in <code>hydroutils\\hydro_plot.py</code> <pre><code>def plot_scatter_xyc(\n    x_label,\n    x,\n    y_label,\n    y,\n    c_label=None,\n    c=None,\n    size=20,\n    is_reg=False,\n    xlim=None,\n    ylim=None,\n    quadrant=None,\n):\n    \"\"\"\n    scatter plot: x-y relationship with c as colorbar\n    Parameters\n    ----------\n    x_label : _type_\n        _description_\n    x : _type_\n        _description_\n    y_label : _type_\n        _description_\n    y : _type_\n        _description_\n    c_label : _type_, optional\n        _description_, by default None\n    c : _type_, optional\n        _description_, by default None\n    size : int, optional\n        size of points, by default 20\n    is_reg : bool, optional\n        _description_, by default False\n    xlim : _type_, optional\n        _description_, by default None\n    ylim : _type_, optional\n        _description_, by default None\n    quadrant: list, optional\n        if it is not None, it should be a list like [0.0,0.0],\n        the first means we put a new axis in x=0.0, second for y=0.0,\n        so that we can build a 4-quadrant plot\n    \"\"\"\n    fig, ax = plt.subplots()\n    if type(x) is list:\n        for i in range(len(x)):\n            ax.plot(\n                x[i], y[i], marker=\"o\", linestyle=\"\", ms=size, label=c_label[i], c=c[i]\n            )\n        ax.legend()\n\n    elif c is None:\n        df = pd.DataFrame({x_label: x, y_label: y})\n        points = plt.scatter(df[x_label], df[y_label], s=size)\n        if quadrant is not None:\n            plt.axvline(quadrant[0], c=\"grey\", lw=1, linestyle=\"--\")\n            plt.axhline(quadrant[1], c=\"grey\", lw=1, linestyle=\"--\")\n            q2 = df[(df[x_label] &lt; 0) &amp; (df[y_label] &gt; 0)].shape[0]\n            q3 = df[(df[x_label] &lt; 0) &amp; (df[y_label] &lt; 0)].shape[0]\n            q4 = df[(df[x_label] &gt; 0) &amp; (df[y_label] &lt; 0)].shape[0]\n            q5 = df[(df[x_label] == 0) &amp; (df[y_label] == 0)].shape[0]\n            q1 = df[(df[x_label] &gt; 0) &amp; (df[y_label] &gt; 0)].shape[0]\n            q = q1 + q2 + q3 + q4 + q5\n            r1 = int(round(q1 / q, 2) * 100)\n            r2 = int(round(q2 / q, 2) * 100)\n            r3 = int(round(q3 / q, 2) * 100)\n            r4 = int(round(q4 / q, 2) * 100)\n            r5 = 100 - r1 - r2 - r3 - r4\n            plt.text(\n                xlim[1] - (xlim[1] - xlim[0]) * 0.1,\n                ylim[1] - (ylim[1] - ylim[0]) * 0.1,\n                f\"{r1}%\",\n                fontsize=16,\n            )\n            plt.text(\n                xlim[0] + (xlim[1] - xlim[0]) * 0.1,\n                ylim[1] - (ylim[1] - ylim[0]) * 0.1,\n                f\"{r2}%\",\n                fontsize=16,\n            )\n            plt.text(\n                xlim[0] + (xlim[1] - xlim[0]) * 0.1,\n                ylim[0] + (ylim[1] - ylim[0]) * 0.1,\n                f\"{r3}%\",\n                fontsize=16,\n            )\n            plt.text(\n                xlim[1] - (xlim[1] - xlim[0]) * 0.1,\n                ylim[0] + (ylim[1] - ylim[0]) * 0.1,\n                f\"{r4}%\",\n                fontsize=16,\n            )\n            plt.text(0.2, 0.02, f\"{str(r5)}%\", fontsize=16)\n    else:\n        df = pd.DataFrame({x_label: x, y_label: y, c_label: c})\n        points = plt.scatter(\n            df[x_label], df[y_label], c=df[c_label], s=size, cmap=\"Spectral\"\n        )  # set style options\n        # add a color bar\n        plt.colorbar(points)\n\n    # set limits\n    if xlim is not None:\n        plt.xlim(xlim[0], xlim[1])\n    if ylim is not None:\n        plt.ylim(ylim[0], ylim[1])\n    # Hide the right and top spines\n    ax.spines[\"right\"].set_visible(False)\n    ax.spines[\"top\"].set_visible(False)\n    # build the regression plot\n    if is_reg:\n        plot = sns.regplot(x_label, y_label, data=df, scatter=False)  # , color=\".1\"\n        plot = plot.set(xlabel=x_label, ylabel=y_label)  # add labels\n    else:\n        plt.xlabel(x_label, fontsize=18)\n        plt.ylabel(y_label, fontsize=18)\n        plt.xticks(fontsize=16)\n        plt.yticks(fontsize=16)\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.plot_ts","title":"<code>plot_ts(t, y, ax=None, t_bar=None, title=None, xlabel=None, ylabel=None, fig_size=(12, 4), c_lst='rbkgcmyrbkgcmyrbkgcmy', leg_lst=None, marker_lst=None, linewidth=2, linespec=None, dash_lines=None, alpha=1)</code>","text":"<p>Plot multiple time series with customizable styling.</p> <p>This function creates a time series plot that can handle multiple series, with extensive customization options for appearance and formatting. It supports both continuous lines and scatter plots, with optional vertical bars and legends.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Union[list, array]</code> <p>Time values. Can be dates, numbers, or a list of arrays (one per series).</p> required <code>y</code> <code>Union[list, array]</code> <p>Data values to plot. Can be a single array or list of arrays for multiple series.</p> required <code>ax</code> <code>Axes</code> <p>Existing axes to plot on. Defaults to None.</p> <code>None</code> <code>t_bar</code> <code>Union[float, list]</code> <p>Position(s) for vertical bars. Defaults to None.</p> <code>None</code> <code>title</code> <code>str</code> <p>Plot title. Defaults to None.</p> <code>None</code> <code>xlabel</code> <code>str</code> <p>X-axis label. Defaults to None.</p> <code>None</code> <code>ylabel</code> <code>str</code> <p>Y-axis label. Defaults to None.</p> <code>None</code> <code>fig_size</code> <code>tuple</code> <p>Figure size as (width, height). Defaults to (12, 4).</p> <code>(12, 4)</code> <code>c_lst</code> <code>str</code> <p>String of color characters for lines. Defaults to \"rbkgcmyrbkgcmyrbkgcmy\".</p> <code>'rbkgcmyrbkgcmyrbkgcmy'</code> <code>leg_lst</code> <code>list</code> <p>Legend labels for each series. Defaults to None.</p> <code>None</code> <code>marker_lst</code> <code>list</code> <p>Marker styles for each series. Defaults to None.</p> <code>None</code> <code>linewidth</code> <code>Union[int, list]</code> <p>Line width(s). Can be single value or list. Defaults to 2.</p> <code>2</code> <code>linespec</code> <code>list</code> <p>Line style specifications. Defaults to None.</p> <code>None</code> <code>dash_lines</code> <code>list[bool]</code> <p>Which lines should be dashed. Defaults to None.</p> <code>None</code> <code>alpha</code> <code>Union[float, list]</code> <p>Opacity value(s) between 0 and 1. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <p>Union[Tuple[plt.Figure, plt.Axes], plt.Axes]: If ax is None, returns</p> <p>(figure, axes), otherwise returns just the axes.</p> Note <ul> <li>Automatically handles NaN values by plotting points instead of lines</li> <li>Supports multiple line styles including solid, dashed, and markers</li> <li>Right and top spines are hidden for cleaner appearance</li> <li>Grid is enabled by default</li> <li>Font size is set to 16 for tick labels</li> <li>Legend is placed in upper right if provided</li> </ul> Example <p>t = np.arange(100) y1 = np.sin(t/10) y2 = np.cos(t/10) fig, ax = plot_ts(t, [y1, y2], ...                  leg_lst=['sin', 'cos'], ...                  xlabel='Time', ...                  ylabel='Value')</p> Source code in <code>hydroutils\\hydro_plot.py</code> <pre><code>def plot_ts(\n    t: Union[list, np.array],\n    y: Union[list, np.array],\n    ax=None,\n    t_bar=None,\n    title=None,\n    xlabel: str = None,\n    ylabel: str = None,\n    fig_size=(12, 4),\n    c_lst=\"rbkgcmyrbkgcmyrbkgcmy\",\n    leg_lst=None,\n    marker_lst=None,\n    linewidth=2,\n    linespec=None,\n    dash_lines=None,\n    alpha=1,\n):\n    \"\"\"Plot multiple time series with customizable styling.\n\n    This function creates a time series plot that can handle multiple series,\n    with extensive customization options for appearance and formatting. It supports\n    both continuous lines and scatter plots, with optional vertical bars and\n    legends.\n\n    Args:\n        t (Union[list, np.array]): Time values. Can be dates, numbers, or a list\n            of arrays (one per series).\n        y (Union[list, np.array]): Data values to plot. Can be a single array or\n            list of arrays for multiple series.\n        ax (matplotlib.axes.Axes, optional): Existing axes to plot on.\n            Defaults to None.\n        t_bar (Union[float, list], optional): Position(s) for vertical bars.\n            Defaults to None.\n        title (str, optional): Plot title. Defaults to None.\n        xlabel (str, optional): X-axis label. Defaults to None.\n        ylabel (str, optional): Y-axis label. Defaults to None.\n        fig_size (tuple, optional): Figure size as (width, height).\n            Defaults to (12, 4).\n        c_lst (str, optional): String of color characters for lines.\n            Defaults to \"rbkgcmyrbkgcmyrbkgcmy\".\n        leg_lst (list, optional): Legend labels for each series. Defaults to None.\n        marker_lst (list, optional): Marker styles for each series.\n            Defaults to None.\n        linewidth (Union[int, list], optional): Line width(s). Can be single value\n            or list. Defaults to 2.\n        linespec (list, optional): Line style specifications. Defaults to None.\n        dash_lines (list[bool], optional): Which lines should be dashed.\n            Defaults to None.\n        alpha (Union[float, list], optional): Opacity value(s) between 0 and 1.\n            Defaults to 1.\n\n    Returns:\n        Union[Tuple[plt.Figure, plt.Axes], plt.Axes]: If ax is None, returns\n        (figure, axes), otherwise returns just the axes.\n\n    Note:\n        - Automatically handles NaN values by plotting points instead of lines\n        - Supports multiple line styles including solid, dashed, and markers\n        - Right and top spines are hidden for cleaner appearance\n        - Grid is enabled by default\n        - Font size is set to 16 for tick labels\n        - Legend is placed in upper right if provided\n\n    Example:\n        &gt;&gt;&gt; t = np.arange(100)\n        &gt;&gt;&gt; y1 = np.sin(t/10)\n        &gt;&gt;&gt; y2 = np.cos(t/10)\n        &gt;&gt;&gt; fig, ax = plot_ts(t, [y1, y2],\n        ...                  leg_lst=['sin', 'cos'],\n        ...                  xlabel='Time',\n        ...                  ylabel='Value')\n    \"\"\"\n    is_new_fig = False\n    if ax is None:\n        fig = plt.figure(figsize=fig_size)\n        ax = fig.subplots()\n        is_new_fig = True\n    if dash_lines is not None:\n        assert isinstance(dash_lines, list)\n    else:\n        dash_lines = np.full(len(t), False).tolist()\n        # dash_lines[-1] = True\n    if type(y) is np.ndarray:\n        y = [y]\n    if type(linewidth) is not list:\n        linewidth = [linewidth] * len(y)\n    if type(alpha) is not list:\n        alpha = [alpha] * len(y)\n    for k in range(len(y)):\n        tt = t[k] if type(t) is list else t\n        yy = y[k]\n        leg_str = None\n        if leg_lst is not None:\n            leg_str = leg_lst[k]\n        if marker_lst is None:\n            (line_i,) = (\n                ax.plot(tt, yy, \"*\", color=c_lst[k], label=leg_str, alpha=alpha[k])\n                if True in np.isnan(yy)\n                else ax.plot(\n                    tt,\n                    yy,\n                    color=c_lst[k],\n                    label=leg_str,\n                    linewidth=linewidth[k],\n                    alpha=alpha[k],\n                )\n            )\n        elif marker_lst[k] == \"-\":\n            if linespec is not None:\n                (line_i,) = ax.plot(\n                    tt,\n                    yy,\n                    color=c_lst[k],\n                    label=leg_str,\n                    linestyle=linespec[k],\n                    lw=linewidth[k],\n                    alpha=alpha[k],\n                )\n            else:\n                (line_i,) = ax.plot(\n                    tt,\n                    yy,\n                    color=c_lst[k],\n                    label=leg_str,\n                    lw=linewidth[k],\n                    alpha=alpha[k],\n                )\n        else:\n            (line_i,) = ax.plot(\n                tt,\n                yy,\n                color=c_lst[k],\n                label=leg_str,\n                marker=marker_lst[k],\n                lw=linewidth[k],\n                alpha=alpha[k],\n            )\n        if dash_lines[k]:\n            line_i.set_dashes([2, 2, 10, 2])\n        if ylabel is not None:\n            ax.set_ylabel(ylabel, fontsize=18)\n        if xlabel is not None:\n            ax.set_xlabel(xlabel, fontsize=18)\n    if t_bar is not None:\n        ylim = ax.get_ylim()\n        t_bar = [t_bar] if type(t_bar) is not list else t_bar\n        for tt in t_bar:\n            ax.plot([tt, tt], ylim, \"-k\")\n\n    if leg_lst is not None:\n        ax.legend(loc=\"upper right\", frameon=False)\n        plt.legend(prop={\"size\": 16})\n    if title is not None:\n        ax.set_title(title, loc=\"center\", fontdict={\"fontsize\": 17})\n    # plot the grid of the figure\n    plt.grid()\n    plt.xticks(fontsize=16)\n    plt.yticks(fontsize=16)\n    # Hide the right and top spines\n    ax.spines[\"right\"].set_visible(False)\n    ax.spines[\"top\"].set_visible(False)\n    plt.tight_layout()\n    return (fig, ax) if is_new_fig else ax\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.plot_unit_hydrograph","title":"<code>plot_unit_hydrograph(U_optimized, title, smoothing_factor=None, peak_violation_weight=None, delta_t_hours=3.0)</code>","text":"<p>Create a unit hydrograph plot with optimization parameters.</p> <p>This function visualizes a unit hydrograph (UH) as a line plot with markers. If optimization parameters are provided, they are included in the title. The plot includes a grid and appropriate axis labels.</p> <p>Parameters:</p> Name Type Description Default <code>U_optimized</code> <code>ndarray</code> <p>Optimized unit hydrograph ordinates.</p> required <code>title</code> <code>str</code> <p>Base title for the plot.</p> required <code>smoothing_factor</code> <code>float</code> <p>Smoothing factor used in optimization. Defaults to None.</p> <code>None</code> <code>peak_violation_weight</code> <code>float</code> <p>Weight for peak violation penalty in optimization. Defaults to None.</p> <code>None</code> <code>delta_t_hours</code> <code>float</code> <p>Time step in hours. Defaults to 3.0.</p> <code>3.0</code> Note <ul> <li>Uses markers ('o') at each UH ordinate</li> <li>Includes dashed grid lines with 0.7 alpha</li> <li>X-axis shows time in hours</li> <li>Y-axis shows UH ordinates in mm/3h</li> <li>If optimization parameters are provided, they are shown in parentheses   after the title</li> </ul> Example <p>uh = np.array([0.1, 0.3, 0.4, 0.2, 0.0]) plot_unit_hydrograph(uh, \"Test Basin UH\", ...                     smoothing_factor=0.1, ...                     peak_violation_weight=0.5)</p> Source code in <code>hydroutils\\hydro_plot.py</code> <pre><code>def plot_unit_hydrograph(\n    U_optimized,\n    title,\n    smoothing_factor=None,\n    peak_violation_weight=None,\n    delta_t_hours=3.0,\n):\n    \"\"\"Create a unit hydrograph plot with optimization parameters.\n\n    This function visualizes a unit hydrograph (UH) as a line plot with markers.\n    If optimization parameters are provided, they are included in the title.\n    The plot includes a grid and appropriate axis labels.\n\n    Args:\n        U_optimized (np.ndarray): Optimized unit hydrograph ordinates.\n        title (str): Base title for the plot.\n        smoothing_factor (float, optional): Smoothing factor used in optimization.\n            Defaults to None.\n        peak_violation_weight (float, optional): Weight for peak violation penalty\n            in optimization. Defaults to None.\n        delta_t_hours (float, optional): Time step in hours. Defaults to 3.0.\n\n    Note:\n        - Uses markers ('o') at each UH ordinate\n        - Includes dashed grid lines with 0.7 alpha\n        - X-axis shows time in hours\n        - Y-axis shows UH ordinates in mm/3h\n        - If optimization parameters are provided, they are shown in parentheses\n          after the title\n\n    Example:\n        &gt;&gt;&gt; uh = np.array([0.1, 0.3, 0.4, 0.2, 0.0])\n        &gt;&gt;&gt; plot_unit_hydrograph(uh, \"Test Basin UH\",\n        ...                     smoothing_factor=0.1,\n        ...                     peak_violation_weight=0.5)\n    \"\"\"\n    if U_optimized is None:\n        print(f\"\u26a0\ufe0f \u65e0\u6cd5\u7ed8\u5236\u5355\u4f4d\u7ebf\uff1a{title} - \u4f18\u5316\u5931\u8d25\")\n        return\n\n    time_axis_uh = np.arange(1, len(U_optimized) + 1) * delta_t_hours\n\n    plt.figure(figsize=(12, 6))\n    plt.plot(time_axis_uh, U_optimized, marker=\"o\", linestyle=\"-\")\n\n    # \u6784\u5efa\u5b8c\u6574\u6807\u9898\n    full_title = title\n    if smoothing_factor is not None and peak_violation_weight is not None:\n        full_title += f\" (\u5e73\u6ed1={smoothing_factor}, \u5355\u5cf0\u7f5a={peak_violation_weight})\"\n\n    plt.title(full_title)\n    plt.xlabel(f\"\u65f6\u95f4 (\u5c0f\u65f6, \u0394t={delta_t_hours}h)\")\n    plt.ylabel(\"1mm\u51c0\u96e8\u5355\u4f4d\u7ebf\u7eb5\u5750\u6807 (mm/3h)\")\n    plt.grid(True, linestyle=\"--\", alpha=0.7)\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.remove_abnormal_data","title":"<code>remove_abnormal_data(data, *, q1=1e-05, q2=0.99999)</code>","text":"<p>Remove extreme values from data using quantile thresholds.</p> <p>This function removes data points that fall outside specified quantile ranges by replacing them with NaN values. This is useful for removing outliers or extreme values that might affect analysis.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input data array.</p> required <code>q1</code> <code>float</code> <p>Lower quantile threshold. Values below this quantile will be replaced with NaN. Defaults to 0.00001.</p> <code>1e-05</code> <code>q2</code> <code>float</code> <p>Upper quantile threshold. Values above this quantile will be replaced with NaN. Defaults to 0.99999.</p> <code>0.99999</code> <p>Returns:</p> Type Description <p>np.ndarray: Data array with extreme values replaced by NaN.</p> Note <ul> <li>Uses numpy.quantile for threshold calculation</li> <li>Values equal to thresholds are kept</li> <li>Original array shape is preserved</li> <li>NaN values in input are preserved</li> <li>Default thresholds keep 99.998% of data</li> </ul> Example <p>data = np.array([1, 2, 3, 100, 4, 5, 0.001, 6]) cleaned = remove_abnormal_data(data, q1=0.1, q2=0.9) print(cleaned) array([nan,  2.,  3.,  nan,  4.,  5.,  nan,  6.])</p> Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def remove_abnormal_data(data, *, q1=0.00001, q2=0.99999):\n    \"\"\"Remove extreme values from data using quantile thresholds.\n\n    This function removes data points that fall outside specified quantile\n    ranges by replacing them with NaN values. This is useful for removing\n    outliers or extreme values that might affect analysis.\n\n    Args:\n        data (np.ndarray): Input data array.\n        q1 (float, optional): Lower quantile threshold. Values below this\n            quantile will be replaced with NaN. Defaults to 0.00001.\n        q2 (float, optional): Upper quantile threshold. Values above this\n            quantile will be replaced with NaN. Defaults to 0.99999.\n\n    Returns:\n        np.ndarray: Data array with extreme values replaced by NaN.\n\n    Note:\n        - Uses numpy.quantile for threshold calculation\n        - Values equal to thresholds are kept\n        - Original array shape is preserved\n        - NaN values in input are preserved\n        - Default thresholds keep 99.998% of data\n\n    Example:\n        &gt;&gt;&gt; data = np.array([1, 2, 3, 100, 4, 5, 0.001, 6])\n        &gt;&gt;&gt; cleaned = remove_abnormal_data(data, q1=0.1, q2=0.9)\n        &gt;&gt;&gt; print(cleaned)\n        array([nan,  2.,  3.,  nan,  4.,  5.,  nan,  6.])\n    \"\"\"\n    # remove abnormal data\n    data[data &lt; np.quantile(data, q1)] = np.nan\n    data[data &gt; np.quantile(data, q2)] = np.nan\n    return data\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.serialize_json","title":"<code>serialize_json(my_dict, my_file, encoding='utf-8', ensure_ascii=True)</code>","text":"<p>Serialize a dictionary to a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>my_dict</code> <code>dict</code> <p>Dictionary to serialize.</p> required <code>my_file</code> <code>str</code> <p>Path to the output JSON file.</p> required <code>encoding</code> <code>str</code> <p>File encoding. Defaults to \"utf-8\".</p> <code>'utf-8'</code> <code>ensure_ascii</code> <code>bool</code> <p>If True, ensure ASCII output. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def serialize_json(my_dict, my_file, encoding=\"utf-8\", ensure_ascii=True):\n    \"\"\"Serialize a dictionary to a JSON file.\n\n    Args:\n        my_dict (dict): Dictionary to serialize.\n        my_file (str): Path to the output JSON file.\n        encoding (str, optional): File encoding. Defaults to \"utf-8\".\n        ensure_ascii (bool, optional): If True, ensure ASCII output. Defaults to True.\n\n    Returns:\n        None\n    \"\"\"\n    with open(my_file, \"w\", encoding=encoding) as FP:\n        json.dump(my_dict, FP, ensure_ascii=ensure_ascii, indent=4)\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.serialize_json_np","title":"<code>serialize_json_np(my_dict, my_file)</code>","text":"<p>Serialize a dictionary containing NumPy arrays to a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>my_dict</code> <code>dict</code> <p>Dictionary containing NumPy arrays to serialize.</p> required <code>my_file</code> <code>str</code> <p>Path to the output JSON file.</p> required <p>Returns:</p> Type Description <p>None</p> Note <p>Uses NumpyArrayEncoder to handle NumPy types.</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def serialize_json_np(my_dict, my_file):\n    \"\"\"Serialize a dictionary containing NumPy arrays to a JSON file.\n\n    Args:\n        my_dict (dict): Dictionary containing NumPy arrays to serialize.\n        my_file (str): Path to the output JSON file.\n\n    Returns:\n        None\n\n    Note:\n        Uses NumpyArrayEncoder to handle NumPy types.\n    \"\"\"\n    with open(my_file, \"w\") as FP:\n        json.dump(my_dict, FP, cls=NumpyArrayEncoder)\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.serialize_numpy","title":"<code>serialize_numpy(my_array, my_file)</code>","text":"<p>Save a NumPy array to a binary file.</p> <p>Parameters:</p> Name Type Description Default <code>my_array</code> <code>ndarray</code> <p>NumPy array to save.</p> required <code>my_file</code> <code>str</code> <p>Path to the output file.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def serialize_numpy(my_array, my_file):\n    \"\"\"Save a NumPy array to a binary file.\n\n    Args:\n        my_array (np.ndarray): NumPy array to save.\n        my_file (str): Path to the output file.\n\n    Returns:\n        None\n    \"\"\"\n    np.save(my_file, my_array)\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.serialize_pickle","title":"<code>serialize_pickle(my_object, my_file)</code>","text":"<p>Serialize an object to a pickle file.</p> <p>Parameters:</p> Name Type Description Default <code>my_object</code> <code>object</code> <p>Python object to serialize.</p> required <code>my_file</code> <code>str</code> <p>Path to the output pickle file.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def serialize_pickle(my_object, my_file):\n    \"\"\"Serialize an object to a pickle file.\n\n    Args:\n        my_object (object): Python object to serialize.\n        my_file (str): Path to the output pickle file.\n\n    Returns:\n        None\n    \"\"\"\n    with open(my_file, \"wb\") as f:\n        pickle.dump(my_object, f)\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.setup_matplotlib_chinese","title":"<code>setup_matplotlib_chinese()</code>","text":"<p>Configure matplotlib for Chinese font support and math rendering.</p> <p>This function sets up matplotlib to properly display Chinese characters and mathematical expressions. It configures the font family, handles negative signs, and sets up math rendering to work harmoniously with Chinese text.</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if configuration was successful, False if there was an error (usually due to missing SimHei font).</p> Note <ul> <li>Uses SimHei as the primary Chinese font</li> <li>Sets up STIX fonts for math rendering to match Times New Roman</li> <li>Handles negative sign display in Chinese context</li> <li>Configures sans-serif as the default font family</li> </ul> Example <p>if setup_matplotlib_chinese(): ...     plt.title(\"\u4e2d\u6587\u6807\u9898\") ...     plt.xlabel(\"\u65f6\u95f4 (s)\") ... else: ...     print(\"Chinese font setup failed\")</p> Source code in <code>hydroutils\\hydro_plot.py</code> <pre><code>def setup_matplotlib_chinese():\n    \"\"\"Configure matplotlib for Chinese font support and math rendering.\n\n    This function sets up matplotlib to properly display Chinese characters and\n    mathematical expressions. It configures the font family, handles negative signs,\n    and sets up math rendering to work harmoniously with Chinese text.\n\n    Returns:\n        bool: True if configuration was successful, False if there was an error\n            (usually due to missing SimHei font).\n\n    Note:\n        - Uses SimHei as the primary Chinese font\n        - Sets up STIX fonts for math rendering to match Times New Roman\n        - Handles negative sign display in Chinese context\n        - Configures sans-serif as the default font family\n\n    Example:\n        &gt;&gt;&gt; if setup_matplotlib_chinese():\n        ...     plt.title(\"\u4e2d\u6587\u6807\u9898\")\n        ...     plt.xlabel(\"\u65f6\u95f4 (s)\")\n        ... else:\n        ...     print(\"Chinese font setup failed\")\n    \"\"\"\n    try:\n        plt.rcParams[\"font.sans-serif\"] = [\"SimHei\"]\n        plt.rcParams[\"axes.unicode_minus\"] = False\n        # --- \u65b0\u589e\uff1a\u4e3aLaTeX\u6570\u5b66\u516c\u5f0f\u6e32\u67d3\u914d\u7f6e\u5b57\u4f53 ---\n        # \u8fd9\u53ef\u4ee5\u5e2e\u52a9\u786e\u4fdd\u6570\u5b66\u7b26\u53f7\u548c\u4e2d\u6587\u5b57\u4f53\u770b\u8d77\u6765\u66f4\u548c\u8c10\n        plt.rcParams[\"mathtext.fontset\"] = (\n            \"stix\"  # 'stix' \u662f\u4e00\u79cd\u4e0eTimes New Roman\u76f8\u4f3c\u7684\u79d1\u5b66\u5b57\u4f53\n        )\n        plt.rcParams[\"font.family\"] = \"sans-serif\"  # \u4fdd\u6301\u5176\u4ed6\u6587\u672c\u4e3a\u65e0\u886c\u7ebf\u5b57\u4f53\n        return True\n    except Exception as e:\n        warnings.warn(\n            f\"Warning: Chinese font 'SimHei' not found, Chinese text may not display correctly. Error: {e}\"\n        )\n        return False\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.stat_error","title":"<code>stat_error(target, pred, fill_nan='no')</code>","text":"<p>Calculate statistical metrics for 2D arrays with NaN handling options.</p> <p>This function computes multiple statistical metrics comparing predicted values against target (observed) values for multiple time series (e.g., multiple basins). It provides different options for handling NaN values.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>ndarray</code> <p>Target (observed) values. 2D array [basin, sequence].</p> required <code>pred</code> <code>ndarray</code> <p>Predicted values. Same shape as target.</p> required <code>fill_nan</code> <code>str</code> <p>Method for handling NaN values. Options: - \"no\": Ignore NaN values (default) - \"sum\": Sum values in NaN locations - \"mean\": Average values in NaN locations</p> <code>'no'</code> <p>Returns:</p> Type Description <code>Union[Dict[str, ndarray], Dict[str, List[float]]]</code> <p>Union[Dict[str, np.ndarray], Dict[str, List[float]]]: Dictionary with metrics: - Bias: Mean error - RMSE: Root mean square error - ubRMSE: Unbiased root mean square error - Corr: Pearson correlation coefficient - R2: Coefficient of determination - NSE: Nash-Sutcliffe efficiency - KGE: Kling-Gupta efficiency - FHV: Peak flow bias (top 2%) - FLV: Low flow bias (bottom 30%)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input arrays have wrong dimensions or incompatible shapes.</p> Note <p>For fill_nan options: - \"no\": [1, nan, nan, 2] vs [0.3, 0.3, 0.3, 1.5] becomes [1, 2] vs [0.3, 1.5] - \"sum\": [1, nan, nan, 2] vs [0.3, 0.3, 0.3, 1.5] becomes [1, 2] vs [0.9, 1.5] - \"mean\": Similar to \"sum\" but takes average instead of sum</p> Example <p>target = np.array([[1.0, np.nan, np.nan, 2.0], ...                    [3.0, 4.0, np.nan, 6.0]]) pred = np.array([[1.1, 0.3, 0.3, 1.9], ...                  [3.2, 3.8, 0.5, 5.8]]) metrics = stat_error(target, pred, fill_nan=\"sum\") print(metrics['RMSE'])  # Example output array([0.158, 0.245])</p> Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def stat_error(\n    target: np.ndarray, pred: np.ndarray, fill_nan: str = \"no\"\n) -&gt; Union[Dict[str, np.ndarray], Dict[str, List[float]]]:\n    \"\"\"Calculate statistical metrics for 2D arrays with NaN handling options.\n\n    This function computes multiple statistical metrics comparing predicted values\n    against target (observed) values for multiple time series (e.g., multiple\n    basins). It provides different options for handling NaN values.\n\n    Args:\n        target (np.ndarray): Target (observed) values. 2D array [basin, sequence].\n        pred (np.ndarray): Predicted values. Same shape as target.\n        fill_nan (str, optional): Method for handling NaN values. Options:\n            - \"no\": Ignore NaN values (default)\n            - \"sum\": Sum values in NaN locations\n            - \"mean\": Average values in NaN locations\n\n    Returns:\n        Union[Dict[str, np.ndarray], Dict[str, List[float]]]: Dictionary with metrics:\n            - Bias: Mean error\n            - RMSE: Root mean square error\n            - ubRMSE: Unbiased root mean square error\n            - Corr: Pearson correlation coefficient\n            - R2: Coefficient of determination\n            - NSE: Nash-Sutcliffe efficiency\n            - KGE: Kling-Gupta efficiency\n            - FHV: Peak flow bias (top 2%)\n            - FLV: Low flow bias (bottom 30%)\n\n    Raises:\n        ValueError: If input arrays have wrong dimensions or incompatible shapes.\n\n    Note:\n        For fill_nan options:\n        - \"no\": [1, nan, nan, 2] vs [0.3, 0.3, 0.3, 1.5] becomes [1, 2] vs [0.3, 1.5]\n        - \"sum\": [1, nan, nan, 2] vs [0.3, 0.3, 0.3, 1.5] becomes [1, 2] vs [0.9, 1.5]\n        - \"mean\": Similar to \"sum\" but takes average instead of sum\n\n    Example:\n        &gt;&gt;&gt; target = np.array([[1.0, np.nan, np.nan, 2.0],\n        ...                    [3.0, 4.0, np.nan, 6.0]])\n        &gt;&gt;&gt; pred = np.array([[1.1, 0.3, 0.3, 1.9],\n        ...                  [3.2, 3.8, 0.5, 5.8]])\n        &gt;&gt;&gt; metrics = stat_error(target, pred, fill_nan=\"sum\")\n        &gt;&gt;&gt; print(metrics['RMSE'])  # Example output\n        array([0.158, 0.245])\n    \"\"\"\n    if len(target.shape) == 3:\n        raise ValueError(\n            \"The input data should be 2-dim, not 3-dim. If you want to calculate metrics for 3-d arrays, please use stat_errors function.\"\n        )\n    if type(fill_nan) is not str:\n        raise ValueError(\"fill_nan should be a string.\")\n    if target.shape != pred.shape:\n        raise ValueError(\"The shape of target and pred should be the same.\")\n    if fill_nan != \"no\":\n        each_non_nan_idx = []\n        all_non_nan_idx: list[int] = []\n        for i in range(target.shape[0]):\n            tmp = target[i]\n            non_nan_idx_tmp = [j for j in range(tmp.size) if not np.isnan(tmp[j])]\n            each_non_nan_idx.append(non_nan_idx_tmp)\n            # TODO: now all_non_nan_idx is only set for ET, because of its irregular nan values\n            all_non_nan_idx = all_non_nan_idx + non_nan_idx_tmp\n            non_nan_idx = np.unique(all_non_nan_idx).tolist()\n        # some NaN data appear in different dates in different basins, so we have to calculate the metric for each basin\n        # but for ET, it is not very resonable to calculate the metric for each basin in this way, for example,\n        # the non_nan_idx: [1, 9, 17, 33, 41], then there are 16 elements in 17 -&gt; 33, so use all_non_nan_idx is better\n        # hence we don't use each_non_nan_idx finally\n        out_dict: Dict[str, List[float]] = dict(\n            Bias=[],\n            RMSE=[],\n            ubRMSE=[],\n            Corr=[],\n            R2=[],\n            NSE=[],\n            KGE=[],\n            FHV=[],\n            FLV=[],\n        )\n    if fill_nan == \"sum\":\n        for i in range(target.shape[0]):\n            tmp = target[i]\n            # non_nan_idx = each_non_nan_idx[i]\n            targ_i = tmp[non_nan_idx]\n            pred_i = np.add.reduceat(pred[i], non_nan_idx)\n            dict_i = stat_error_i(targ_i, pred_i)\n            out_dict[\"Bias\"].append(dict_i[\"Bias\"])\n            out_dict[\"RMSE\"].append(dict_i[\"RMSE\"])\n            out_dict[\"ubRMSE\"].append(dict_i[\"ubRMSE\"])\n            out_dict[\"Corr\"].append(dict_i[\"Corr\"])\n            out_dict[\"R2\"].append(dict_i[\"R2\"])\n            out_dict[\"NSE\"].append(dict_i[\"NSE\"])\n            out_dict[\"KGE\"].append(dict_i[\"KGE\"])\n            out_dict[\"FHV\"].append(dict_i[\"FHV\"])\n            out_dict[\"FLV\"].append(dict_i[\"FLV\"])\n        return out_dict\n    elif fill_nan == \"mean\":\n        for i in range(target.shape[0]):\n            tmp = target[i]\n            # non_nan_idx = each_non_nan_idx[i]\n            targ_i = tmp[non_nan_idx]\n            pred_i_sum = np.add.reduceat(pred[i], non_nan_idx)\n            if non_nan_idx[-1] &lt; len(pred[i]):\n                idx4mean = non_nan_idx + [len(pred[i])]\n            else:\n                idx4mean = copy.copy(non_nan_idx)\n            idx_interval = [y - x for x, y in zip(idx4mean, idx4mean[1:])]\n            pred_i = pred_i_sum / idx_interval\n            dict_i = stat_error_i(targ_i, pred_i)\n            out_dict[\"Bias\"].append(dict_i[\"Bias\"])\n            out_dict[\"RMSE\"].append(dict_i[\"RMSE\"])\n            out_dict[\"ubRMSE\"].append(dict_i[\"ubRMSE\"])\n            out_dict[\"Corr\"].append(dict_i[\"Corr\"])\n            out_dict[\"R2\"].append(dict_i[\"R2\"])\n            out_dict[\"NSE\"].append(dict_i[\"NSE\"])\n            out_dict[\"KGE\"].append(dict_i[\"KGE\"])\n            out_dict[\"FHV\"].append(dict_i[\"FHV\"])\n            out_dict[\"FLV\"].append(dict_i[\"FLV\"])\n        return out_dict\n    ngrid, nt = pred.shape\n    # Bias\n    Bias = np.nanmean(pred - target, axis=1)\n    # RMSE\n    RMSE = np.sqrt(np.nanmean((pred - target) ** 2, axis=1))\n    # ubRMSE\n    predMean = np.tile(np.nanmean(pred, axis=1), (nt, 1)).transpose()\n    targetMean = np.tile(np.nanmean(target, axis=1), (nt, 1)).transpose()\n    predAnom = pred - predMean\n    targetAnom = target - targetMean\n    ubRMSE = np.sqrt(np.nanmean((predAnom - targetAnom) ** 2, axis=1))\n    # rho R2 NSE\n    Corr = np.full(ngrid, np.nan)\n    R2 = np.full(ngrid, np.nan)\n    NSE = np.full(ngrid, np.nan)\n    KGe = np.full(ngrid, np.nan)\n    PBiaslow = np.full(ngrid, np.nan)\n    PBiashigh = np.full(ngrid, np.nan)\n    PBias = np.full(ngrid, np.nan)\n    num_lowtarget_zero = 0\n    for k in range(ngrid):\n        x = pred[k, :]\n        y = target[k, :]\n        ind = np.where(np.logical_and(~np.isnan(x), ~np.isnan(y)))[0]\n        if ind.shape[0] &gt; 0:\n            xx = x[ind]\n            yy = y[ind]\n            # percent bias\n            PBias[k] = np.sum(xx - yy) / np.sum(yy) * 100\n            if ind.shape[0] &gt; 1:\n                # Theoretically at least two points for correlation\n                Corr[k] = scipy.stats.pearsonr(xx, yy)[0]\n                yymean = yy.mean()\n                SST: float = np.sum((yy - yymean) ** 2)\n                SSReg: float = np.sum((xx - yymean) ** 2)\n                SSRes: float = np.sum((yy - xx) ** 2)\n                R2[k] = 1 - SSRes / SST\n                NSE[k] = 1 - SSRes / SST\n                KGe[k] = KGE(xx, yy)\n            # FHV the peak flows bias 2%\n            # FLV the low flows bias bottom 30%, log space\n            pred_sort = np.sort(xx)\n            target_sort = np.sort(yy)\n            indexlow = round(0.3 * len(pred_sort))\n            indexhigh = round(0.98 * len(pred_sort))\n            lowpred = pred_sort[:indexlow]\n            highpred = pred_sort[indexhigh:]\n            lowtarget = target_sort[:indexlow]\n            hightarget = target_sort[indexhigh:]\n            if np.sum(lowtarget) == 0:\n                num_lowtarget_zero = num_lowtarget_zero + 1\n            with warnings.catch_warnings():\n                # Sometimes the lowtarget is all 0, which will cause a warning\n                # but I know it is not an error, so I ignore it\n                warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n                PBiaslow[k] = np.sum(lowpred - lowtarget) / np.sum(lowtarget) * 100\n            PBiashigh[k] = np.sum(highpred - hightarget) / np.sum(hightarget) * 100\n    outDict = dict(\n        Bias=Bias,\n        RMSE=RMSE,\n        ubRMSE=ubRMSE,\n        Corr=Corr,\n        R2=R2,\n        NSE=NSE,\n        KGE=KGe,\n        FHV=PBiashigh,\n        FLV=PBiaslow,\n    )\n    # \"The CDF of BFLV will not reach 1.0 because some basins have all zero flow observations for the \"\n    # \"30% low flow interval, the percent bias can be infinite\\n\"\n    # \"The number of these cases is \" + str(num_lowtarget_zero)\n    return outDict\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.stat_error_i","title":"<code>stat_error_i(targ_i, pred_i)</code>","text":"<p>Calculate multiple statistical metrics for one-dimensional arrays.</p> <p>This function computes a comprehensive set of statistical metrics comparing predicted values against target (observed) values. It handles NaN values and requires at least two valid data points for correlation-based metrics.</p> <p>Parameters:</p> Name Type Description Default <code>targ_i</code> <code>ndarray</code> <p>Target (observed) values.</p> required <code>pred_i</code> <code>ndarray</code> <p>Predicted values.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: Dictionary containing the following metrics: - Bias: Mean error - RMSE: Root mean square error - ubRMSE: Unbiased root mean square error - Corr: Pearson correlation coefficient - R2: Coefficient of determination - NSE: Nash-Sutcliffe efficiency - KGE: Kling-Gupta efficiency - FHV: Peak flow bias (top 2%) - FLV: Low flow bias (bottom 30%)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If there are fewer than 2 valid data points for correlation.</p> Note <ul> <li>NaN values are automatically handled (removed from calculations)</li> <li>FHV and FLV are calculated in percentage</li> <li>All metrics are calculated on valid (non-NaN) data points only</li> </ul> Example <p>target = np.array([1.0, 2.0, 3.0, np.nan, 5.0]) predicted = np.array([1.1, 2.2, 2.9, np.nan, 4.8]) metrics = stat_error_i(target, predicted) print(metrics['RMSE'])  # Example output 0.173</p> Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def stat_error_i(targ_i: np.ndarray, pred_i: np.ndarray) -&gt; Dict[str, float]:\n    \"\"\"Calculate multiple statistical metrics for one-dimensional arrays.\n\n    This function computes a comprehensive set of statistical metrics comparing\n    predicted values against target (observed) values. It handles NaN values\n    and requires at least two valid data points for correlation-based metrics.\n\n    Args:\n        targ_i (np.ndarray): Target (observed) values.\n        pred_i (np.ndarray): Predicted values.\n\n    Returns:\n        Dict[str, float]: Dictionary containing the following metrics:\n            - Bias: Mean error\n            - RMSE: Root mean square error\n            - ubRMSE: Unbiased root mean square error\n            - Corr: Pearson correlation coefficient\n            - R2: Coefficient of determination\n            - NSE: Nash-Sutcliffe efficiency\n            - KGE: Kling-Gupta efficiency\n            - FHV: Peak flow bias (top 2%)\n            - FLV: Low flow bias (bottom 30%)\n\n    Raises:\n        ValueError: If there are fewer than 2 valid data points for correlation.\n\n    Note:\n        - NaN values are automatically handled (removed from calculations)\n        - FHV and FLV are calculated in percentage\n        - All metrics are calculated on valid (non-NaN) data points only\n\n    Example:\n        &gt;&gt;&gt; target = np.array([1.0, 2.0, 3.0, np.nan, 5.0])\n        &gt;&gt;&gt; predicted = np.array([1.1, 2.2, 2.9, np.nan, 4.8])\n        &gt;&gt;&gt; metrics = stat_error_i(target, predicted)\n        &gt;&gt;&gt; print(metrics['RMSE'])  # Example output\n        0.173\n    \"\"\"\n    ind = np.where(np.logical_and(~np.isnan(pred_i), ~np.isnan(targ_i)))[0]\n    # Theoretically at least two points for correlation\n    if ind.shape[0] &gt; 1:\n        xx = pred_i[ind]\n        yy = targ_i[ind]\n        bias = he.me(xx, yy)\n        # RMSE\n        rmse = he.rmse(xx, yy)\n        # ubRMSE\n        pred_mean = np.nanmean(xx)\n        target_mean = np.nanmean(yy)\n        pred_anom = xx - pred_mean\n        target_anom = yy - target_mean\n        ubrmse = np.sqrt(np.nanmean((pred_anom - target_anom) ** 2))\n        # rho R2 NSE\n        corr = he.pearson_r(xx, yy)\n        r2 = he.r_squared(xx, yy)\n        nse = he.nse(xx, yy)\n        kge = he.kge_2009(xx, yy)\n        # percent bias\n        pbias = np.sum(xx - yy) / np.sum(yy) * 100\n        # FHV the peak flows bias 2%\n        # FLV the low flows bias bottom 30%, log space\n        pred_sort = np.sort(xx)\n        target_sort = np.sort(yy)\n        indexlow = round(0.3 * len(pred_sort))\n        indexhigh = round(0.98 * len(pred_sort))\n        lowpred = pred_sort[:indexlow]\n        highpred = pred_sort[indexhigh:]\n        lowtarget = target_sort[:indexlow]\n        hightarget = target_sort[indexhigh:]\n        pbiaslow = np.sum(lowpred - lowtarget) / np.sum(lowtarget) * 100\n        pbiashigh = np.sum(highpred - hightarget) / np.sum(hightarget) * 100\n        return dict(\n            Bias=bias,\n            RMSE=rmse,\n            ubRMSE=ubrmse,\n            Corr=corr,\n            R2=r2,\n            NSE=nse,\n            KGE=kge,\n            FHV=pbiashigh,\n            FLV=pbiaslow,\n        )\n    else:\n        raise ValueError(\n            \"The number of data is less than 2, we don't calculate the statistics.\"\n        )\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.stat_errors","title":"<code>stat_errors(target, pred, fill_nan=None)</code>","text":"<p>Calculate statistical metrics for 3D arrays with multiple variables.</p> <p>This function extends stat_error to handle 3D arrays where the third dimension represents different variables. Each variable can have its own NaN handling method.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>ndarray</code> <p>Target (observed) values. 3D array [basin, sequence, variable].</p> required <code>pred</code> <code>ndarray</code> <p>Predicted values. Same shape as target.</p> required <code>fill_nan</code> <code>List[str]</code> <p>List of NaN handling methods, one per variable. Each element can be \"no\", \"sum\", or \"mean\". Defaults to [\"no\"].</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, ndarray]]</code> <p>List[Dict[str, np.ndarray]]: List of dictionaries, one per variable. Each dictionary contains: - Bias: Mean error - RMSE: Root mean square error - ubRMSE: Unbiased root mean square error - Corr: Pearson correlation coefficient - R2: Coefficient of determination - NSE: Nash-Sutcliffe efficiency - KGE: Kling-Gupta efficiency - FHV: Peak flow bias (top 2%) - FLV: Low flow bias (bottom 30%)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If: - Input arrays are not 3D - Arrays have incompatible shapes - fill_nan length doesn't match number of variables</p> Example <p>target = np.array([[[1.0, 2.0], [np.nan, 4.0], [5.0, 6.0]]])  # 1x3x2 pred = np.array([[[1.1, 2.1], [3.0, 3.9], [4.9, 5.8]]]) metrics = stat_errors(target, pred, fill_nan=[\"no\", \"sum\"]) print(len(metrics))  # Number of variables 2 print(metrics[0]['RMSE'])  # RMSE for first variable array([0.141])</p> Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def stat_errors(\n    target: np.ndarray, pred: np.ndarray, fill_nan: Optional[List[str]] = None\n) -&gt; List[Dict[str, np.ndarray]]:\n    \"\"\"Calculate statistical metrics for 3D arrays with multiple variables.\n\n    This function extends stat_error to handle 3D arrays where the third dimension\n    represents different variables. Each variable can have its own NaN handling\n    method.\n\n    Args:\n        target (np.ndarray): Target (observed) values. 3D array [basin, sequence, variable].\n        pred (np.ndarray): Predicted values. Same shape as target.\n        fill_nan (List[str], optional): List of NaN handling methods, one per variable.\n            Each element can be \"no\", \"sum\", or \"mean\". Defaults to [\"no\"].\n\n    Returns:\n        List[Dict[str, np.ndarray]]: List of dictionaries, one per variable.\n            Each dictionary contains:\n            - Bias: Mean error\n            - RMSE: Root mean square error\n            - ubRMSE: Unbiased root mean square error\n            - Corr: Pearson correlation coefficient\n            - R2: Coefficient of determination\n            - NSE: Nash-Sutcliffe efficiency\n            - KGE: Kling-Gupta efficiency\n            - FHV: Peak flow bias (top 2%)\n            - FLV: Low flow bias (bottom 30%)\n\n    Raises:\n        ValueError: If:\n            - Input arrays are not 3D\n            - Arrays have incompatible shapes\n            - fill_nan length doesn't match number of variables\n\n    Example:\n        &gt;&gt;&gt; target = np.array([[[1.0, 2.0], [np.nan, 4.0], [5.0, 6.0]]])  # 1x3x2\n        &gt;&gt;&gt; pred = np.array([[[1.1, 2.1], [3.0, 3.9], [4.9, 5.8]]])\n        &gt;&gt;&gt; metrics = stat_errors(target, pred, fill_nan=[\"no\", \"sum\"])\n        &gt;&gt;&gt; print(len(metrics))  # Number of variables\n        2\n        &gt;&gt;&gt; print(metrics[0]['RMSE'])  # RMSE for first variable\n        array([0.141])\n    \"\"\"\n    if fill_nan is None:\n        fill_nan = [\"no\"]\n    if len(target.shape) != 3:\n        raise ValueError(\n            \"The input data should be 3-dim, not 2-dim. If you want to calculate \"\n            \"metrics for 2-d arrays, please use stat_error function.\"\n        )\n    if target.shape != pred.shape:\n        raise ValueError(\"The shape of target and pred should be the same.\")\n    if type(fill_nan) is not list or len(fill_nan) != target.shape[-1]:\n        raise ValueError(\n            \"Please give same length of fill_nan as the number of variables.\"\n        )\n    dict_list = []\n    for k in range(target.shape[-1]):\n        k_dict = stat_error(target[:, :, k], pred[:, :, k], fill_nan=fill_nan[k])\n        dict_list.append(k_dict)\n    return dict_list\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.t2str","title":"<code>t2str(t_)</code>","text":"<p>Convert between datetime string and datetime object.</p> <p>Parameters:</p> Name Type Description Default <code>t_</code> <code>Union[str, datetime]</code> <p>Input time, either as string or datetime object.</p> required <p>Returns:</p> Type Description <p>Union[str, datetime.datetime]: If input is string, returns datetime object.                          If input is datetime, returns string.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If input type is not supported.</p> Note <p>String format is always \"%Y-%m-%d\".</p> Source code in <code>hydroutils\\hydro_time.py</code> <pre><code>def t2str(t_: Union[str, datetime.datetime]):\n    \"\"\"Convert between datetime string and datetime object.\n\n    Args:\n        t_ (Union[str, datetime.datetime]): Input time, either as string or datetime object.\n\n    Returns:\n        Union[str, datetime.datetime]: If input is string, returns datetime object.\n                                     If input is datetime, returns string.\n\n    Raises:\n        NotImplementedError: If input type is not supported.\n\n    Note:\n        String format is always \"%Y-%m-%d\".\n    \"\"\"\n    if type(t_) is str:\n        return datetime.datetime.strptime(t_, \"%Y-%m-%d\")\n    elif type(t_) is datetime.datetime:\n        return t_.strftime(\"%Y-%m-%d\")\n    else:\n        raise NotImplementedError(\"We don't support this data type yet\")\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.t_days_lst2range","title":"<code>t_days_lst2range(t_array)</code>","text":"<p>Transform a list of dates into a start-end interval.</p> <p>Parameters:</p> Name Type Description Default <code>t_array</code> <code>list[Union[datetime64, str]]</code> <p>List of dates in chronological order.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>Two-element list containing first and last dates as strings.</p> Example <p>t_days_lst2range([\"2000-01-01\", \"2000-01-02\", \"2000-01-03\", \"2000-01-04\"]) [\"2000-01-01\", \"2000-01-04\"]</p> Source code in <code>hydroutils\\hydro_time.py</code> <pre><code>def t_days_lst2range(t_array: list) -&gt; list:\n    \"\"\"Transform a list of dates into a start-end interval.\n\n    Args:\n        t_array (list[Union[np.datetime64, str]]): List of dates in chronological order.\n\n    Returns:\n        list: Two-element list containing first and last dates as strings.\n\n    Example:\n        &gt;&gt;&gt; t_days_lst2range([\"2000-01-01\", \"2000-01-02\", \"2000-01-03\", \"2000-01-04\"])\n        [\"2000-01-01\", \"2000-01-04\"]\n    \"\"\"\n    if type(t_array[0]) == np.datetime64:\n        t0 = t_array[0].astype(datetime.datetime)\n        t1 = t_array[-1].astype(datetime.datetime)\n    else:\n        t0 = t_array[0]\n        t1 = t_array[-1]\n    sd = t0.strftime(\"%Y-%m-%d\")\n    ed = t1.strftime(\"%Y-%m-%d\")\n    return [sd, ed]\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.t_range_days","title":"<code>t_range_days(t_range, *, step=np.timedelta64(1, 'D'))</code>","text":"<p>Transform a date range into a uniformly-spaced array of dates.</p> <p>Parameters:</p> Name Type Description Default <code>t_range</code> <code>list</code> <p>Two-element list containing start and end dates as strings.</p> required <code>step</code> <code>timedelta64</code> <p>Time interval between dates. Defaults to 1 day.</p> <code>timedelta64(1, 'D')</code> <p>Returns:</p> Type Description <code>array</code> <p>np.array: Array of datetime64 objects with uniform spacing.</p> Example <p>t_range_days([\"2000-01-01\", \"2000-01-05\"]) array(['2000-01-01', '2000-01-02', '2000-01-03', '2000-01-04'],       dtype='datetime64[D]')</p> Source code in <code>hydroutils\\hydro_time.py</code> <pre><code>def t_range_days(t_range, *, step=np.timedelta64(1, \"D\")) -&gt; np.array:\n    \"\"\"Transform a date range into a uniformly-spaced array of dates.\n\n    Args:\n        t_range (list): Two-element list containing start and end dates as strings.\n        step (np.timedelta64, optional): Time interval between dates. Defaults to 1 day.\n\n    Returns:\n        np.array: Array of datetime64 objects with uniform spacing.\n\n    Example:\n        &gt;&gt;&gt; t_range_days([\"2000-01-01\", \"2000-01-05\"])\n        array(['2000-01-01', '2000-01-02', '2000-01-03', '2000-01-04'],\n              dtype='datetime64[D]')\n    \"\"\"\n    sd = datetime.datetime.strptime(t_range[0], \"%Y-%m-%d\")\n    ed = datetime.datetime.strptime(t_range[1], \"%Y-%m-%d\")\n    return np.arange(sd, ed, step)\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.t_range_days_timedelta","title":"<code>t_range_days_timedelta(t_array, td=12, td_type='h')</code>","text":"<p>Add a time delta to each date in an array.</p> <p>Parameters:</p> Name Type Description Default <code>t_array</code> <code>array</code> <p>Array of datetime64 objects (output of t_range_days).</p> required <code>td</code> <code>int</code> <p>Time period value. Defaults to 12.</p> <code>12</code> <code>td_type</code> <code>str</code> <p>Time period unit ('Y','M','D','h','m','s'). Defaults to \"h\".</p> <code>'h'</code> <p>Returns:</p> Type Description <p>np.array: New array with time delta added to each element.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If td_type is not one of 'Y','M','D','h','m','s'.</p> Source code in <code>hydroutils\\hydro_time.py</code> <pre><code>def t_range_days_timedelta(t_array, td=12, td_type=\"h\"):\n    \"\"\"Add a time delta to each date in an array.\n\n    Args:\n        t_array (np.array): Array of datetime64 objects (output of t_range_days).\n        td (int, optional): Time period value. Defaults to 12.\n        td_type (str, optional): Time period unit ('Y','M','D','h','m','s'). Defaults to \"h\".\n\n    Returns:\n        np.array: New array with time delta added to each element.\n\n    Raises:\n        AssertionError: If td_type is not one of 'Y','M','D','h','m','s'.\n    \"\"\"\n    assert td_type in [\"Y\", \"M\", \"D\", \"h\", \"m\", \"s\"]\n    t_array_final = [t + np.timedelta64(td, td_type) for t in t_array]\n    return np.array(t_array_final)\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.t_range_to_julian","title":"<code>t_range_to_julian(t_range)</code>","text":"<p>Convert a date range to a list of Julian days.</p> <p>Parameters:</p> Name Type Description Default <code>t_range</code> <code>list</code> <p>Two-element list of dates as strings [\"YYYY-MM-DD\", \"YYYY-MM-DD\"].</p> required <p>Returns:</p> Type Description <p>list[int]: List of Julian days for each date in the range.</p> Source code in <code>hydroutils\\hydro_time.py</code> <pre><code>def t_range_to_julian(t_range):\n    \"\"\"Convert a date range to a list of Julian days.\n\n    Args:\n        t_range (list): Two-element list of dates as strings [\"YYYY-MM-DD\", \"YYYY-MM-DD\"].\n\n    Returns:\n        list[int]: List of Julian days for each date in the range.\n    \"\"\"\n    t_array = t_range_days(t_range)\n    t_array_str = np.datetime_as_string(t_array)\n    return [date_to_julian(a_time[:10]) for a_time in t_array_str]\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.t_range_years","title":"<code>t_range_years(t_range)</code>","text":"<p>Get array of years covered by a date range.</p> <p>Parameters:</p> Name Type Description Default <code>t_range</code> <code>list</code> <p>Two-element list of dates as strings [\"YYYY-MM-DD\", \"YYYY-MM-DD\"].</p> required <p>Returns:</p> Type Description <p>np.array: Array of years covered by the date range.</p> Note <ul> <li>Range is left-closed and right-open interval.</li> <li>If end date is not January 1st, end year is included.</li> <li>Example: [\"2000-01-01\", \"2002-01-01\"] -&gt; [2000, 2001]</li> <li>Example: [\"2000-01-01\", \"2002-06-01\"] -&gt; [2000, 2001, 2002]</li> </ul> Source code in <code>hydroutils\\hydro_time.py</code> <pre><code>def t_range_years(t_range):\n    \"\"\"Get array of years covered by a date range.\n\n    Args:\n        t_range (list): Two-element list of dates as strings [\"YYYY-MM-DD\", \"YYYY-MM-DD\"].\n\n    Returns:\n        np.array: Array of years covered by the date range.\n\n    Note:\n        - Range is left-closed and right-open interval.\n        - If end date is not January 1st, end year is included.\n        - Example: [\"2000-01-01\", \"2002-01-01\"] -&gt; [2000, 2001]\n        - Example: [\"2000-01-01\", \"2002-06-01\"] -&gt; [2000, 2001, 2002]\n    \"\"\"\n    start_year = int(t_range[0].split(\"-\")[0])\n    end_year = int(t_range[1].split(\"-\")[0])\n    end_month = int(t_range[1].split(\"-\")[1])\n    end_day = int(t_range[1].split(\"-\")[2])\n    return (\n        np.arange(start_year, end_year)\n        if end_month == 1 and end_day == 1\n        else np.arange(start_year, end_year + 1)\n    )\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.time_to_ten_digits","title":"<code>time_to_ten_digits(time_obj)</code>","text":"<p>Convert a time object to a ten-digit format YYYYMMDDHH.</p> <p>Parameters:</p> Name Type Description Default <code>time_obj</code> <code>Union[datetime, datetime64, str]</code> <p>Time object to convert. Can be datetime, numpy.datetime64, or string.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Ten-digit time string in YYYYMMDDHH format.</p> Example <p>time_to_ten_digits(datetime.datetime(2020, 1, 1, 12, 0)) '2020010112' time_to_ten_digits(np.datetime64('2020-01-01T12')) '2020010112' time_to_ten_digits('2020-01-01T12:00:00') '2020010112'</p> Source code in <code>hydroutils\\hydro_event.py</code> <pre><code>def time_to_ten_digits(time_obj) -&gt; str:\n    \"\"\"Convert a time object to a ten-digit format YYYYMMDDHH.\n\n    Args:\n        time_obj (Union[datetime.datetime, np.datetime64, str]): Time object to convert.\n            Can be datetime, numpy.datetime64, or string.\n\n    Returns:\n        str: Ten-digit time string in YYYYMMDDHH format.\n\n    Example:\n        &gt;&gt;&gt; time_to_ten_digits(datetime.datetime(2020, 1, 1, 12, 0))\n        '2020010112'\n        &gt;&gt;&gt; time_to_ten_digits(np.datetime64('2020-01-01T12'))\n        '2020010112'\n        &gt;&gt;&gt; time_to_ten_digits('2020-01-01T12:00:00')\n        '2020010112'\n    \"\"\"\n    if isinstance(time_obj, np.datetime64):\n        # \u5982\u679c\u662fnumpy datetime64\u5bf9\u8c61\n        return (\n            time_obj.astype(\"datetime64[h]\")\n            .astype(str)\n            .replace(\"-\", \"\")\n            .replace(\"T\", \"\")\n            .replace(\":\", \"\")\n        )\n    elif hasattr(time_obj, \"strftime\"):\n        # \u5982\u679c\u662fdatetime\u5bf9\u8c61\n        return time_obj.strftime(\"%Y%m%d%H\")\n    else:\n        # \u5982\u679c\u662f\u5b57\u7b26\u4e32\uff0c\u5c1d\u8bd5\u89e3\u6790\n        try:\n            if isinstance(time_obj, str):\n                dt = datetime.fromisoformat(time_obj.replace(\"Z\", \"+00:00\"))\n                return dt.strftime(\"%Y%m%d%H\")\n            else:\n                return \"0000000000\"  # \u9ed8\u8ba4\u503c\n        except Exception:\n            return \"0000000000\"  # \u9ed8\u8ba4\u503c\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.trans_norm","title":"<code>trans_norm(x, var_lst, stat_dict, *, to_norm)</code>","text":"<p>Normalize or denormalize data using statistical parameters.</p> <p>This function performs normalization or denormalization on 2D or 3D data arrays using pre-computed statistical parameters. It supports multiple variables and can handle both site-based and time series data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input data array: - 2D: [sites, variables] - 3D: [sites, time, variables]</p> required <code>var_lst</code> <code>Union[str, List[str]]</code> <p>Variable name(s) to process.</p> required <code>stat_dict</code> <code>Dict[str, List[float]]</code> <p>Dictionary containing statistics for each variable. Each value is [p10, p90, mean, std].</p> required <code>to_norm</code> <code>bool</code> <p>If True, normalize data; if False, denormalize data.</p> required <p>Returns:</p> Type Description <p>np.ndarray: Normalized or denormalized data with same shape as input.</p> Note <ul> <li>Normalization: (x - mean) / std</li> <li>Denormalization: x * std + mean</li> <li>Statistics should be pre-computed for each variable</li> <li>Handles single variable (str) or multiple variables (list)</li> <li>Preserves input array dimensions</li> </ul> Example Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def trans_norm(x, var_lst, stat_dict, *, to_norm):\n    \"\"\"Normalize or denormalize data using statistical parameters.\n\n    This function performs normalization or denormalization on 2D or 3D data\n    arrays using pre-computed statistical parameters. It supports multiple\n    variables and can handle both site-based and time series data.\n\n    Args:\n        x (np.ndarray): Input data array:\n            - 2D: [sites, variables]\n            - 3D: [sites, time, variables]\n        var_lst (Union[str, List[str]]): Variable name(s) to process.\n        stat_dict (Dict[str, List[float]]): Dictionary containing statistics\n            for each variable. Each value is [p10, p90, mean, std].\n        to_norm (bool): If True, normalize data; if False, denormalize data.\n\n    Returns:\n        np.ndarray: Normalized or denormalized data with same shape as input.\n\n    Note:\n        - Normalization: (x - mean) / std\n        - Denormalization: x * std + mean\n        - Statistics should be pre-computed for each variable\n        - Handles single variable (str) or multiple variables (list)\n        - Preserves input array dimensions\n\n    Example:\n        &gt;&gt;&gt; # Normalization example\n        &gt;&gt;&gt; data = np.array([[1.0, 2.0], [3.0, 4.0]])  # 2 sites, 2 variables\n        &gt;&gt;&gt; stats = {'var1': [0, 2, 1, 0.5], 'var2': [1, 5, 3, 1.0]}\n        &gt;&gt;&gt; vars = ['var1', 'var2']\n        &gt;&gt;&gt; normalized = trans_norm(data, vars, stats, to_norm=True)\n        &gt;&gt;&gt; print(normalized)  # Example output\n        array([[0. , -1.],\n               [4. ,  1.]])\n    \"\"\"\n    if type(var_lst) is str:\n        var_lst = [var_lst]\n    out = np.zeros(x.shape)\n    for k in range(len(var_lst)):\n        var = var_lst[k]\n        stat = stat_dict[var]\n        if to_norm is True:\n            if len(x.shape) == 3:\n                out[:, :, k] = (x[:, :, k] - stat[2]) / stat[3]\n            elif len(x.shape) == 2:\n                out[:, k] = (x[:, k] - stat[2]) / stat[3]\n        elif len(x.shape) == 3:\n            out[:, :, k] = x[:, :, k] * stat[3] + stat[2]\n        elif len(x.shape) == 2:\n            out[:, k] = x[:, k] * stat[3] + stat[2]\n    return out\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.trans_norm--normalization-example","title":"Normalization example","text":"<p>data = np.array([[1.0, 2.0], [3.0, 4.0]])  # 2 sites, 2 variables stats = {'var1': [0, 2, 1, 0.5], 'var2': [1, 5, 3, 1.0]} vars = ['var1', 'var2'] normalized = trans_norm(data, vars, stats, to_norm=True) print(normalized)  # Example output array([[0. , -1.],        [4. ,  1.]])</p>"},{"location":"api/hydroutils/#hydroutils.unserialize_json","title":"<code>unserialize_json(my_file)</code>","text":"<p>Load a JSON file into a Python object.</p> <p>Parameters:</p> Name Type Description Default <code>my_file</code> <code>str</code> <p>Path to the JSON file to read.</p> required <p>Returns:</p> Name Type Description <code>object</code> <p>Python object (typically dict or list) loaded from the JSON file.</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def unserialize_json(my_file):\n    \"\"\"Load a JSON file into a Python object.\n\n    Args:\n        my_file (str): Path to the JSON file to read.\n\n    Returns:\n        object: Python object (typically dict or list) loaded from the JSON file.\n    \"\"\"\n    with open(my_file, \"r\") as fp:\n        my_object = json.load(fp)\n    return my_object\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.unserialize_json_ordered","title":"<code>unserialize_json_ordered(my_file)</code>","text":"<p>Load a JSON file into an OrderedDict, preserving key order.</p> <p>Parameters:</p> Name Type Description Default <code>my_file</code> <code>str</code> <p>Path to the JSON file to read.</p> required <p>Returns:</p> Name Type Description <code>OrderedDict</code> <p>Dictionary with preserved key order from the JSON file.</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def unserialize_json_ordered(my_file):\n    \"\"\"Load a JSON file into an OrderedDict, preserving key order.\n\n    Args:\n        my_file (str): Path to the JSON file to read.\n\n    Returns:\n        OrderedDict: Dictionary with preserved key order from the JSON file.\n    \"\"\"\n    with open(my_file, \"r\") as fp:\n        m_dict = json.load(fp, object_pairs_hook=OrderedDict)\n    return m_dict\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.unserialize_numpy","title":"<code>unserialize_numpy(my_file)</code>","text":"<p>Load a NumPy array from a binary file.</p> <p>Parameters:</p> Name Type Description Default <code>my_file</code> <code>str</code> <p>Path to the NumPy array file.</p> required <p>Returns:</p> Type Description <p>np.ndarray: NumPy array loaded from the file.</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def unserialize_numpy(my_file):\n    \"\"\"Load a NumPy array from a binary file.\n\n    Args:\n        my_file (str): Path to the NumPy array file.\n\n    Returns:\n        np.ndarray: NumPy array loaded from the file.\n    \"\"\"\n    return np.load(my_file)\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.unserialize_pickle","title":"<code>unserialize_pickle(my_file)</code>","text":"<p>Load an object from a pickle file.</p> <p>Parameters:</p> Name Type Description Default <code>my_file</code> <code>str</code> <p>Path to the pickle file to read.</p> required <p>Returns:</p> Name Type Description <code>object</code> <p>Python object loaded from the pickle file.</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def unserialize_pickle(my_file):\n    \"\"\"Load an object from a pickle file.\n\n    Args:\n        my_file (str): Path to the pickle file to read.\n\n    Returns:\n        object: Python object loaded from the pickle file.\n    \"\"\"\n    with open(my_file, \"rb\") as f:\n        my_object = pickle.load(f)\n    return my_object\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.unzip_file","title":"<code>unzip_file(data_zip, path_unzip)</code>","text":"<p>Extract a zip file to the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>data_zip</code> <code>str</code> <p>Path to the zip file to extract.</p> required <code>path_unzip</code> <code>str</code> <p>Directory where the contents will be extracted.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def unzip_file(data_zip, path_unzip):\n    \"\"\"Extract a zip file to the specified directory.\n\n    Args:\n        data_zip (str): Path to the zip file to extract.\n        path_unzip (str): Directory where the contents will be extracted.\n\n    Returns:\n        None\n    \"\"\"\n    with zipfile.ZipFile(data_zip, \"r\") as zip_temp:\n        zip_temp.extractall(path_unzip)\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.unzip_nested_zip","title":"<code>unzip_nested_zip(dataset_zip, path_unzip)</code>","text":"<p>Extract a zip file including any nested zip files If a file's name is \"xxx_\", it seems the \"extractall\" function in the \"zipfile\" lib will throw an OSError, so please check the unzipped files manually when this occurs. Parameters</p> <p>dataset_zip: the zip file path_unzip: where it is unzipped</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def unzip_nested_zip(dataset_zip, path_unzip):\n    \"\"\"\n    Extract a zip file including any nested zip files\n    If a file's name is \"xxx_\", it seems the \"extractall\" function in the \"zipfile\" lib will throw an OSError,\n    so please check the unzipped files manually when this occurs.\n    Parameters\n    ----------\n    dataset_zip: the zip file\n    path_unzip: where it is unzipped\n    \"\"\"\n\n    with zipfile.ZipFile(dataset_zip, \"r\") as zfile:\n        try:\n            zfile.extractall(path=path_unzip)\n        except OSError as e:\n            logging.warning(\n                \"Please check the unzipped files manually. There may be some missed important files.\"\n            )\n            logging.warning(f\"The directory is: {path_unzip}\")\n            logging.warning(f\"Error message: {e}\")\n    for root, dirs, files in os.walk(path_unzip):\n        for filename in files:\n            if re.search(r\"\\.zip$\", filename):\n                file_spec = os.path.join(root, filename)\n                new_dir = os.path.join(root, filename[:-4])\n                unzip_nested_zip(file_spec, new_dir)\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.wilcoxon_t_test","title":"<code>wilcoxon_t_test(xs, xo)</code>","text":"<p>Perform Wilcoxon signed-rank test on paired samples.</p> <p>This function performs a Wilcoxon signed-rank test to determine whether two related samples have the same distribution. It's particularly useful for comparing model predictions against observations.</p> <p>Parameters:</p> Name Type Description Default <code>xs</code> <code>ndarray</code> <p>First sample (typically simulated/predicted values).</p> required <code>xo</code> <code>ndarray</code> <p>Second sample (typically observed values).</p> required <p>Returns:</p> Type Description <code>Tuple[float, float]</code> <p>Tuple[float, float]: Test statistics: - w: Wilcoxon test statistic - p: p-value for the test</p> Note <ul> <li>Non-parametric alternative to paired t-test</li> <li>Assumes samples are paired and same length</li> <li>Direction of difference (xs-xo vs xo-xs) doesn't affect results</li> <li>Uses scipy.stats.wilcoxon under the hood</li> </ul> Example <p>sim = np.array([102, 104, 98, 101, 96, 103, 95]) obs = np.array([100, 102, 95, 100, 93, 101, 94]) w, p = wilcoxon_t_test(sim, obs) print(f\"W-statistic: {w:.2f}, p-value: {p:.4f}\") W-statistic: 26.50, p-value: 0.0234</p> Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def wilcoxon_t_test(xs: np.ndarray, xo: np.ndarray) -&gt; Tuple[float, float]:\n    \"\"\"Perform Wilcoxon signed-rank test on paired samples.\n\n    This function performs a Wilcoxon signed-rank test to determine whether two\n    related samples have the same distribution. It's particularly useful for\n    comparing model predictions against observations.\n\n    Args:\n        xs (np.ndarray): First sample (typically simulated/predicted values).\n        xo (np.ndarray): Second sample (typically observed values).\n\n    Returns:\n        Tuple[float, float]: Test statistics:\n            - w: Wilcoxon test statistic\n            - p: p-value for the test\n\n    Note:\n        - Non-parametric alternative to paired t-test\n        - Assumes samples are paired and same length\n        - Direction of difference (xs-xo vs xo-xs) doesn't affect results\n        - Uses scipy.stats.wilcoxon under the hood\n\n    Example:\n        &gt;&gt;&gt; sim = np.array([102, 104, 98, 101, 96, 103, 95])\n        &gt;&gt;&gt; obs = np.array([100, 102, 95, 100, 93, 101, 94])\n        &gt;&gt;&gt; w, p = wilcoxon_t_test(sim, obs)\n        &gt;&gt;&gt; print(f\"W-statistic: {w:.2f}, p-value: {p:.4f}\")\n        W-statistic: 26.50, p-value: 0.0234\n    \"\"\"\n    diff = xs - xo  # same result when using xo-xs\n    w, p = wilcoxon(diff)\n    return w, p\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.wilcoxon_t_test_for_lst","title":"<code>wilcoxon_t_test_for_lst(x_lst, rnd_num=2)</code>","text":"<p>Perform pairwise Wilcoxon tests on multiple arrays.</p> <p>This function performs Wilcoxon signed-rank tests on every possible pair of arrays in a list of arrays. Results are rounded to specified precision.</p> <p>Parameters:</p> Name Type Description Default <code>x_lst</code> <code>List[ndarray]</code> <p>List of arrays to compare pairwise.</p> required <code>rnd_num</code> <code>int</code> <p>Number of decimal places to round results to. Defaults to 2.</p> <code>2</code> <p>Returns:</p> Type Description <p>Tuple[List[float], List[float]]: Two lists: - w: List of Wilcoxon test statistics for each pair - p: List of p-values for each pair</p> Note <ul> <li>Generates all possible pairs using itertools.combinations</li> <li>Results are ordered by pair combinations</li> <li>Number of pairs = n*(n-1)/2 where n is number of arrays</li> <li>All test statistics and p-values are rounded</li> </ul> Example <p>arrays = [ ...     np.array([1, 2, 3, 4]), ...     np.array([2, 3, 4, 5]), ...     np.array([3, 4, 5, 6]) ... ] w, p = wilcoxon_t_test_for_lst(arrays) print(f\"W-statistics: {w}\") W-statistics: [0.00, 0.00, 0.00] print(f\"p-values: {p}\") p-values: [0.07, 0.07, 0.07]</p> Source code in <code>hydroutils\\hydro_stat.py</code> <pre><code>def wilcoxon_t_test_for_lst(x_lst, rnd_num=2):\n    \"\"\"Perform pairwise Wilcoxon tests on multiple arrays.\n\n    This function performs Wilcoxon signed-rank tests on every possible pair\n    of arrays in a list of arrays. Results are rounded to specified precision.\n\n    Args:\n        x_lst (List[np.ndarray]): List of arrays to compare pairwise.\n        rnd_num (int, optional): Number of decimal places to round results to.\n            Defaults to 2.\n\n    Returns:\n        Tuple[List[float], List[float]]: Two lists:\n            - w: List of Wilcoxon test statistics for each pair\n            - p: List of p-values for each pair\n\n    Note:\n        - Generates all possible pairs using itertools.combinations\n        - Results are ordered by pair combinations\n        - Number of pairs = n*(n-1)/2 where n is number of arrays\n        - All test statistics and p-values are rounded\n\n    Example:\n        &gt;&gt;&gt; arrays = [\n        ...     np.array([1, 2, 3, 4]),\n        ...     np.array([2, 3, 4, 5]),\n        ...     np.array([3, 4, 5, 6])\n        ... ]\n        &gt;&gt;&gt; w, p = wilcoxon_t_test_for_lst(arrays)\n        &gt;&gt;&gt; print(f\"W-statistics: {w}\")\n        W-statistics: [0.00, 0.00, 0.00]\n        &gt;&gt;&gt; print(f\"p-values: {p}\")\n        p-values: [0.07, 0.07, 0.07]\n    \"\"\"\n    arr_lst = np.asarray(x_lst)\n    w, p = [], []\n    arr_lst_pair = list(itertools.combinations(arr_lst, 2))\n    for arr_pair in arr_lst_pair:\n        wi, pi = wilcoxon_t_test(arr_pair[0], arr_pair[1])\n        w.append(round(wi, rnd_num))\n        p.append(round(pi, rnd_num))\n    return w, p\n</code></pre>"},{"location":"api/hydroutils/#hydroutils.zip_extract","title":"<code>zip_extract(the_dir)</code>","text":"<p>Extract the downloaded zip files in the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>the_dir</code> <code>Path</code> <p>The directory containing zip files to extract.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>hydroutils\\hydro_file.py</code> <pre><code>def zip_extract(the_dir) -&gt; None:\n    \"\"\"Extract the downloaded zip files in the specified directory.\n\n    Args:\n        the_dir (Path): The directory containing zip files to extract.\n\n    Returns:\n        None\n    \"\"\"\n    for f in the_dir.glob(\"*.zip\"):\n        with zipfile.ZipFile(f) as zf:\n            # extract files to a directory named by f.stem\n            zf.extractall(the_dir.joinpath(f.stem))\n</code></pre>"},{"location":"api/hydroutils/#submodules","title":"Submodules","text":"<p>The <code>hydroutils</code> package consists of several specialized modules:</p> <ul> <li>hydro_stat - Statistical analysis and performance metrics</li> <li>hydro_plot - Visualization tools for hydrological data  </li> <li>hydro_time - Time series processing utilities</li> <li>hydro_file - File I/O operations</li> <li>hydro_units - Unit conversion utilities</li> <li>hydro_event - Flood event extraction and analysis</li> <li>hydro_s3 - AWS S3 integration</li> <li>hydro_log - Logging utilities</li> </ul> <p>Click on any module above for detailed function documentation.</p>"}]}